{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTgrxJWKRlzZ"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y7_UCg8N5Qeu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2TokenizerFast,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mICAot6NRn46"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0et_iRKT5UDt"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    model_name: str = \"t5-small\"\n",
        "    gpt2_name: str = \"gpt2\"\n",
        "    max_source_length: int = 256\n",
        "    max_target_length: int = 4\n",
        "    batch_size: int = 16\n",
        "    lr: float = 1e-3\n",
        "    num_epochs: int = 5\n",
        "    prompt_length: int = 10\n",
        "    lambda_grid: List[float] = None\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.lambda_grid is None:\n",
        "            self.lambda_grid = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yF-b68es5X9s"
      },
      "outputs": [],
      "source": [
        "def preprocess_boolq(example, tokenizer: T5TokenizerFast, max_source_length: int, max_target_length: int):\n",
        "    \"\"\"\n",
        "    Turn BoolQ into T5 inputs:\n",
        "      \"question: {question} passage: {passage}\"\n",
        "    Targets are \"yes\" or \"no\".\n",
        "    \"\"\"\n",
        "    question = example[\"question\"]\n",
        "    passage = example[\"passage\"]\n",
        "    answer = \"yes\" if example[\"answer\"] else \"no\"\n",
        "\n",
        "    source = f\"question: {question} passage: {passage}\"\n",
        "    target = answer\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        source,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_source_length,\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            target,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_target_length,\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def load_boolq(tokenizer: T5TokenizerFast, cfg: ExperimentConfig):\n",
        "    ds = load_dataset(\"boolq\")\n",
        "\n",
        "    preprocess_fn = lambda ex: preprocess_boolq(\n",
        "        ex,\n",
        "        tokenizer=tokenizer,\n",
        "        max_source_length=cfg.max_source_length,\n",
        "        max_target_length=cfg.max_target_length,\n",
        "    )\n",
        "\n",
        "    ds = ds.map(preprocess_fn, batched=False)\n",
        "    ds.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    )\n",
        "\n",
        "    train_dl = DataLoader(ds[\"train\"], batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_dl = DataLoader(ds[\"validation\"], batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    return train_dl, val_dl\n",
        "\n",
        "\n",
        "def load_boolq_balanced(tokenizer: T5TokenizerFast, cfg: ExperimentConfig, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Load BoolQ, balance the train split on the raw dataset (True/False),\n",
        "    then preprocess into T5-style inputs and wrap in DataLoaders.\n",
        "    \"\"\"\n",
        "    raw = load_dataset(\"boolq\")\n",
        "    train_raw = raw[\"train\"]\n",
        "    val_raw   = raw[\"validation\"]\n",
        "\n",
        "    # --- Balance train: downsample majority label ---\n",
        "    true_indices  = [i for i, ex in enumerate(train_raw) if ex[\"answer\"]]\n",
        "    false_indices = [i for i, ex in enumerate(train_raw) if not ex[\"answer\"]]\n",
        "\n",
        "    min_count = min(len(true_indices), len(false_indices))\n",
        "    true_indices  = true_indices[:min_count]\n",
        "    false_indices = false_indices[:min_count]\n",
        "\n",
        "    balanced_indices = true_indices + false_indices\n",
        "    # Optional but usually helpful: shuffle indices for randomness\n",
        "    rng = torch.Generator().manual_seed(seed)\n",
        "    perm = torch.randperm(len(balanced_indices), generator=rng).tolist()\n",
        "    balanced_indices = [balanced_indices[i] for i in perm]\n",
        "\n",
        "    train_balanced = train_raw.select(balanced_indices)\n",
        "\n",
        "    print(\n",
        "        f\"Balanced BoolQ train: {len(train_balanced)} examples \"\n",
        "        f\"({min_count} True, {min_count} False)\"\n",
        "    )\n",
        "\n",
        "    # --- Preprocess to T5 format ---\n",
        "    def preprocess_fn(ex):\n",
        "        return preprocess_boolq(\n",
        "            ex,\n",
        "            tokenizer=tokenizer,\n",
        "            max_source_length=cfg.max_source_length,\n",
        "            max_target_length=cfg.max_target_length,\n",
        "        )\n",
        "\n",
        "    train_proc = train_balanced.map(preprocess_fn, batched=False)\n",
        "    val_proc   = val_raw.map(preprocess_fn, batched=False)\n",
        "\n",
        "    # Keep only the model fields and cast to torch\n",
        "    cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    train_proc.set_format(type=\"torch\", columns=cols)\n",
        "    val_proc.set_format(type=\"torch\", columns=cols)\n",
        "\n",
        "    train_dl = DataLoader(train_proc, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_dl   = DataLoader(val_proc,   batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    return train_dl, val_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hAkwGMi9J4C",
        "outputId": "3ecbadfd-cf90-4379-8b23-7eaf5270c6e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_true: 5874\n",
            "num_false: 3553\n",
            "P(True) = 0.6231038506417736\n",
            "P(False) = 0.37689614935822635\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "ds = load_dataset(\"boolq\")\n",
        "labels = [ex[\"answer\"] for ex in ds[\"train\"]]   # True/False labels\n",
        "ctr = Counter(labels)\n",
        "\n",
        "num_true  = ctr[True]\n",
        "num_false = ctr[False]\n",
        "total     = num_true + num_false\n",
        "\n",
        "print(\"num_true:\", num_true)\n",
        "print(\"num_false:\", num_false)\n",
        "print(\"P(True) =\", num_true / total)\n",
        "print(\"P(False) =\", num_false / total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MF5gFm545goZ"
      },
      "outputs": [],
      "source": [
        "class T5ContinuousSoftPrompt(nn.Module):\n",
        "    \"\"\"\n",
        "    Continuous soft prompts: learn a (prompt_length, d_model) tensor\n",
        "    and prepend it as prefix embeddings to the encoder input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base: T5ForConditionalGeneration, prompt_length: int):\n",
        "        super().__init__()\n",
        "        self.t5 = base\n",
        "        self.prompt_length = prompt_length\n",
        "\n",
        "        d_model = base.encoder.embed_tokens.weight.shape[1]\n",
        "        self.soft_prompt = nn.Parameter(\n",
        "            torch.zeros(prompt_length, d_model)\n",
        "        )\n",
        "        nn.init.normal_(self.soft_prompt, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: torch.Tensor,\n",
        "    ):\n",
        "        batch_size = input_ids.size(0)\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Original token embeddings\n",
        "        inputs_embeds = self.t5.encoder.embed_tokens(input_ids)\n",
        "\n",
        "        # Broadcast prompt to batch\n",
        "        prompt_embeds = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # Prepend to input sequence\n",
        "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
        "\n",
        "        # Extend attention mask\n",
        "        prompt_mask = torch.ones(batch_size, self.prompt_length, device=device, dtype=attention_mask.dtype)\n",
        "        attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)\n",
        "\n",
        "        outputs = self.t5(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        return outputs  # has .loss and .logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHbVDFyp5iem"
      },
      "outputs": [],
      "source": [
        "class T5PEZPrompt(nn.Module):\n",
        "    \"\"\"\n",
        "    PEZ-style one-hot prompt over the T5 vocabulary.\n",
        "\n",
        "    - self.prompt is (L, V) with rows ~ one-hot\n",
        "    - FORWARD: treat self.prompt as continuous, embed via matrix multiply\n",
        "               prompt_embeds = prompt @ embed_tokens.weight\n",
        "      This keeps the computation differentiable w.r.t. self.prompt.\n",
        "    - DISCRETIZATION: use argmax *outside* the forward (for decoding / GPT-2).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base: T5ForConditionalGeneration, prompt_length: int):\n",
        "        super().__init__()\n",
        "        self.t5 = base\n",
        "        self.vocab_size = base.encoder.embed_tokens.weight.shape[0]\n",
        "        self.prompt_length = prompt_length\n",
        "\n",
        "        # Initialize as random one-hot rows\n",
        "        init_ids = torch.randint(0, self.vocab_size, (prompt_length,))\n",
        "        prompt = torch.nn.functional.one_hot(init_ids, num_classes=self.vocab_size).float()\n",
        "        self.prompt = nn.Parameter(prompt)  # (L, V), requires_grad=True by default\n",
        "\n",
        "    # ---- helper used ONLY for decoding / perplexity, NOT in forward ----\n",
        "    def get_prompt_token_ids(self) -> torch.Tensor:\n",
        "        return self.prompt.argmax(dim=-1)  # (L,)\n",
        "\n",
        "    def decode_prompt(self, tokenizer: T5TokenizerFast) -> str:\n",
        "        token_ids = self.get_prompt_token_ids().tolist()\n",
        "        return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "    \n",
        "    def get_projected_prompt_embeds(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        PEZ projection: Proj_E(P) = embed(argmax(P))\n",
        "        Returns projected prompt embeddings for use in forward pass.\n",
        "        This is differentiable via straight-through estimator.\n",
        "        \"\"\"\n",
        "        # Get token IDs via argmax (discrete projection)\n",
        "        token_ids = self.get_prompt_token_ids()  # (L,)\n",
        "        # Embed the projected tokens\n",
        "        embed_matrix = self.t5.encoder.embed_tokens.weight  # (V, d_model)\n",
        "        prompt_embeds = embed_matrix[token_ids]  # (L, d_model)\n",
        "        prompt_embeds = prompt_embeds.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        return prompt_embeds\n",
        "\n",
        "    # ---- differentiable forward ----\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: torch.Tensor,\n",
        "        use_projection: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Embed prompt by multiplying the (L, V) prompt matrix with the\n",
        "        (V, d_model) embedding matrix. This is linear in self.prompt,\n",
        "        so gradients flow into self.prompt.\n",
        "        \n",
        "        If use_projection=True, uses PEZ projection: Proj_E(P) = embed(argmax(P))\n",
        "        \"\"\"\n",
        "        batch_size = input_ids.size(0)\n",
        "        device = input_ids.device\n",
        "\n",
        "        # 1) Embed the prompt: either continuous or projected\n",
        "        if use_projection:\n",
        "            # PEZ: use projected embeddings (discrete tokens)\n",
        "            prompt_embeds = self.get_projected_prompt_embeds(batch_size)\n",
        "        else:\n",
        "            # Original: continuous embedding\n",
        "            embed_matrix = self.t5.encoder.embed_tokens.weight  # (V, d_model)\n",
        "            prompt_embeds = self.prompt @ embed_matrix          # (L, d_model)\n",
        "            prompt_embeds = prompt_embeds.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # 2) Embed the original input tokens\n",
        "        inputs_embeds = self.t5.encoder.embed_tokens(input_ids)\n",
        "\n",
        "        # 3) Prepend prompt embeddings\n",
        "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
        "\n",
        "        # 4) Extend attention mask\n",
        "        prompt_mask = torch.ones(\n",
        "            batch_size,\n",
        "            self.prompt_length,\n",
        "            device=device,\n",
        "            dtype=attention_mask.dtype,\n",
        "        )\n",
        "        attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)\n",
        "\n",
        "        # 5) Standard T5 forward\n",
        "        outputs = self.t5(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        return outputs  # has .loss and .logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i2sO9MX55oHZ"
      },
      "outputs": [],
      "source": [
        "def compute_prompt_ppl_loss_from_text(\n",
        "    gpt2_model: GPT2LMHeadModel,\n",
        "    gpt2_tokenizer: GPT2TokenizerFast,\n",
        "    prompt_text: str,\n",
        "    device: str,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Take the decoded prompt text, feed to GPT-2, compute LM loss.\n",
        "    Returns 0.0 if the text tokenizes to an empty sequence.\n",
        "    \"\"\"\n",
        "    # Guard 1: decoded text is empty or whitespace\n",
        "    if not prompt_text or not prompt_text.strip():\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    enc = gpt2_tokenizer(\n",
        "        prompt_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "\n",
        "    # Guard 2: tokenizer produced no tokens\n",
        "    if input_ids.numel() == 0:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_model(input_ids=input_ids, labels=labels)\n",
        "    return outputs.loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rRjqZKA75scC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_accuracy_t5(model: nn.Module, dataloader: DataLoader,\n",
        "                         tokenizer: T5TokenizerFast, device: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    BoolQ accuracy for T5-style models:\n",
        "    - We look at the first decoder position's logits (position 0 in labels)\n",
        "    - Compare scores for 'yes' vs 'no'\n",
        "    \n",
        "    Returns a dictionary with:\n",
        "    - 'overall': overall accuracy\n",
        "    - 'true_acc': accuracy on questions where answer is True (yes)\n",
        "    - 'false_acc': accuracy on questions where answer is False (no)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Correct way to get the IDs for \"yes\" / \"no\" for T5\n",
        "    yes_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
        "    no_id  = tokenizer(\"no\",  add_special_tokens=False).input_ids[0]\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Separate tracking for True and False answers\n",
        "    correct_true = 0\n",
        "    total_true = 0\n",
        "    correct_false = 0\n",
        "    total_false = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids      = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass: all your wrappers accept labels=...\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        # logits: (B, T_out, V)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # First decoder step (the token that should be \"yes\" or \"no\")\n",
        "        first_step_logits = logits[:, 0, :]  # (B, V)\n",
        "\n",
        "        # Scores only for \"yes\" and \"no\"\n",
        "        yes_scores = first_step_logits[:, yes_id]\n",
        "        no_scores  = first_step_logits[:, no_id]\n",
        "\n",
        "        # Predict yes if yes_score >= no_score else no\n",
        "        pred_is_yes = (yes_scores >= no_scores)\n",
        "\n",
        "        # Ground truth: first label token\n",
        "        target_ids  = labels[:, 0]\n",
        "        target_is_yes = (target_ids == yes_id)\n",
        "        target_is_no  = (target_ids == no_id)\n",
        "\n",
        "        # Correct if our yes/no prediction matches target\n",
        "        correct_batch = (pred_is_yes & target_is_yes) | (~pred_is_yes & target_is_no)\n",
        "        correct += correct_batch.sum().item()\n",
        "        total   += target_ids.size(0)\n",
        "        \n",
        "        # Track accuracy separately for True and False answers\n",
        "        # True answers (yes)\n",
        "        true_mask = target_is_yes\n",
        "        if true_mask.any():\n",
        "            correct_true_batch = (pred_is_yes & target_is_yes)[true_mask]\n",
        "            correct_true += correct_true_batch.sum().item()\n",
        "            total_true += true_mask.sum().item()\n",
        "        \n",
        "        # False answers (no)\n",
        "        false_mask = target_is_no\n",
        "        if false_mask.any():\n",
        "            correct_false_batch = (~pred_is_yes & target_is_no)[false_mask]\n",
        "            correct_false += correct_false_batch.sum().item()\n",
        "            total_false += false_mask.sum().item()\n",
        "\n",
        "    overall_acc = correct / total if total > 0 else 0.0\n",
        "    true_acc = correct_true / total_true if total_true > 0 else 0.0\n",
        "    false_acc = correct_false / total_false if total_false > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        \"overall\": overall_acc,\n",
        "        \"true_acc\": true_acc,\n",
        "        \"false_acc\": false_acc\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-G92MA2C5tLQ"
      },
      "outputs": [],
      "source": [
        "def train_continuous_soft_prompt(\n",
        "    cfg: ExperimentConfig,\n",
        "    tokenizer: T5TokenizerFast,\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader,\n",
        "    adversarial: bool,\n",
        ") -> Dict[str, Any]:\n",
        "    device = cfg.device\n",
        "    base = T5ForConditionalGeneration.from_pretrained(cfg.model_name).to(device)\n",
        "    base.eval()\n",
        "    for p in base.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    model = T5ContinuousSoftPrompt(base, prompt_length=cfg.prompt_length).to(device)\n",
        "\n",
        "    # Use lower learning rate for adversarial training to prevent explosion\n",
        "    effective_lr = cfg.lr * 0.1 if adversarial else cfg.lr\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=effective_lr, weight_decay=0.01)\n",
        "\n",
        "    # --- label ids for flipping (yes/no) ---\n",
        "    if adversarial:\n",
        "        yes_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
        "        no_id  = tokenizer(\"no\",  add_special_tokens=False).input_ids[0]\n",
        "    # --------------------------------------#\n",
        "\n",
        "    history = {\n",
        "        \"train_joint\": [],\n",
        "        \"train_task\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"val_acc_true\": [],\n",
        "        \"val_acc_false\": [],\n",
        "        \"prompt_norm\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        model.train()\n",
        "        running_joint = 0.0\n",
        "        running_task  = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in train_dl:\n",
        "            input_ids      = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "            # ---- flip labels during adversarial TRAINING ----\n",
        "            if adversarial:\n",
        "                labels_flipped = labels.clone()\n",
        "                mask_yes = labels == yes_id\n",
        "                mask_no  = labels == no_id\n",
        "                labels_flipped[mask_yes] = no_id\n",
        "                labels_flipped[mask_no]  = yes_id\n",
        "                labels_for_loss = labels_flipped\n",
        "            else:\n",
        "                labels_for_loss = labels\n",
        "            # -------------------------------------------------#\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels_for_loss,\n",
        "            )\n",
        "            task_loss = outputs.loss\n",
        "            joint_loss = task_loss  # no sign flip\n",
        "\n",
        "            joint_loss.backward()\n",
        "            # gradient clipping (tighter for adversarial)\n",
        "            max_norm = 0.5 if adversarial else 1.0\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_joint += joint_loss.item()\n",
        "            running_task  += task_loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_train_joint = running_joint / max(1, n_batches)\n",
        "        avg_train_task  = running_task  / max(1, n_batches)\n",
        "\n",
        "        # ----------------- validation: TRUE labels -----------------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                input_ids      = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,  # real yes/no labels\n",
        "                )\n",
        "                val_loss += outputs.loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, val_batches)\n",
        "        val_acc_dict = evaluate_accuracy_t5(model, val_dl, tokenizer, device)\n",
        "        val_acc = val_acc_dict[\"overall\"]\n",
        "        val_acc_true = val_acc_dict[\"true_acc\"]\n",
        "        val_acc_false = val_acc_dict[\"false_acc\"]\n",
        "\n",
        "        prompt_norm = model.soft_prompt.norm().item()\n",
        "\n",
        "        history[\"train_joint\"].append(avg_train_joint)\n",
        "        history[\"train_task\"].append(avg_train_task)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"val_acc_true\"].append(val_acc_true)\n",
        "        history[\"val_acc_false\"].append(val_acc_false)\n",
        "        history[\"prompt_norm\"].append(prompt_norm)\n",
        "\n",
        "        print(\n",
        "            f\"[Continuous {'ADV' if adversarial else 'NON-ADV'}] \"\n",
        "            f\"Epoch {epoch+1}/{cfg.num_epochs} | \"\n",
        "            f\"joint={avg_train_joint:.4f} task={avg_train_task:.4f} \"\n",
        "            f\"val_loss={avg_val_loss:.4f} val_acc={val_acc:.4f} \"\n",
        "            f\"(true={val_acc_true:.4f} false={val_acc_false:.4f}) \"\n",
        "            f\"‖prompt‖={prompt_norm:.2f}\"\n",
        "        )\n",
        "\n",
        "    return {\"model\": model, \"history\": history}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ykfVADCa5wCZ"
      },
      "outputs": [],
      "source": [
        "def train_pez(\n",
        "    cfg,\n",
        "    tokenizer: T5TokenizerFast,\n",
        "    gpt2_model: GPT2LMHeadModel,\n",
        "    gpt2_tokenizer: GPT2TokenizerFast,\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader,\n",
        "    lambda_ppl: float,\n",
        "    adversarial: bool,\n",
        "    log_every: int = 50,\n",
        "):\n",
        "    device = cfg.device\n",
        "    base = T5ForConditionalGeneration.from_pretrained(cfg.model_name).to(device)\n",
        "    base.eval()\n",
        "    for p in base.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    model = T5PEZPrompt(base, prompt_length=cfg.prompt_length).to(device)\n",
        "\n",
        "    # --- label ids for flipping (yes/no) ---\n",
        "    if adversarial:\n",
        "        yes_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
        "        no_id  = tokenizer(\"no\",  add_special_tokens=False).input_ids[0]\n",
        "    # --------------------------------------#\n",
        "\n",
        "    history = {\n",
        "        \"lambda_ppl\": lambda_ppl,\n",
        "        \"train_joint\": [],\n",
        "        \"train_task\": [],\n",
        "        \"train_ppl_loss\": [],\n",
        "        \"train_ppl_ppx\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"val_acc_true\": [],\n",
        "        \"val_acc_false\": [],\n",
        "        \"prompt_ppl_ppx\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        model.train()\n",
        "        running_joint = 0.0\n",
        "        running_task = 0.0\n",
        "        running_ppl  = 0.0\n",
        "        running_ppl_ppx = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        empty_ppl_calls = 0\n",
        "        nonempty_ppl_calls = 0\n",
        "\n",
        "        for batch in train_dl:\n",
        "            input_ids      = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "            # ---- flip labels during adversarial TRAINING ----\n",
        "            if adversarial:\n",
        "                labels_flipped = labels.clone()\n",
        "                mask_yes = labels == yes_id\n",
        "                mask_no  = labels == no_id\n",
        "                labels_flipped[mask_yes] = no_id\n",
        "                labels_flipped[mask_no]  = yes_id\n",
        "                labels_for_loss = labels_flipped\n",
        "            else:\n",
        "                labels_for_loss = labels\n",
        "            # -------------------------------------------------#\n",
        "\n",
        "            model.prompt.grad = None\n",
        "\n",
        "            # ---- PEZ Algorithm: Forward with projected prompt ----\n",
        "            # According to PEZ paper: P' = Proj_E(P), then compute L(B(P', X), Y)\n",
        "            # We use projected embeddings (discrete tokens) in forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels_for_loss,\n",
        "                use_projection=True,  # Use projected embeddings: Proj_E(P)\n",
        "            )\n",
        "            task_loss = outputs.loss\n",
        "\n",
        "            # ---- perplexity term ----\n",
        "            if lambda_ppl > 0.0:\n",
        "                prompt_ids = model.get_prompt_token_ids()\n",
        "                prompt_text = tokenizer.decode(\n",
        "                    prompt_ids.tolist(),\n",
        "                    skip_special_tokens=True,\n",
        "                    clean_up_tokenization_spaces=True,\n",
        "                ).strip()\n",
        "\n",
        "                if not prompt_text:\n",
        "                    ppl_loss = torch.tensor(0.0, device=device)\n",
        "                    empty_ppl_calls += 1\n",
        "                else:\n",
        "                    nonempty_ppl_calls += 1\n",
        "                    ppl_loss = compute_prompt_ppl_loss_from_text(\n",
        "                        gpt2_model, gpt2_tokenizer, prompt_text, device=device\n",
        "                    )\n",
        "                    if torch.isnan(ppl_loss) or torch.isinf(ppl_loss):\n",
        "                        ppl_loss = torch.tensor(0.0, device=device)\n",
        "            else:\n",
        "                ppl_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "            joint_loss = task_loss + lambda_ppl * ppl_loss\n",
        "            \n",
        "            # ---- PEZ Algorithm: Compute gradient w.r.t. projected prompt ----\n",
        "            # Since projection (argmax) is non-differentiable, we need to handle gradients\n",
        "            # We use continuous prompt for gradient computation (straight-through estimator)\n",
        "            # Forward used projected, but for backward we need gradients w.r.t. continuous\n",
        "            \n",
        "            # Re-run forward with continuous prompt to get gradient flow\n",
        "            outputs_cont = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels_for_loss,\n",
        "                use_projection=False,  # Use continuous for gradient computation\n",
        "            )\n",
        "            task_loss_cont = outputs_cont.loss\n",
        "            joint_loss_cont = task_loss_cont + lambda_ppl * ppl_loss\n",
        "            joint_loss_cont.backward()\n",
        "            \n",
        "            # ---- PEZ Algorithm: Update continuous prompt ----\n",
        "            # P = P - γ * ∇_P' L, where P' = Proj_E(P)\n",
        "            # Using straight-through: ∇_P' L ≈ ∇_P L (gradient w.r.t. continuous approximates gradient w.r.t. projected)\n",
        "            with torch.no_grad():\n",
        "                if model.prompt.grad is not None:\n",
        "                    # Update: P = P - lr * ∇_P L\n",
        "                    model.prompt.data = model.prompt.data - cfg.lr * model.prompt.grad\n",
        "                    model.prompt.grad.zero_()\n",
        "            # ---------------------------------------------\n",
        "\n",
        "            running_joint += joint_loss.item()\n",
        "            running_task  += task_loss.item()\n",
        "            running_ppl   += ppl_loss.item()\n",
        "            if lambda_ppl > 0.0:\n",
        "                running_ppl_ppx += math.exp(ppl_loss.item())\n",
        "            n_batches += 1\n",
        "\n",
        "            if (n_batches % log_every) == 0:\n",
        "                avg_joint_so_far = running_joint / n_batches\n",
        "                avg_task_so_far  = running_task  / n_batches\n",
        "                avg_ppl_so_far   = running_ppl   / n_batches\n",
        "                avg_ppl_ppx_so_far = (\n",
        "                    running_ppl_ppx / n_batches if lambda_ppl > 0.0 else 0.0\n",
        "                )\n",
        "                print(\n",
        "                    f\"[PEZ λ={lambda_ppl} {'ADV' if adversarial else 'NON-ADV'}] \"\n",
        "                    f\"Epoch {epoch+1}/{cfg.num_epochs}, \"\n",
        "                    f\"batch {n_batches} | \"\n",
        "                    f\"joint={avg_joint_so_far:.4f} \"\n",
        "                    f\"task={avg_task_so_far:.4f} \"\n",
        "                    f\"ppl_loss={avg_ppl_so_far:.4f} \"\n",
        "                    f\"ppl={avg_ppl_ppx_so_far:.2f}\"\n",
        "                )\n",
        "\n",
        "        # ---- end-of-epoch aggregation ----\n",
        "        avg_joint = running_joint / max(1, n_batches)\n",
        "        avg_task  = running_task  / max(1, n_batches)\n",
        "        avg_ppl   = running_ppl   / max(1, n_batches)\n",
        "        avg_ppl_ppx = running_ppl_ppx / max(1, n_batches) if lambda_ppl > 0.0 else 0.0\n",
        "\n",
        "        history[\"train_joint\"].append(avg_joint)\n",
        "        history[\"train_task\"].append(avg_task)\n",
        "        history[\"train_ppl_loss\"].append(avg_ppl)\n",
        "        history[\"train_ppl_ppx\"].append(avg_ppl_ppx)\n",
        "\n",
        "        # ---- validation: TRUE labels ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                input_ids      = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,   # real yes/no labels here\n",
        "                )\n",
        "                val_loss += outputs.loss.item()\n",
        "                val_batches += 1\n",
        "        avg_val_loss = val_loss / max(1, val_batches)\n",
        "        val_acc_dict = evaluate_accuracy_t5(model, val_dl, tokenizer, device)\n",
        "        val_acc = val_acc_dict[\"overall\"]\n",
        "        val_acc_true = val_acc_dict[\"true_acc\"]\n",
        "        val_acc_false = val_acc_dict[\"false_acc\"]\n",
        "\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"val_acc_true\"].append(val_acc_true)\n",
        "        history[\"val_acc_false\"].append(val_acc_false)\n",
        "\n",
        "        # ---- prompt perplexity once per epoch ----\n",
        "        if lambda_ppl > 0.0:\n",
        "            prompt_text_epoch = model.decode_prompt(tokenizer).strip()\n",
        "            if prompt_text_epoch:\n",
        "                ppl_loss_epoch = compute_prompt_ppl_loss_from_text(\n",
        "                    gpt2_model, gpt2_tokenizer, prompt_text_epoch, device=device\n",
        "                )\n",
        "                prompt_ppx = math.exp(ppl_loss_epoch.item())\n",
        "            else:\n",
        "                prompt_ppx = float(\"nan\")\n",
        "        else:\n",
        "            prompt_ppx = 0.0\n",
        "        history[\"prompt_ppl_ppx\"].append(prompt_ppx)\n",
        "\n",
        "        decoded_prompt = model.decode_prompt(tokenizer)\n",
        "        print(\n",
        "            f\"[PEZ λ={lambda_ppl} {'ADV' if adversarial else 'NON-ADV'}] \"\n",
        "            f\"Epoch {epoch+1}/{cfg.num_epochs} | \"\n",
        "            f\"joint={avg_joint:.4f} task={avg_task:.4f} \"\n",
        "            f\"ppl_loss={avg_ppl:.4f} ppl={avg_ppl_ppx:.2f} \"\n",
        "            f\"val_loss={avg_val_loss:.4f} val_acc={val_acc:.4f} \"\n",
        "            f\"(true={val_acc_true:.4f} false={val_acc_false:.4f}) \"\n",
        "            f\"prompt_ppl={prompt_ppx:.2f}\\n\"\n",
        "            f\"Prompt: {decoded_prompt}\"\n",
        "        )\n",
        "\n",
        "    return {\"model\": model, \"history\": history}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# running stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CBs66Bn588X",
        "outputId": "d9d2b657-989a-4a9e-cccd-a3977d08b76b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "cfg = ExperimentConfig()\n",
        "\n",
        "device = cfg.device\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "a17df50c4f204519873f9106027417b7",
            "3b564b95f24b47018a0c061b713fd2f2",
            "537277c120ca4dd495f0e3b948d81ed7",
            "7e3ca244fb6d4a248b3798617f150856",
            "b0a00dfc20db40a3a75a8afde035f4d2",
            "ff2fd178419e48f0a922f7fb354ccaaa",
            "783196c6ba9d4aa4b805d5625f5c9b98",
            "e3cccc52a75343b28f52bc928d3619df",
            "b83c602ff4df496f852b03d7c903988d",
            "5f84e12c24e24469a4d7b0d96cef0f21",
            "b29fe5a23ed44819a1c5d07abd6a6ce7",
            "0c0c7beb4c0e48f19d55760a141462ef",
            "c61ad218e38a4f91bee3d2aba749b418",
            "b10cb7508ac449199e8616cecccfc52c",
            "2efead2471ca4a3bb20607bb896e5eb2",
            "c177a5639a1f4f1e8bb11d7f4cb423b0",
            "086934b5adb1494c82f3cfd2afad0f34",
            "863e5e6b0574490687e34f80d592fe97",
            "a0cda38cd3f34a958bc37682c22e41ac",
            "6f559210513640af97f3766efe23a3a9",
            "a93c739224fe48ae870d8a00619edc9a",
            "ddb0f6c766214bc08d2a4cc6ac162723"
          ]
        },
        "id": "VlXZEBhl59rG",
        "outputId": "ac0c8340-f894-4ff2-a6ce-609192695d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced BoolQ train: 7106 examples (3553 True, 3553 False)\n"
          ]
        }
      ],
      "source": [
        "# Tokenizers and data\n",
        "tokenizer = T5TokenizerFast.from_pretrained(cfg.model_name)\n",
        "train_dl, val_dl = load_boolq_balanced(tokenizer, cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRABVGTW7vZI",
        "outputId": "e8e6f9d8-94bc-4af9-91a0-a2c9ee76ef9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([150,   1,   0,   0])\n",
            "yes_id 4273\n",
            "no_id 150\n",
            "decoded label: no</s><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(val_dl))\n",
        "print(batch[\"labels\"][0][:5])  # first 5 label tokens\n",
        "print(\"yes_id\", tokenizer(\"yes\", add_special_tokens=False).input_ids[0])\n",
        "print(\"no_id\",  tokenizer(\"no\",  add_special_tokens=False).input_ids[0])\n",
        "print(\"decoded label:\", tokenizer.decode(batch[\"labels\"][0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## baseline accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BASELINE: T5 Small (No Soft Prompt)\n",
            "================================================================================\n",
            "\n",
            "Loading t5-small for baseline evaluation...\n",
            "Evaluating baseline model on validation set...\n",
            "\n",
            "================================================================================\n",
            "Baseline Results:\n",
            "================================================================================\n",
            "Overall Accuracy: 0.4624\n",
            "True (yes) Accuracy:  0.2007\n",
            "False (no) Accuracy:  0.8925\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# put baseline here\n",
        "# Baseline accuracy of T5 Large on validation set (no soft prompt tuning)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BASELINE: T5 Small (No Soft Prompt)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load T5 Large model (baseline without any prompt tuning)\n",
        "baseline_model_name = \"t5-small\"  # Use t5-large for baseline\n",
        "print(f\"\\nLoading {baseline_model_name} for baseline evaluation...\")\n",
        "baseline_model = T5ForConditionalGeneration.from_pretrained(baseline_model_name).to(device)\n",
        "baseline_model.eval()\n",
        "for p in baseline_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Evaluating baseline model on validation set...\")\n",
        "baseline_acc_dict = evaluate_accuracy_t5(baseline_model, val_dl, tokenizer, device)\n",
        "\n",
        "baseline_overall = baseline_acc_dict[\"overall\"]\n",
        "baseline_true = baseline_acc_dict[\"true_acc\"]\n",
        "baseline_false = baseline_acc_dict[\"false_acc\"]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Baseline Results:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Overall Accuracy: {baseline_overall:.4f}\")\n",
        "print(f\"True (yes) Accuracy:  {baseline_true:.4f}\")\n",
        "print(f\"False (no) Accuracy:  {baseline_false:.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Store baseline for comparison\n",
        "baseline_results = {\n",
        "    \"model_name\": baseline_model_name,\n",
        "    \"overall_acc\": baseline_overall,\n",
        "    \"true_acc\": baseline_true,\n",
        "    \"false_acc\": baseline_false\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPY-tVcTTF2r"
      },
      "source": [
        "## baselines (no interpretability) with 10 length prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxrpJIeQ6AO4",
        "outputId": "3281a199-215e-4007-f4e9-4a5b05a19f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Continuous Soft Prompt: Non-Adversarial ===\n",
            "[Continuous NON-ADV] Epoch 1/5 | joint=2.3661 task=2.3661 val_loss=0.1970 val_acc=0.5214 (true=0.5967 false=0.3977) ‖prompt‖=2.22\n",
            "[Continuous NON-ADV] Epoch 2/5 | joint=0.7890 task=0.7890 val_loss=0.1749 val_acc=0.5593 (true=0.8032 false=0.1584) ‖prompt‖=2.64\n",
            "[Continuous NON-ADV] Epoch 3/5 | joint=0.5732 task=0.5732 val_loss=0.1902 val_acc=0.4599 (true=0.3443 false=0.6500) ‖prompt‖=2.97\n",
            "[Continuous NON-ADV] Epoch 4/5 | joint=0.4702 task=0.4702 val_loss=0.1745 val_acc=0.6122 (true=0.9734 false=0.0186) ‖prompt‖=3.27\n",
            "[Continuous NON-ADV] Epoch 5/5 | joint=0.4453 task=0.4453 val_loss=0.1848 val_acc=0.6199 (true=0.9966 false=0.0008) ‖prompt‖=3.55\n",
            "\n",
            "=== Continuous Soft Prompt: Adversarial ===\n",
            "[Continuous ADV] Epoch 1/5 | joint=6.5342 task=6.5342 val_loss=1.2764 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 2/5 | joint=3.0816 task=3.0816 val_loss=0.4022 val_acc=0.4009 (true=0.0841 false=0.9216) ‖prompt‖=1.47\n",
            "[Continuous ADV] Epoch 3/5 | joint=2.2973 task=2.2973 val_loss=0.2190 val_acc=0.5122 (true=0.5376 false=0.4705) ‖prompt‖=1.48\n",
            "[Continuous ADV] Epoch 4/5 | joint=1.8743 task=1.8743 val_loss=0.1885 val_acc=0.5838 (true=0.8672 false=0.1180) ‖prompt‖=1.49\n",
            "[Continuous ADV] Epoch 5/5 | joint=1.5848 task=1.5848 val_loss=0.1834 val_acc=0.6159 (true=0.9798 false=0.0178) ‖prompt‖=1.49\n"
          ]
        }
      ],
      "source": [
        "# Continuous soft prompt baselines\n",
        "print(\"\\n=== Continuous Soft Prompt: Non-Adversarial ===\")\n",
        "cont_non_adv = train_continuous_soft_prompt(\n",
        "    cfg, tokenizer, train_dl, val_dl, adversarial=False\n",
        ")\n",
        "\n",
        "print(\"\\n=== Continuous Soft Prompt: Adversarial ===\")\n",
        "cont_adv = train_continuous_soft_prompt(\n",
        "    cfg, tokenizer, train_dl, val_dl, adversarial=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LR gridsearch for non-adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GRID SEARCH: Learning Rates for Continuous Soft Prompt (Non-Adversarial)\n",
            "================================================================================\n",
            "\n",
            "--- Testing LR = 1e-05 ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Continuous NON-ADV] Epoch 1/10 | joint=11.5514 task=11.5514 val_loss=8.4130 val_acc=0.3966 (true=0.0812 false=0.9151) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=9.0340 task=9.0340 val_loss=5.5060 val_acc=0.3948 (true=0.0654 false=0.9361) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=7.2160 task=7.2160 val_loss=4.2161 val_acc=0.4190 (true=0.1574 false=0.8488) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=6.3452 task=6.3452 val_loss=3.4328 val_acc=0.4339 (true=0.2071 false=0.8068) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=5.6695 task=5.6695 val_loss=2.7355 val_acc=0.4618 (true=0.3350 false=0.6702) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=5.2135 task=5.2135 val_loss=2.1152 val_acc=0.5214 (true=0.5332 false=0.5020) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=4.8249 task=4.8249 val_loss=1.6781 val_acc=0.5859 (true=0.8028 false=0.2296) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=4.4413 task=4.4413 val_loss=1.3539 val_acc=0.6049 (true=0.9302 false=0.0703) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=4.1696 task=4.1696 val_loss=1.1448 val_acc=0.6107 (true=0.9562 false=0.0428) ‖prompt‖=1.43\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=3.8545 task=3.8545 val_loss=0.9884 val_acc=0.6183 (true=0.9818 false=0.0210) ‖prompt‖=1.43\n",
            "LR=1e-05 (10 epochs): best_val_acc=0.6183, final_val_acc=0.6183\n",
            "\n",
            "--- Testing LR = 5e-05 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=7.6482 task=7.6482 val_loss=2.6547 val_acc=0.3798 (true=0.0044 false=0.9968) ‖prompt‖=1.44\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=4.5203 task=4.5203 val_loss=1.1798 val_acc=0.3813 (true=0.0162 false=0.9814) ‖prompt‖=1.44\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=3.2877 task=3.2877 val_loss=0.4023 val_acc=0.5379 (true=0.6940 false=0.2813) ‖prompt‖=1.45\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=2.4962 task=2.4962 val_loss=0.2146 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=1.45\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=1.9652 task=1.9652 val_loss=0.1894 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=1.46\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=1.6819 task=1.6819 val_loss=0.1894 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=1.46\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=1.4831 task=1.4831 val_loss=0.1792 val_acc=0.6217 (true=0.9985 false=0.0024) ‖prompt‖=1.47\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=1.3134 task=1.3134 val_loss=0.1792 val_acc=0.6107 (true=0.9675 false=0.0243) ‖prompt‖=1.47\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=1.1496 task=1.1496 val_loss=0.1784 val_acc=0.6107 (true=0.9725 false=0.0162) ‖prompt‖=1.47\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=1.0468 task=1.0468 val_loss=0.1789 val_acc=0.6220 (true=0.9975 false=0.0049) ‖prompt‖=1.47\n",
            "LR=5e-05 (10 epochs): best_val_acc=0.6220, final_val_acc=0.6220\n",
            "\n",
            "--- Testing LR = 0.0001 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=6.4088 task=6.4088 val_loss=1.7455 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=3.2331 task=3.2331 val_loss=0.3690 val_acc=0.3991 (true=0.0708 false=0.9386) ‖prompt‖=1.46\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=2.1807 task=2.1807 val_loss=0.1900 val_acc=0.5893 (true=0.8505 false=0.1601) ‖prompt‖=1.46\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=1.6844 task=1.6844 val_loss=0.1849 val_acc=0.5572 (true=0.7078 false=0.3096) ‖prompt‖=1.47\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=1.3867 task=1.3867 val_loss=0.2043 val_acc=0.4116 (true=0.1613 false=0.8230) ‖prompt‖=1.48\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=1.2516 task=1.2516 val_loss=0.2350 val_acc=0.3847 (true=0.0172 false=0.9887) ‖prompt‖=1.49\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=1.1126 task=1.1126 val_loss=0.2132 val_acc=0.3899 (true=0.0610 false=0.9305) ‖prompt‖=1.49\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=1.0208 task=1.0208 val_loss=0.2318 val_acc=0.3826 (true=0.0074 false=0.9992) ‖prompt‖=1.50\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.9761 task=0.9761 val_loss=0.1796 val_acc=0.5468 (true=0.6621 false=0.3573) ‖prompt‖=1.50\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.9143 task=0.9143 val_loss=0.2083 val_acc=0.3820 (true=0.0300 false=0.9604) ‖prompt‖=1.51\n",
            "LR=0.0001 (10 epochs): best_val_acc=0.5893, final_val_acc=0.3820\n",
            "\n",
            "--- Testing LR = 0.0005 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=3.3224 task=3.3224 val_loss=0.2160 val_acc=0.4648 (true=0.3271 false=0.6912) ‖prompt‖=1.68\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=1.2709 task=1.2709 val_loss=0.1867 val_acc=0.6208 (true=0.9951 false=0.0057) ‖prompt‖=1.83\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=1.0164 task=1.0164 val_loss=0.1815 val_acc=0.5920 (true=0.8864 false=0.1083) ‖prompt‖=1.98\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=0.8677 task=0.8677 val_loss=0.1789 val_acc=0.6049 (true=0.9405 false=0.0534) ‖prompt‖=2.10\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=0.7846 task=0.7846 val_loss=0.1894 val_acc=0.4602 (true=0.3163 false=0.6968) ‖prompt‖=2.22\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=0.6949 task=0.6949 val_loss=0.1764 val_acc=0.5303 (true=0.5844 false=0.4414) ‖prompt‖=2.33\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=0.6609 task=0.6609 val_loss=0.1863 val_acc=0.4257 (true=0.2125 false=0.7761) ‖prompt‖=2.42\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=0.6266 task=0.6266 val_loss=0.1753 val_acc=0.5092 (true=0.5726 false=0.4050) ‖prompt‖=2.52\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.5862 task=0.5862 val_loss=0.1822 val_acc=0.4294 (true=0.2445 false=0.7332) ‖prompt‖=2.61\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.5464 task=0.5464 val_loss=0.1726 val_acc=0.6211 (true=0.9980 false=0.0016) ‖prompt‖=2.69\n",
            "LR=0.0005 (10 epochs): best_val_acc=0.6211, final_val_acc=0.6211\n",
            "\n",
            "--- Testing LR = 0.001 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=2.0971 task=2.0971 val_loss=0.1865 val_acc=0.4529 (true=0.4073 false=0.5279) ‖prompt‖=2.22\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=0.6185 task=0.6185 val_loss=0.1730 val_acc=0.6196 (true=0.9961 false=0.0008) ‖prompt‖=2.67\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=0.4995 task=0.4995 val_loss=0.1696 val_acc=0.6073 (true=0.9518 false=0.0412) ‖prompt‖=3.04\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=0.4643 task=0.4643 val_loss=0.1694 val_acc=0.6205 (true=0.9970 false=0.0016) ‖prompt‖=3.35\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=0.4266 task=0.4266 val_loss=0.1708 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=3.63\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=0.4239 task=0.4239 val_loss=0.1764 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=3.90\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=0.4226 task=0.4226 val_loss=0.1697 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=4.15\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=0.4047 task=0.4047 val_loss=0.2090 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=4.37\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.3927 task=0.3927 val_loss=0.1707 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=4.58\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.4069 task=0.4069 val_loss=0.1711 val_acc=0.5817 (true=0.8790 false=0.0930) ‖prompt‖=4.78\n",
            "LR=0.001 (10 epochs): best_val_acc=0.6217, final_val_acc=0.5817\n",
            "\n",
            "================================================================================\n",
            "Learning Rate Grid Search Results:\n",
            "================================================================================\n",
            "LR=1e-05 (10 epochs): best_val_acc=0.6183, final_val_acc=0.6183\n",
            "LR=5e-05 (10 epochs): best_val_acc=0.6220, final_val_acc=0.6220\n",
            "LR=1e-04 (10 epochs): best_val_acc=0.5893, final_val_acc=0.3820\n",
            "LR=5e-04 (10 epochs): best_val_acc=0.6211, final_val_acc=0.6211\n",
            "LR=1e-03 (10 epochs): best_val_acc=0.6217, final_val_acc=0.5817\n",
            "\n",
            "Best LR: 5e-05 (10 epochs) with best_val_acc=0.6220\n"
          ]
        }
      ],
      "source": [
        "# Grid search over learning rates for continuous soft prompt (non-adversarial)\n",
        "# Using lower learning rates and gradient clipping to prevent collapse\n",
        "# Smaller LRs need more epochs to converge\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GRID SEARCH: Learning Rates for Continuous Soft Prompt (Non-Adversarial)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "lr_grid = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]  # Lower range to prevent collapse\n",
        "continuous_lr_results = []\n",
        "\n",
        "for lr in lr_grid:\n",
        "    print(f\"\\n--- Testing LR = {lr} ---\")\n",
        "    # Create a temporary config with this learning rate\n",
        "    temp_cfg = ExperimentConfig()\n",
        "    temp_cfg.lr = lr\n",
        "    # Use more epochs for smaller learning rates\n",
        "    temp_cfg.num_epochs = 10\n",
        "    \n",
        "    result = train_continuous_soft_prompt(\n",
        "        temp_cfg, tokenizer, train_dl, val_dl, adversarial=False\n",
        "    )\n",
        "    \n",
        "    best_val_acc = max(result[\"history\"][\"val_acc\"])\n",
        "    final_val_acc = result[\"history\"][\"val_acc\"][-1]\n",
        "    continuous_lr_results.append({\n",
        "        \"lr\": lr,\n",
        "        \"num_epochs\": temp_cfg.num_epochs,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "        \"history\": result[\"history\"]\n",
        "    })\n",
        "    print(f\"LR={lr} ({temp_cfg.num_epochs} epochs): best_val_acc={best_val_acc:.4f}, final_val_acc={final_val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Learning Rate Grid Search Results:\")\n",
        "print(\"=\" * 80)\n",
        "for r in continuous_lr_results:\n",
        "    print(f\"LR={r['lr']:.0e} ({r['num_epochs']} epochs): best_val_acc={r['best_val_acc']:.4f}, final_val_acc={r['final_val_acc']:.4f}\")\n",
        "\n",
        "best_lr_result = max(continuous_lr_results, key=lambda x: x[\"best_val_acc\"])\n",
        "print(f\"\\nBest LR: {best_lr_result['lr']:.0e} ({best_lr_result['num_epochs']} epochs) with best_val_acc={best_lr_result['best_val_acc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to /mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_non_adv.json\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "\n",
        "save_path = \"/mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_non_adv.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(continuous_lr_results, f, indent=2)\n",
        "\n",
        "print(\"Saved to\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## lr gridsearch for adversarial prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GRID SEARCH: Learning Rates for Continuous Soft Prompt (Adversarial)\n",
            "================================================================================\n",
            "\n",
            "--- Testing LR = 1e-05 ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Continuous ADV] Epoch 1/10 | joint=13.0710 task=13.0710 val_loss=13.2291 val_acc=0.3957 (true=0.0748 false=0.9232) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 2/10 | joint=12.8019 task=12.8019 val_loss=12.6625 val_acc=0.3920 (true=0.0649 false=0.9297) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 3/10 | joint=12.4954 task=12.4954 val_loss=12.1305 val_acc=0.3917 (true=0.0605 false=0.9361) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 4/10 | joint=12.2576 task=12.2576 val_loss=11.5831 val_acc=0.3905 (true=0.0541 false=0.9434) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 5/10 | joint=11.9486 task=11.9486 val_loss=11.0842 val_acc=0.3908 (true=0.0497 false=0.9515) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 6/10 | joint=11.6819 task=11.6819 val_loss=10.6132 val_acc=0.3875 (true=0.0394 false=0.9596) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 7/10 | joint=11.4364 task=11.4364 val_loss=10.1425 val_acc=0.3829 (true=0.0251 false=0.9709) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 8/10 | joint=11.2416 task=11.2416 val_loss=9.6896 val_acc=0.3826 (true=0.0192 false=0.9798) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 9/10 | joint=10.9584 task=10.9584 val_loss=9.2676 val_acc=0.3813 (true=0.0138 false=0.9854) ‖prompt‖=1.41\n",
            "[Continuous ADV] Epoch 10/10 | joint=10.7473 task=10.7473 val_loss=8.8497 val_acc=0.3786 (true=0.0069 false=0.9895) ‖prompt‖=1.41\n",
            "LR=1e-05 (10 epochs): final_val_acc=0.3786 (true=0.0748, false=0.9895)\n",
            "\n",
            "--- Testing LR = 5e-05 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=12.8173 task=12.8173 val_loss=10.6882 val_acc=0.4511 (true=0.2740 false=0.7421) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 2/10 | joint=10.8891 task=10.8891 val_loss=7.8646 val_acc=0.3960 (true=0.0615 false=0.9458) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 3/10 | joint=9.6579 task=9.6579 val_loss=6.2257 val_acc=0.3810 (true=0.0103 false=0.9903) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 4/10 | joint=8.5996 task=8.5996 val_loss=5.7602 val_acc=0.3786 (true=0.0034 false=0.9951) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 5/10 | joint=7.7866 task=7.7866 val_loss=5.4482 val_acc=0.3774 (true=0.0015 false=0.9951) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 6/10 | joint=7.2310 task=7.2310 val_loss=5.0220 val_acc=0.3777 (true=0.0005 false=0.9976) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 7/10 | joint=6.6823 task=6.6823 val_loss=4.3371 val_acc=0.3780 (true=0.0000 false=0.9992) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 8/10 | joint=6.2494 task=6.2494 val_loss=3.8403 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 9/10 | joint=5.8406 task=5.8406 val_loss=3.4004 val_acc=0.3783 (true=0.0005 false=0.9992) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 10/10 | joint=5.5604 task=5.5604 val_loss=3.0430 val_acc=0.3789 (true=0.0030 false=0.9968) ‖prompt‖=1.44\n",
            "LR=5e-05 (10 epochs): final_val_acc=0.3789 (true=0.2740, false=1.0000)\n",
            "\n",
            "--- Testing LR = 0.0001 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=12.8863 task=12.8863 val_loss=11.6674 val_acc=0.4229 (true=0.1810 false=0.8205) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 2/10 | joint=10.5024 task=10.5024 val_loss=6.7631 val_acc=0.3771 (true=0.0025 false=0.9927) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 3/10 | joint=8.2169 task=8.2169 val_loss=4.5716 val_acc=0.3780 (true=0.0000 false=0.9992) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 4/10 | joint=6.7703 task=6.7703 val_loss=3.9524 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 5/10 | joint=5.8460 task=5.8460 val_loss=3.4083 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 6/10 | joint=5.3187 task=5.3187 val_loss=2.8501 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 7/10 | joint=4.8653 task=4.8653 val_loss=2.3378 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 8/10 | joint=4.5127 task=4.5127 val_loss=1.8353 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 9/10 | joint=4.1340 task=4.1340 val_loss=1.5024 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 10/10 | joint=3.8637 task=3.8637 val_loss=1.1644 val_acc=0.3783 (true=0.0000 false=1.0000) ‖prompt‖=1.44\n",
            "LR=0.0001 (10 epochs): final_val_acc=0.3783 (true=0.1810, false=1.0000)\n",
            "\n",
            "--- Testing LR = 0.0005 ---\n",
            "[Continuous ADV] Epoch 1/8 | joint=8.4191 task=8.4191 val_loss=3.1032 val_acc=0.3774 (true=0.0000 false=0.9976) ‖prompt‖=1.43\n",
            "[Continuous ADV] Epoch 2/8 | joint=4.3461 task=4.3461 val_loss=1.1498 val_acc=0.5287 (true=0.6163 false=0.3848) ‖prompt‖=1.43\n",
            "[Continuous ADV] Epoch 3/8 | joint=3.3329 task=3.3329 val_loss=0.4438 val_acc=0.6180 (true=0.9877 false=0.0105) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 4/8 | joint=2.6826 task=2.6826 val_loss=0.2521 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 5/8 | joint=2.2567 task=2.2567 val_loss=0.2645 val_acc=0.6217 (true=1.0000 false=0.0000) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 6/8 | joint=1.9443 task=1.9443 val_loss=0.2488 val_acc=0.6214 (true=0.9995 false=0.0000) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 7/8 | joint=1.6259 task=1.6259 val_loss=0.1871 val_acc=0.6208 (true=0.9980 false=0.0008) ‖prompt‖=1.46\n",
            "[Continuous ADV] Epoch 8/8 | joint=1.4219 task=1.4219 val_loss=0.1791 val_acc=0.6211 (true=0.9897 false=0.0154) ‖prompt‖=1.46\n",
            "LR=0.0005 (8 epochs): final_val_acc=0.6211 (true=1.0000, false=0.9976)\n",
            "\n",
            "--- Testing LR = 0.001 ---\n",
            "[Continuous ADV] Epoch 1/5 | joint=6.6835 task=6.6835 val_loss=1.0178 val_acc=0.5920 (true=0.8416 false=0.1819) ‖prompt‖=1.44\n",
            "[Continuous ADV] Epoch 2/5 | joint=2.6699 task=2.6699 val_loss=0.1884 val_acc=0.5098 (true=0.5288 false=0.4786) ‖prompt‖=1.45\n",
            "[Continuous ADV] Epoch 3/5 | joint=1.5299 task=1.5299 val_loss=0.1797 val_acc=0.6177 (true=0.9902 false=0.0057) ‖prompt‖=1.46\n",
            "[Continuous ADV] Epoch 4/5 | joint=1.1578 task=1.1578 val_loss=0.1898 val_acc=0.6220 (true=1.0000 false=0.0008) ‖prompt‖=1.47\n",
            "[Continuous ADV] Epoch 5/5 | joint=0.9826 task=0.9826 val_loss=0.1836 val_acc=0.6220 (true=1.0000 false=0.0008) ‖prompt‖=1.47\n",
            "LR=0.001 (5 epochs): final_val_acc=0.6220 (true=1.0000, false=0.4786)\n",
            "Saved to /mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_adv.json\n"
          ]
        }
      ],
      "source": [
        "# Grid search over learning rates for continuous soft prompt (adversarial)\n",
        "# Using lower learning rates and gradient clipping to prevent collapse\n",
        "# Smaller LRs need more epochs to converge\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GRID SEARCH: Learning Rates for Continuous Soft Prompt (Adversarial)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "lr_grid = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]  # Lower range to prevent collapse\n",
        "continuous_lr_results_adv = []\n",
        "\n",
        "for lr in lr_grid:\n",
        "    print(f\"\\n--- Testing LR = {lr} ---\")\n",
        "    # Create a temporary config with this learning rate\n",
        "    temp_cfg = ExperimentConfig()\n",
        "    temp_cfg.lr = lr\n",
        "    # Use more epochs for smaller learning rates\n",
        "    if lr <= 1e-4:\n",
        "        temp_cfg.num_epochs = 10  # More epochs for very small LRs\n",
        "    elif lr <= 5e-4:\n",
        "        temp_cfg.num_epochs = 8   # Moderate epochs for small LRs\n",
        "    else:\n",
        "        temp_cfg.num_epochs = 5   # Default for larger LRs\n",
        "    \n",
        "    result = train_continuous_soft_prompt(\n",
        "        temp_cfg, tokenizer, train_dl, val_dl, adversarial=True\n",
        "    )\n",
        "    \n",
        "    best_val_acc = max(result[\"history\"][\"val_acc\"])\n",
        "    final_val_acc = result[\"history\"][\"val_acc\"][-1]\n",
        "    best_val_acc_true = max(result[\"history\"][\"val_acc_true\"])\n",
        "    best_val_acc_false = max(result[\"history\"][\"val_acc_false\"])\n",
        "    continuous_lr_results_adv.append({\n",
        "        \"lr\": lr,\n",
        "        \"num_epochs\": temp_cfg.num_epochs,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "        \"best_val_acc_true\": best_val_acc_true,\n",
        "        \"best_val_acc_false\": best_val_acc_false,\n",
        "        \"history\": result[\"history\"]\n",
        "    })\n",
        "    print(\n",
        "        f\"LR={lr} ({temp_cfg.num_epochs} epochs): \"\n",
        "        f\"final_val_acc={final_val_acc:.4f} \"\n",
        "        f\"(true={best_val_acc_true:.4f}, false={best_val_acc_false:.4f})\"\n",
        "    )\n",
        "\n",
        "import json, os\n",
        "\n",
        "save_path = \"/mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_adv.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(continuous_lr_results_adv, f, indent=2)\n",
        "\n",
        "print(\"Saved to\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to /mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_adv.json\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "\n",
        "save_path = \"/mnt/polished-lake/home/annabelma/other/results_small/continuous/continuous_lr_results_adv.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(continuous_lr_results_adv, f, indent=2)\n",
        "\n",
        "print(\"Saved to\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid search: Prompt Length for Continuous Soft Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GRID SEARCH: Prompt Lengths for Continuous Soft Prompt (Non-Adversarial)\n",
            "================================================================================\n",
            "\n",
            "--- Testing Prompt Length = 1 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=14.0544 task=14.0544 val_loss=13.2658 val_acc=0.5547 (true=0.3163 false=0.9466) ‖prompt‖=0.62\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=12.8282 task=12.8282 val_loss=12.8821 val_acc=0.5015 (true=0.2189 false=0.9660) ‖prompt‖=0.63\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=12.6152 task=12.6152 val_loss=12.7073 val_acc=0.4483 (true=0.1328 false=0.9669) ‖prompt‖=0.63\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=12.6664 task=12.6664 val_loss=12.3868 val_acc=0.4557 (true=0.1466 false=0.9636) ‖prompt‖=0.63\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=12.4508 task=12.4508 val_loss=11.7772 val_acc=0.4459 (true=0.1417 false=0.9458) ‖prompt‖=0.64\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=12.2701 task=12.2701 val_loss=11.7364 val_acc=0.4532 (true=0.1589 false=0.9369) ‖prompt‖=0.64\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=12.1014 task=12.1014 val_loss=11.7052 val_acc=0.4716 (true=0.1894 false=0.9353) ‖prompt‖=0.65\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=11.9930 task=11.9930 val_loss=11.6582 val_acc=0.4624 (true=0.1820 false=0.9232) ‖prompt‖=0.65\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=11.9799 task=11.9799 val_loss=11.6168 val_acc=0.4691 (true=0.1968 false=0.9167) ‖prompt‖=0.65\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=11.8935 task=11.8935 val_loss=11.5625 val_acc=0.4789 (true=0.2135 false=0.9151) ‖prompt‖=0.65\n",
            "Prompt Length=1: best_val_acc=0.5547, final_val_acc=0.4789 (true=0.3163, false=0.9669) ‖prompt‖=0.65\n",
            "\n",
            "--- Testing Prompt Length = 5 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=14.0617 task=14.0617 val_loss=12.8790 val_acc=0.4734 (true=0.1648 false=0.9806) ‖prompt‖=1.44\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=12.8441 task=12.8441 val_loss=12.4924 val_acc=0.4541 (true=0.1338 false=0.9806) ‖prompt‖=1.44\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=12.5695 task=12.5695 val_loss=12.1691 val_acc=0.4391 (true=0.1067 false=0.9854) ‖prompt‖=1.45\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=12.1981 task=12.1981 val_loss=11.3205 val_acc=0.4391 (true=0.1072 false=0.9846) ‖prompt‖=1.46\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=11.3833 task=11.3833 val_loss=10.0796 val_acc=0.4024 (true=0.0433 false=0.9927) ‖prompt‖=1.47\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=10.5688 task=10.5688 val_loss=9.1419 val_acc=0.3869 (true=0.0152 false=0.9976) ‖prompt‖=1.48\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=10.0501 task=10.0501 val_loss=8.7167 val_acc=0.3893 (true=0.0192 false=0.9976) ‖prompt‖=1.48\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=9.6488 task=9.6488 val_loss=8.2256 val_acc=0.4306 (true=0.0905 false=0.9895) ‖prompt‖=1.49\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=9.2412 task=9.2412 val_loss=7.6371 val_acc=0.5150 (true=0.2469 false=0.9555) ‖prompt‖=1.50\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=8.9310 task=8.9310 val_loss=7.3158 val_acc=0.5676 (true=0.3409 false=0.9402) ‖prompt‖=1.50\n",
            "Prompt Length=5: best_val_acc=0.5676, final_val_acc=0.5676 (true=0.3409, false=0.9976) ‖prompt‖=1.50\n",
            "\n",
            "--- Testing Prompt Length = 20 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=13.7413 task=13.7413 val_loss=10.9732 val_acc=0.4388 (true=0.1141 false=0.9725) ‖prompt‖=2.87\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=10.6710 task=10.6710 val_loss=8.7534 val_acc=0.3914 (true=0.0236 false=0.9960) ‖prompt‖=2.88\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=8.8708 task=8.8708 val_loss=5.9452 val_acc=0.5324 (true=0.3856 false=0.7736) ‖prompt‖=2.89\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=6.7588 task=6.7588 val_loss=2.6585 val_acc=0.3829 (true=0.0152 false=0.9871) ‖prompt‖=2.91\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=4.8951 task=4.8951 val_loss=0.7658 val_acc=0.3908 (true=0.0310 false=0.9822) ‖prompt‖=2.92\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=3.3561 task=3.3561 val_loss=0.5032 val_acc=0.4498 (true=0.1579 false=0.9297) ‖prompt‖=2.93\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=2.4195 task=2.4195 val_loss=0.2298 val_acc=0.5657 (true=0.4860 false=0.6968) ‖prompt‖=2.95\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=1.8177 task=1.8177 val_loss=0.2023 val_acc=0.5388 (true=0.4009 false=0.7656) ‖prompt‖=2.96\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=1.4372 task=1.4372 val_loss=0.1873 val_acc=0.5725 (true=0.5509 false=0.6079) ‖prompt‖=2.98\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=1.1877 task=1.1877 val_loss=0.1614 val_acc=0.6557 (true=0.9021 false=0.2506) ‖prompt‖=2.99\n",
            "Prompt Length=20: best_val_acc=0.6557, final_val_acc=0.6557 (true=0.9021, false=0.9960) ‖prompt‖=2.99\n",
            "\n",
            "--- Testing Prompt Length = 50 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=11.5538 task=11.5538 val_loss=8.2412 val_acc=0.4217 (true=0.1067 false=0.9394) ‖prompt‖=4.57\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=6.9638 task=6.9638 val_loss=1.7342 val_acc=0.5691 (true=0.4058 false=0.8375) ‖prompt‖=4.59\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=3.7193 task=3.7193 val_loss=0.4086 val_acc=0.6823 (true=0.9626 false=0.2215) ‖prompt‖=4.61\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=2.1998 task=2.1998 val_loss=0.1685 val_acc=0.6829 (true=0.9695 false=0.2118) ‖prompt‖=4.63\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=1.4912 task=1.4912 val_loss=0.1831 val_acc=0.6575 (true=0.9951 false=0.1027) ‖prompt‖=4.65\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=1.0669 task=1.0669 val_loss=0.1729 val_acc=0.6639 (true=0.9897 false=0.1285) ‖prompt‖=4.67\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=0.8108 task=0.8108 val_loss=0.2018 val_acc=0.6361 (true=0.9985 false=0.0404) ‖prompt‖=4.70\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=0.6594 task=0.6594 val_loss=0.2947 val_acc=0.6312 (true=0.9995 false=0.0259) ‖prompt‖=4.72\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.5542 task=0.5542 val_loss=0.1585 val_acc=0.6584 (true=0.8529 false=0.3387) ‖prompt‖=4.74\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.4797 task=0.4797 val_loss=0.1975 val_acc=0.6428 (true=0.9980 false=0.0590) ‖prompt‖=4.76\n",
            "Prompt Length=50: best_val_acc=0.6829, final_val_acc=0.6428 (true=0.9995, false=0.9394) ‖prompt‖=4.76\n",
            "\n",
            "--- Testing Prompt Length = 100 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=11.1806 task=11.1806 val_loss=8.8053 val_acc=0.4153 (true=0.1097 false=0.9175) ‖prompt‖=6.45\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=8.3084 task=8.3084 val_loss=3.8385 val_acc=0.3789 (true=0.0030 false=0.9968) ‖prompt‖=6.47\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=3.9116 task=3.9116 val_loss=0.9857 val_acc=0.3792 (true=0.0015 false=1.0000) ‖prompt‖=6.50\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=2.0743 task=2.0743 val_loss=0.4132 val_acc=0.4015 (true=0.0477 false=0.9830) ‖prompt‖=6.53\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=1.2457 task=1.2457 val_loss=0.2240 val_acc=0.4486 (true=0.1677 false=0.9103) ‖prompt‖=6.56\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=0.7681 task=0.7681 val_loss=0.1682 val_acc=0.5942 (true=0.6473 false=0.5069) ‖prompt‖=6.59\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=0.5711 task=0.5711 val_loss=0.1608 val_acc=0.6330 (true=0.8303 false=0.3088) ‖prompt‖=6.62\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=0.4518 task=0.4518 val_loss=0.1574 val_acc=0.6560 (true=0.9769 false=0.1285) ‖prompt‖=6.65\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.3848 task=0.3848 val_loss=0.1609 val_acc=0.6373 (true=0.7585 false=0.4382) ‖prompt‖=6.68\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.3512 task=0.3512 val_loss=0.1573 val_acc=0.6550 (true=0.8672 false=0.3064) ‖prompt‖=6.71\n",
            "Prompt Length=100: best_val_acc=0.6560, final_val_acc=0.6550 (true=0.9769, false=1.0000) ‖prompt‖=6.71\n",
            "\n",
            "--- Testing Prompt Length = 150 ---\n",
            "[Continuous NON-ADV] Epoch 1/10 | joint=10.2871 task=10.2871 val_loss=6.5060 val_acc=0.3823 (true=0.0108 false=0.9927) ‖prompt‖=7.87\n",
            "[Continuous NON-ADV] Epoch 2/10 | joint=5.3562 task=5.3562 val_loss=1.3192 val_acc=0.3985 (true=0.0384 false=0.9903) ‖prompt‖=7.91\n",
            "[Continuous NON-ADV] Epoch 3/10 | joint=2.0788 task=2.0788 val_loss=0.1911 val_acc=0.5456 (true=0.4624 false=0.6823) ‖prompt‖=7.95\n",
            "[Continuous NON-ADV] Epoch 4/10 | joint=1.0063 task=1.0063 val_loss=0.1792 val_acc=0.5728 (true=0.5578 false=0.5974) ‖prompt‖=7.99\n",
            "[Continuous NON-ADV] Epoch 5/10 | joint=0.6182 task=0.6182 val_loss=0.1749 val_acc=0.5385 (true=0.4294 false=0.7179) ‖prompt‖=8.03\n",
            "[Continuous NON-ADV] Epoch 6/10 | joint=0.4423 task=0.4423 val_loss=0.1659 val_acc=0.6067 (true=0.6660 false=0.5093) ‖prompt‖=8.07\n",
            "[Continuous NON-ADV] Epoch 7/10 | joint=0.3636 task=0.3636 val_loss=0.1711 val_acc=0.5563 (true=0.4752 false=0.6896) ‖prompt‖=8.10\n",
            "[Continuous NON-ADV] Epoch 8/10 | joint=0.3278 task=0.3278 val_loss=0.1737 val_acc=0.5294 (true=0.3797 false=0.7753) ‖prompt‖=8.14\n",
            "[Continuous NON-ADV] Epoch 9/10 | joint=0.2974 task=0.2974 val_loss=0.1623 val_acc=0.6257 (true=0.6498 false=0.5861) ‖prompt‖=8.18\n",
            "[Continuous NON-ADV] Epoch 10/10 | joint=0.2719 task=0.2719 val_loss=0.1717 val_acc=0.5306 (true=0.3896 false=0.7623) ‖prompt‖=8.21\n",
            "Prompt Length=150: best_val_acc=0.6257, final_val_acc=0.5306 (true=0.6660, false=0.9927) ‖prompt‖=8.21\n",
            "\n",
            "================================================================================\n",
            "Prompt Length Grid Search Results:\n",
            "================================================================================\n",
            "Length=  1: best_val_acc=0.5547, final_val_acc=0.4789 (true=0.3163, false=0.9669) ‖prompt‖=0.65\n",
            "Length=  5: best_val_acc=0.5676, final_val_acc=0.5676 (true=0.3409, false=0.9976) ‖prompt‖=1.50\n",
            "Length= 20: best_val_acc=0.6557, final_val_acc=0.6557 (true=0.9021, false=0.9960) ‖prompt‖=2.99\n",
            "Length= 50: best_val_acc=0.6829, final_val_acc=0.6428 (true=0.9995, false=0.9394) ‖prompt‖=4.76\n",
            "Length=100: best_val_acc=0.6560, final_val_acc=0.6550 (true=0.9769, false=1.0000) ‖prompt‖=6.71\n",
            "Length=150: best_val_acc=0.6257, final_val_acc=0.5306 (true=0.6660, false=0.9927) ‖prompt‖=8.21\n",
            "\n",
            "Best Prompt Length: 50\n",
            "  best_val_acc=0.6829, final_val_acc=0.6428\n",
            "  true_acc=0.9995, false_acc=0.9394\n"
          ]
        }
      ],
      "source": [
        "# Grid search over prompt lengths for continuous soft prompt (non-adversarial)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GRID SEARCH: Prompt Lengths for Continuous Soft Prompt (Non-Adversarial)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompt_length_grid = [1, 5, 20, 50, 100, 150]\n",
        "prompt_length_results = []\n",
        "\n",
        "for prompt_len in prompt_length_grid:\n",
        "    print(f\"\\n--- Testing Prompt Length = {prompt_len} ---\")\n",
        "    # Create a temporary config with this prompt length\n",
        "    temp_cfg = ExperimentConfig()\n",
        "    temp_cfg.prompt_length = prompt_len\n",
        "    # Use a reasonable learning rate (from previous grid search, 5e-5 was good)\n",
        "    temp_cfg.lr = 1e-4\n",
        "    temp_cfg.num_epochs = 10  # Use enough epochs to see convergence\n",
        "    \n",
        "    result = train_continuous_soft_prompt(\n",
        "        temp_cfg, tokenizer, train_dl, val_dl, adversarial=False\n",
        "    )\n",
        "    \n",
        "    best_val_acc = max(result[\"history\"][\"val_acc\"])\n",
        "    final_val_acc = result[\"history\"][\"val_acc\"][-1]\n",
        "    best_val_acc_true = max(result[\"history\"][\"val_acc_true\"])\n",
        "    best_val_acc_false = max(result[\"history\"][\"val_acc_false\"])\n",
        "    final_prompt_norm = result[\"history\"][\"prompt_norm\"][-1]\n",
        "    \n",
        "    prompt_length_results.append({\n",
        "        \"prompt_length\": prompt_len,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "        \"best_val_acc_true\": best_val_acc_true,\n",
        "        \"best_val_acc_false\": best_val_acc_false,\n",
        "        \"final_prompt_norm\": final_prompt_norm,\n",
        "        \"history\": result[\"history\"]\n",
        "    })\n",
        "    print(\n",
        "        f\"Prompt Length={prompt_len}: \"\n",
        "        f\"best_val_acc={best_val_acc:.4f}, final_val_acc={final_val_acc:.4f} \"\n",
        "        f\"(true={best_val_acc_true:.4f}, false={best_val_acc_false:.4f}) \"\n",
        "        f\"‖prompt‖={final_prompt_norm:.2f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Prompt Length Grid Search Results:\")\n",
        "print(\"=\" * 80)\n",
        "for r in prompt_length_results:\n",
        "    print(\n",
        "        f\"Length={r['prompt_length']:3d}: \"\n",
        "        f\"best_val_acc={r['best_val_acc']:.4f}, final_val_acc={r['final_val_acc']:.4f} \"\n",
        "        f\"(true={r['best_val_acc_true']:.4f}, false={r['best_val_acc_false']:.4f}) \"\n",
        "        f\"‖prompt‖={r['final_prompt_norm']:.2f}\"\n",
        "    )\n",
        "\n",
        "best_prompt_length_result = max(prompt_length_results, key=lambda x: x[\"best_val_acc\"])\n",
        "print(f\"\\nBest Prompt Length: {best_prompt_length_result['prompt_length']}\")\n",
        "print(f\"  best_val_acc={best_prompt_length_result['best_val_acc']:.4f}, final_val_acc={best_prompt_length_result['final_val_acc']:.4f}\")\n",
        "print(f\"  true_acc={best_prompt_length_result['best_val_acc_true']:.4f}, false_acc={best_prompt_length_result['best_val_acc_false']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## continuous soft prompt length, adversarial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GRID SEARCH: Prompt Lengths for Continuous Soft Prompt Adversarial\n",
            "================================================================================\n",
            "\n",
            "--- Testing Prompt Length = 1 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=15.1931 task=15.1931 val_loss=14.0680 val_acc=0.6226 (true=0.4348 false=0.9313) ‖prompt‖=0.64\n",
            "[Continuous ADV] Epoch 2/10 | joint=13.8522 task=13.8522 val_loss=13.4714 val_acc=0.5780 (true=0.3581 false=0.9394) ‖prompt‖=0.64\n",
            "[Continuous ADV] Epoch 3/10 | joint=13.3964 task=13.3964 val_loss=13.1242 val_acc=0.5468 (true=0.3055 false=0.9434) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 4/10 | joint=13.0043 task=13.0043 val_loss=12.9168 val_acc=0.5147 (true=0.2410 false=0.9644) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 5/10 | joint=12.8887 task=12.8887 val_loss=12.7184 val_acc=0.4636 (true=0.1604 false=0.9620) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 6/10 | joint=12.7957 task=12.7957 val_loss=12.6969 val_acc=0.4343 (true=0.1087 false=0.9693) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 7/10 | joint=12.8064 task=12.8064 val_loss=11.9074 val_acc=0.4379 (true=0.1176 false=0.9644) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 8/10 | joint=12.8080 task=12.8080 val_loss=11.8363 val_acc=0.4370 (true=0.1181 false=0.9612) ‖prompt‖=0.63\n",
            "[Continuous ADV] Epoch 9/10 | joint=12.5282 task=12.5282 val_loss=11.8089 val_acc=0.4407 (true=0.1264 false=0.9572) ‖prompt‖=0.64\n",
            "[Continuous ADV] Epoch 10/10 | joint=12.3344 task=12.3344 val_loss=11.7897 val_acc=0.4398 (true=0.1289 false=0.9507) ‖prompt‖=0.64\n",
            "Prompt Length=1: best_val_acc=0.4343, final_val_acc=0.4398 (true=0.1087, false=0.9313) ‖prompt‖=0.64\n",
            "\n",
            "--- Testing Prompt Length = 5 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=16.7223 task=16.7223 val_loss=15.3835 val_acc=0.6676 (true=0.5317 false=0.8909) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 2/10 | joint=13.9439 task=13.9439 val_loss=13.0635 val_acc=0.5220 (true=0.2563 false=0.9588) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 3/10 | joint=13.1096 task=13.1096 val_loss=12.8569 val_acc=0.4813 (true=0.1840 false=0.9701) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 4/10 | joint=12.8954 task=12.8954 val_loss=12.6978 val_acc=0.4657 (true=0.1540 false=0.9782) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 5/10 | joint=12.7170 task=12.7170 val_loss=12.5072 val_acc=0.4581 (true=0.1407 false=0.9798) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 6/10 | joint=12.5099 task=12.5099 val_loss=12.1870 val_acc=0.4566 (true=0.1382 false=0.9798) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 7/10 | joint=12.2801 task=12.2801 val_loss=11.9511 val_acc=0.4514 (true=0.1289 false=0.9814) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 8/10 | joint=12.1270 task=12.1270 val_loss=11.6682 val_acc=0.4327 (true=0.0949 false=0.9879) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 9/10 | joint=11.8934 task=11.8934 val_loss=11.3986 val_acc=0.4229 (true=0.0777 false=0.9903) ‖prompt‖=1.42\n",
            "[Continuous ADV] Epoch 10/10 | joint=11.6213 task=11.6213 val_loss=10.5216 val_acc=0.4245 (true=0.0797 false=0.9911) ‖prompt‖=1.42\n",
            "Prompt Length=5: best_val_acc=0.4229, final_val_acc=0.4245 (true=0.0777, false=0.8909) ‖prompt‖=1.42\n",
            "\n",
            "--- Testing Prompt Length = 10 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=16.1749 task=16.1749 val_loss=16.7386 val_acc=0.7159 (true=0.6099 false=0.8901) ‖prompt‖=2.00\n",
            "[Continuous ADV] Epoch 2/10 | joint=14.0225 task=14.0225 val_loss=13.4169 val_acc=0.5410 (true=0.2907 false=0.9523) ‖prompt‖=2.00\n",
            "[Continuous ADV] Epoch 3/10 | joint=12.4471 task=12.4471 val_loss=11.1260 val_acc=0.4688 (true=0.1594 false=0.9774) ‖prompt‖=2.00\n",
            "[Continuous ADV] Epoch 4/10 | joint=11.3567 task=11.3567 val_loss=9.4101 val_acc=0.4089 (true=0.0531 false=0.9935) ‖prompt‖=2.00\n",
            "[Continuous ADV] Epoch 5/10 | joint=10.0990 task=10.0990 val_loss=8.3036 val_acc=0.3838 (true=0.0093 false=0.9992) ‖prompt‖=2.00\n",
            "[Continuous ADV] Epoch 6/10 | joint=9.3075 task=9.3075 val_loss=7.0297 val_acc=0.4508 (true=0.1377 false=0.9652) ‖prompt‖=2.01\n",
            "[Continuous ADV] Epoch 7/10 | joint=8.5384 task=8.5384 val_loss=5.7264 val_acc=0.5853 (true=0.4142 false=0.8666) ‖prompt‖=2.01\n",
            "[Continuous ADV] Epoch 8/10 | joint=7.6195 task=7.6195 val_loss=4.2540 val_acc=0.6746 (true=0.6370 false=0.7365) ‖prompt‖=2.01\n",
            "[Continuous ADV] Epoch 9/10 | joint=6.7193 task=6.7193 val_loss=2.7366 val_acc=0.7009 (true=0.7595 false=0.6047) ‖prompt‖=2.01\n",
            "[Continuous ADV] Epoch 10/10 | joint=5.9428 task=5.9428 val_loss=1.3347 val_acc=0.7159 (true=0.8982 false=0.4163) ‖prompt‖=2.02\n",
            "Prompt Length=10: best_val_acc=0.3838, final_val_acc=0.7159 (true=0.0093, false=0.4163) ‖prompt‖=2.02\n",
            "\n",
            "--- Testing Prompt Length = 20 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=14.2911 task=14.2911 val_loss=13.4811 val_acc=0.5596 (true=0.3689 false=0.8731) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 2/10 | joint=12.1881 task=12.1881 val_loss=11.7610 val_acc=0.4269 (true=0.1048 false=0.9563) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 3/10 | joint=11.5151 task=11.5151 val_loss=10.4668 val_acc=0.4009 (true=0.0472 false=0.9822) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 4/10 | joint=11.0050 task=11.0050 val_loss=9.8168 val_acc=0.4113 (true=0.0644 false=0.9814) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 5/10 | joint=10.4846 task=10.4846 val_loss=8.8535 val_acc=0.4190 (true=0.0787 false=0.9782) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 6/10 | joint=9.9426 task=9.9426 val_loss=8.0367 val_acc=0.4061 (true=0.0571 false=0.9798) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 7/10 | joint=9.3567 task=9.3567 val_loss=7.0251 val_acc=0.4064 (true=0.0585 false=0.9782) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 8/10 | joint=8.7528 task=8.7528 val_loss=6.2898 val_acc=0.4471 (true=0.1471 false=0.9402) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 9/10 | joint=7.9532 task=7.9532 val_loss=4.8636 val_acc=0.5012 (true=0.2568 false=0.9030) ‖prompt‖=2.89\n",
            "[Continuous ADV] Epoch 10/10 | joint=7.0752 task=7.0752 val_loss=3.5230 val_acc=0.6040 (true=0.5401 false=0.7090) ‖prompt‖=2.90\n",
            "Prompt Length=20: best_val_acc=0.4009, final_val_acc=0.6040 (true=0.0472, false=0.7090) ‖prompt‖=2.90\n",
            "\n",
            "--- Testing Prompt Length = 50 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=12.9883 task=12.9883 val_loss=11.3245 val_acc=0.5284 (true=0.4142 false=0.7162) ‖prompt‖=4.53\n",
            "[Continuous ADV] Epoch 2/10 | joint=11.1876 task=11.1876 val_loss=9.6496 val_acc=0.4003 (true=0.0531 false=0.9709) ‖prompt‖=4.54\n",
            "[Continuous ADV] Epoch 3/10 | joint=10.1648 task=10.1648 val_loss=8.4772 val_acc=0.3786 (true=0.0069 false=0.9895) ‖prompt‖=4.54\n",
            "[Continuous ADV] Epoch 4/10 | joint=9.2247 task=9.2247 val_loss=6.9591 val_acc=0.3933 (true=0.0359 false=0.9806) ‖prompt‖=4.54\n",
            "[Continuous ADV] Epoch 5/10 | joint=8.0129 task=8.0129 val_loss=5.0938 val_acc=0.3853 (true=0.0167 false=0.9911) ‖prompt‖=4.54\n",
            "[Continuous ADV] Epoch 6/10 | joint=6.5831 task=6.5831 val_loss=2.0069 val_acc=0.3985 (true=0.0497 false=0.9717) ‖prompt‖=4.55\n",
            "[Continuous ADV] Epoch 7/10 | joint=5.0793 task=5.0793 val_loss=0.9412 val_acc=0.3810 (true=0.0103 false=0.9903) ‖prompt‖=4.55\n",
            "[Continuous ADV] Epoch 8/10 | joint=3.9019 task=3.9019 val_loss=0.7012 val_acc=0.4450 (true=0.1667 false=0.9022) ‖prompt‖=4.56\n",
            "[Continuous ADV] Epoch 9/10 | joint=2.8935 task=2.8935 val_loss=0.4855 val_acc=0.5688 (true=0.5450 false=0.6079) ‖prompt‖=4.56\n",
            "[Continuous ADV] Epoch 10/10 | joint=2.3372 task=2.3372 val_loss=0.4091 val_acc=0.6248 (true=0.7836 false=0.3638) ‖prompt‖=4.57\n",
            "Prompt Length=50: best_val_acc=0.3786, final_val_acc=0.6248 (true=0.0069, false=0.3638) ‖prompt‖=4.57\n",
            "\n",
            "--- Testing Prompt Length = 100 ---\n",
            "[Continuous ADV] Epoch 1/10 | joint=11.7717 task=11.7717 val_loss=9.8226 val_acc=0.3865 (true=0.0290 false=0.9741) ‖prompt‖=6.41\n",
            "[Continuous ADV] Epoch 2/10 | joint=9.5608 task=9.5608 val_loss=7.1442 val_acc=0.3939 (true=0.0487 false=0.9612) ‖prompt‖=6.41\n",
            "[Continuous ADV] Epoch 3/10 | joint=7.3474 task=7.3474 val_loss=3.4233 val_acc=0.4517 (true=0.2415 false=0.7971) ‖prompt‖=6.42\n",
            "[Continuous ADV] Epoch 4/10 | joint=5.0622 task=5.0622 val_loss=1.0224 val_acc=0.6101 (true=0.8106 false=0.2805) ‖prompt‖=6.42\n",
            "[Continuous ADV] Epoch 5/10 | joint=3.3461 task=3.3461 val_loss=0.4361 val_acc=0.6278 (true=0.9798 false=0.0493) ‖prompt‖=6.43\n",
            "[Continuous ADV] Epoch 6/10 | joint=2.2292 task=2.2292 val_loss=0.2591 val_acc=0.6061 (true=0.7988 false=0.2894) ‖prompt‖=6.44\n",
            "[Continuous ADV] Epoch 7/10 | joint=1.5143 task=1.5143 val_loss=0.2230 val_acc=0.5743 (true=0.7054 false=0.3589) ‖prompt‖=6.44\n",
            "[Continuous ADV] Epoch 8/10 | joint=1.0715 task=1.0715 val_loss=0.2067 val_acc=0.4300 (true=0.1545 false=0.8828) ‖prompt‖=6.45\n"
          ]
        }
      ],
      "source": [
        "# Grid search over prompt lengths for continuous soft prompt (non-adversarial)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GRID SEARCH: Prompt Lengths for Continuous Soft Prompt Adversarial\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompt_length_grid = [1, 5, 10, 20, 50, 100, 150]\n",
        "prompt_length_results_adv = []\n",
        "\n",
        "for prompt_len in prompt_length_grid:\n",
        "    print(f\"\\n--- Testing Prompt Length = {prompt_len} ---\")\n",
        "    # Create a temporary config with this prompt length\n",
        "    temp_cfg = ExperimentConfig()\n",
        "    temp_cfg.prompt_length = prompt_len\n",
        "    temp_cfg.lr = 5e-4 # effective lr is 5e-5 \n",
        "    temp_cfg.num_epochs = 10  \n",
        "    \n",
        "    result = train_continuous_soft_prompt(\n",
        "        temp_cfg, tokenizer, train_dl, val_dl, adversarial=True\n",
        "    )\n",
        "    \n",
        "    best_val_acc = min(result[\"history\"][\"val_acc\"])\n",
        "    final_val_acc = result[\"history\"][\"val_acc\"][-1]\n",
        "    best_val_acc_true = min(result[\"history\"][\"val_acc_true\"])\n",
        "    best_val_acc_false = min(result[\"history\"][\"val_acc_false\"])\n",
        "    final_prompt_norm = result[\"history\"][\"prompt_norm\"][-1]\n",
        "    \n",
        "    prompt_length_results_adv.append({\n",
        "        \"prompt_length\": prompt_len,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "        \"best_val_acc_true\": best_val_acc_true,\n",
        "        \"best_val_acc_false\": best_val_acc_false,\n",
        "        \"final_prompt_norm\": final_prompt_norm,\n",
        "        \"history\": result[\"history\"]\n",
        "    })\n",
        "    print(\n",
        "        f\"Prompt Length={prompt_len}: \"\n",
        "        f\"best_val_acc={best_val_acc:.4f}, final_val_acc={final_val_acc:.4f} \"\n",
        "        f\"(true={best_val_acc_true:.4f}, false={best_val_acc_false:.4f}) \"\n",
        "        f\"‖prompt‖={final_prompt_norm:.2f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Prompt Length Grid Search Results:\")\n",
        "print(\"=\" * 80)\n",
        "for r in prompt_length_results_adv:\n",
        "    print(\n",
        "        f\"Length={r['prompt_length']:3d}: \"\n",
        "        f\"best_val_acc={r['best_val_acc']:.4f}, final_val_acc={r['final_val_acc']:.4f} \"\n",
        "        f\"(true={r['best_val_acc_true']:.4f}, false={r['best_val_acc_false']:.4f}) \"\n",
        "        f\"‖prompt‖={r['final_prompt_norm']:.2f}\"\n",
        "    )\n",
        "\n",
        "best_prompt_length_result = min(prompt_length_results_adv, key=lambda x: x[\"best_val_acc\"])\n",
        "print(f\"\\nBest Prompt Length: {best_prompt_length_result['prompt_length']}\")\n",
        "print(f\"  best_val_acc={best_prompt_length_result['best_val_acc']:.4f}, final_val_acc={best_prompt_length_result['final_val_acc']:.4f}\")\n",
        "print(f\"  true_acc={best_prompt_length_result['best_val_acc_true']:.4f}, false_acc={best_prompt_length_result['best_val_acc_false']:.4f}\")\n",
        "\n",
        "import json, os\n",
        "\n",
        "save_path = \"/mnt/polished-lake/home/annabelma/other/results/continuous/prompt_length_results_adv.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(prompt_length_results_adv, f, indent=2)\n",
        "\n",
        "print(\"Saved to\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU1Ebf5JTJBJ"
      },
      "source": [
        "## gpt2 for prompt perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "b6cea69db3f948d4a87de89ba2219766",
            "a7b84fbc86d549d2b2d920b6e3e3f0e5",
            "80cd3377e1114fc0a2867d7ddfc139b1",
            "d58c3b603b5a4da3a3a7d9ebf688da03",
            "20243da2ac294c3aacaf5d772e3949b4",
            "1770a56348fd46faa7dda98fd6e469a1",
            "8d015465f6ef45a3a2523436c980115c",
            "5d25e89862664bdf9da8514b83074871",
            "470fc6596292403eb23f95be83990ca4",
            "b264639e6ab844b2b9e82a12d4d416aa",
            "e59b649c3ab24a4c8eee1a467f43bd87",
            "14da7a720d9149bebc10df04372e1e1a",
            "9d7599aa26904213bd54da21678646a4",
            "34377ca0f40c40c79dc38262f42acc16",
            "71556d7b286c4276a13f1b2d59e38c90",
            "e0855a8f0f544de2bd5bc5d08715ae52",
            "e2873877eb504826955030022536b519",
            "5f9bd90305a34f1ca9144149d4ba4730",
            "c2981cf4f6944050937be182e2d5a689",
            "3e58c98d7d1b4985b70746e03ed024d4",
            "53f3a7e017154ef98643eeaa464f8f48",
            "eb3d0d180d2e4a038b9fcf9980415f92",
            "d8b46c27ef8043f595b98f4325e209ad",
            "215ed698f92440d6a46917aadac3c1c7",
            "1e51377579ba4abfa992c83c1f6cb813",
            "1282c30469414c189d85af443093d9e9",
            "8fdf5679f14f4dce88cc699459e4ed79",
            "e7713e97d01641be959c80f7f026bdbf",
            "fe10d19e5dc54962830988ae3e6a998e",
            "43d518174b1f453088029f68b3b6a8e7",
            "13d38c560176434eb7a59499fc2973d2",
            "21836b672c2043fa921d15e889089787",
            "5b8b5720210447fdb3a94847a1fa2009",
            "8309bfc4dafb44acaf77c95898e29cca",
            "b076905342de477fae2624ff76fd3882",
            "9fc1258416a349c7a9762b600e85b223",
            "9e3b3ee5f9024d56ac18dfeda824dba6",
            "b20e90d9cfb847b7bfc0b42a48b2177e",
            "00b47d8727e74846b0f34f5965c1e594",
            "1d7caa60ce2e4e8cabd76070d280423b",
            "d7f08ad6930247f7afe82da892cf36f1",
            "51a225dc368549bdb1a5afcade5afdc5",
            "4cb307737e234c1680af072581479112",
            "4d752fa70b2740e68d3e4edd94e22fda",
            "fb0254c92eb444729fca77fa6d1ef4cf",
            "f74e787f1ed147c19fb487a581973321",
            "e0d9e9075bcf460b85dd4a67ff2641c9",
            "d9c22dc8fb8a4b5089e10d11d15860c0",
            "4a600e223f82494fb6d68e9bf3e0254a",
            "39557a0229844ffa8de50a9311459fab",
            "d4a756c485ed480abdfe51b4301d1e54",
            "74a02027c1c54ea9831abcf5eb237a5e",
            "11f0dbd8f0094ee286a5d9fea27b6022",
            "7a2b7ab2ab7947d0968f00ae131435ff",
            "967502b1a3514750975abe7dd9fb3eff",
            "4342b8d1363941288490c78ba8f9f0bc",
            "632812daab0d4d4b9b0d175989fff0c8",
            "d1072b96a26d410fa10ebf460b27560e",
            "dad7a2483bc74bd280fb5c330a5a717b",
            "f7b5c045bc3e45898e4aed56024949a9",
            "6cd283a1d6494809ab3b765c265576db",
            "ed7aeb2009b1403582f0f227cff4e501",
            "4fba7520145449a590054610593b8339",
            "c7477dbb47fb435fb346d77f02d608be",
            "3cfdcc6ee0024e4c9d196d1de4c5b015",
            "2fd09996546e4f2baada745c4db3a97f",
            "8d5ca3b68ec44c4d8280112c23209a9e",
            "9ff27408b9644fb0b98c3124531750fa",
            "d5797b4514b6495a9be7fbe2cc8a3813",
            "085b923e86d04f5eb55cb22d6e2c90fe",
            "52a9351b0faa4f2aacb9bf49ece031fb",
            "ddd74b2f17234311a64acd5ad4825181",
            "55d1b5a100854d0589bbd3587273bcfd",
            "f32d7cb08ca24cdf8ada2ef29a808686",
            "83e3eff2246d409eaa4da41eebe9cb2c",
            "863c0a415aee414898b4642b5e8ab66d",
            "50b3ede950b743c2b7fbb95e6af6b034"
          ]
        },
        "id": "nBkVkbCm6DO7",
        "outputId": "ad9dc167-4de3-4be3-8271-58f3ca0c2db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading GPT-2 for prompt perplexity...\n"
          ]
        }
      ],
      "source": [
        "# GPT-2 for prompt perplexity\n",
        "print(\"\\nLoading GPT-2 for prompt perplexity...\")\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(cfg.gpt2_name)\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(cfg.gpt2_name).to(device)\n",
        "gpt2_model.eval()\n",
        "for p in gpt2_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pez non adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNu8UcNgdH-R",
        "outputId": "c557eb1f-723c-429e-b8ba-aaca93e211d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PEZ Non-Adversarial (λ=0.0) ===\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 50 | joint=16.2253 task=16.2253 ppl_loss=0.0000 ppl=0.00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 100 | joint=16.4650 task=16.4650 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 150 | joint=16.4386 task=16.4386 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 200 | joint=16.4043 task=16.4043 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 250 | joint=16.3259 task=16.3259 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 300 | joint=16.3679 task=16.3679 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 350 | joint=16.3726 task=16.3726 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 400 | joint=16.4019 task=16.4019 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5 | joint=16.3984 task=16.3984 ppl_loss=0.0000 ppl=0.00 val_loss=20.9880 val_acc=0.7920 (true=0.7969 false=0.7842) prompt_ppl=0.00\n",
            "Prompt: beginnerenberg Rü maxewusstsurviving Spar Camill Liege inaltime\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 50 | joint=16.3584 task=16.3584 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 100 | joint=16.2475 task=16.2475 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 150 | joint=16.3027 task=16.3027 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 200 | joint=16.2943 task=16.2943 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 250 | joint=16.2946 task=16.2946 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 300 | joint=16.3248 task=16.3248 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 350 | joint=16.3196 task=16.3196 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 400 | joint=16.3207 task=16.3207 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5 | joint=16.2969 task=16.2969 ppl_loss=0.0000 ppl=0.00 val_loss=21.7522 val_acc=0.7933 (true=0.7983 false=0.7850) prompt_ppl=0.00\n",
            "Prompt: informatiihoppertura jpl aceaarratefan Jim iritvette\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 50 | joint=16.2502 task=16.2502 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 100 | joint=16.3092 task=16.3092 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 150 | joint=16.2879 task=16.2879 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 200 | joint=16.2607 task=16.2607 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 250 | joint=16.2801 task=16.2801 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 300 | joint=16.3076 task=16.3076 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 350 | joint=16.3059 task=16.3059 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 400 | joint=16.2994 task=16.2994 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5 | joint=16.2928 task=16.2928 ppl_loss=0.0000 ppl=0.00 val_loss=21.3574 val_acc=0.7924 (true=0.7959 false=0.7866) prompt_ppl=0.00\n",
            "Prompt: riel institutiiAsociația Nativeruc pârRhône cautare opera Fra\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 50 | joint=16.0953 task=16.0953 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 100 | joint=16.2324 task=16.2324 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 150 | joint=16.1946 task=16.1946 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 200 | joint=16.2170 task=16.2170 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 250 | joint=16.2322 task=16.2322 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 300 | joint=16.2524 task=16.2524 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 350 | joint=16.2614 task=16.2614 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 400 | joint=16.2480 task=16.2480 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5 | joint=16.2400 task=16.2400 ppl_loss=0.0000 ppl=0.00 val_loss=21.1885 val_acc=0.7881 (true=0.7875 false=0.7890) prompt_ppl=0.00\n",
            "Prompt: Karlsruhe producatorkett intelege senzor HBO loljudețul Crichant\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 50 | joint=16.1083 task=16.1083 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 100 | joint=16.1178 task=16.1178 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 150 | joint=16.2187 task=16.2187 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 200 | joint=16.2476 task=16.2476 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 250 | joint=16.2551 task=16.2551 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 300 | joint=16.2485 task=16.2485 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 350 | joint=16.2672 task=16.2672 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 400 | joint=16.2747 task=16.2747 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5 | joint=16.2611 task=16.2611 ppl_loss=0.0000 ppl=0.00 val_loss=21.2621 val_acc=0.7878 (true=0.7914 false=0.7817) prompt_ppl=0.00\n",
            "Prompt: concession prevazutnjeuxALS GérardconcentrGénéraleEditura poat caz\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.01) ===\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 50 | joint=16.2088 task=16.1196 ppl_loss=8.9195 ppl=11329.07\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 100 | joint=16.3186 task=16.2300 ppl_loss=8.8558 ppl=11873.84\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 150 | joint=16.2732 task=16.1846 ppl_loss=8.8572 ppl=11523.02\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 200 | joint=16.2759 task=16.1878 ppl_loss=8.8100 ppl=11096.84\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 250 | joint=16.3393 task=16.2512 ppl_loss=8.8063 ppl=11224.27\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 300 | joint=16.3466 task=16.2591 ppl_loss=8.7574 ppl=11085.59\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 350 | joint=16.3352 task=16.2477 ppl_loss=8.7544 ppl=10796.59\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 400 | joint=16.3327 task=16.2451 ppl_loss=8.7531 ppl=10777.77\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5 | joint=16.3611 task=16.2738 ppl_loss=8.7280 ppl=10380.89 val_loss=20.6317 val_acc=0.7942 (true=0.8023 false=0.7809) prompt_ppl=14170.37\n",
            "Prompt: Digitalisierung situatii GalrgicDatorita shellfie cadouri piataitzer\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 50 | joint=16.3786 task=16.2914 ppl_loss=8.7222 ppl=7834.14\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 100 | joint=16.4065 task=16.3211 ppl_loss=8.5396 ppl=7570.14\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 150 | joint=16.3379 task=16.2519 ppl_loss=8.5952 ppl=9085.51\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 200 | joint=16.3857 task=16.3002 ppl_loss=8.5444 ppl=8723.58\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 250 | joint=16.3757 task=16.2902 ppl_loss=8.5416 ppl=8977.48\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 300 | joint=16.3835 task=16.2983 ppl_loss=8.5195 ppl=8961.27\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 350 | joint=16.4038 task=16.3188 ppl_loss=8.4945 ppl=8461.25\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 400 | joint=16.4028 task=16.3178 ppl_loss=8.5017 ppl=8532.36\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5 | joint=16.3749 task=16.2894 ppl_loss=8.5536 ppl=9350.71 val_loss=20.3797 val_acc=0.7856 (true=0.7850 false=0.7866) prompt_ppl=9013.89\n",
            "Prompt: fie spalatliter solutieutilizatorii proaspat Glen Dixon Schlworm\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 50 | joint=16.4075 task=16.3208 ppl_loss=8.6733 ppl=8823.92\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 100 | joint=16.3829 task=16.2961 ppl_loss=8.6721 ppl=8731.05\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 150 | joint=16.3587 task=16.2724 ppl_loss=8.6380 ppl=8672.12\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 200 | joint=16.3199 task=16.2331 ppl_loss=8.6767 ppl=9867.37\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 250 | joint=16.3223 task=16.2353 ppl_loss=8.6968 ppl=9727.03\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 300 | joint=16.3340 task=16.2471 ppl_loss=8.6911 ppl=9958.25\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 350 | joint=16.3361 task=16.2491 ppl_loss=8.6964 ppl=9901.53\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 400 | joint=16.3437 task=16.2570 ppl_loss=8.6682 ppl=9549.63\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5 | joint=16.3813 task=16.2950 ppl_loss=8.6303 ppl=9139.01 val_loss=20.9787 val_acc=0.7930 (true=0.7914 false=0.7955) prompt_ppl=98562.63\n",
            "Prompt: cantitatebourg Knoxcaster Klu Schalt unravel modeUVProdusele\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 50 | joint=16.5533 task=16.4661 ppl_loss=8.7165 ppl=9941.52\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 100 | joint=16.4933 task=16.4070 ppl_loss=8.6285 ppl=9064.90\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 150 | joint=16.4556 task=16.3695 ppl_loss=8.6115 ppl=9313.36\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 200 | joint=16.4225 task=16.3359 ppl_loss=8.6608 ppl=10174.68\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 250 | joint=16.4046 task=16.3180 ppl_loss=8.6611 ppl=9866.71\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 300 | joint=16.3819 task=16.2953 ppl_loss=8.6514 ppl=9574.85\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 350 | joint=16.3579 task=16.2715 ppl_loss=8.6328 ppl=9386.66\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 400 | joint=16.3340 task=16.2474 ppl_loss=8.6580 ppl=9681.45\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5 | joint=16.3306 task=16.2442 ppl_loss=8.6460 ppl=9969.20 val_loss=21.9209 val_acc=0.7914 (true=0.7855 false=0.8011) prompt_ppl=2245.81\n",
            "Prompt: ATS Sprech dorintphéMuzeul Britanie compartimentBiserica Bahrain ciocolat\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 50 | joint=16.4028 task=16.3146 ppl_loss=8.8206 ppl=35114.80\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 100 | joint=16.4103 task=16.3230 ppl_loss=8.7272 ppl=22284.61\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 150 | joint=16.4604 task=16.3736 ppl_loss=8.6864 ppl=17999.60\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 200 | joint=16.4526 task=16.3662 ppl_loss=8.6441 ppl=16571.96\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 250 | joint=16.4407 task=16.3539 ppl_loss=8.6781 ppl=15302.59\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 300 | joint=16.4313 task=16.3443 ppl_loss=8.6943 ppl=14945.19\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 350 | joint=16.4052 task=16.3184 ppl_loss=8.6823 ppl=14089.43\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 400 | joint=16.4052 task=16.3187 ppl_loss=8.6540 ppl=13212.91\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5 | joint=16.3994 task=16.3128 ppl_loss=8.6656 ppl=13025.32 val_loss=19.5979 val_acc=0.7957 (true=0.8042 false=0.7817) prompt_ppl=713.04\n",
            "Prompt: pu AlfaculturaItalie bumbac Vilăsesc végétalarilechel\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.05) ===\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 50 | joint=16.7591 task=16.3443 ppl_loss=8.2971 ppl=8115.98\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 100 | joint=16.7590 task=16.3290 ppl_loss=8.6008 ppl=10593.93\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 150 | joint=16.8219 task=16.3911 ppl_loss=8.6163 ppl=10701.93\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 200 | joint=16.7771 task=16.3483 ppl_loss=8.5766 ppl=9779.11\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 250 | joint=16.7300 task=16.3015 ppl_loss=8.5699 ppl=9484.23\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 300 | joint=16.7758 task=16.3483 ppl_loss=8.5504 ppl=9443.38\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 350 | joint=16.7658 task=16.3368 ppl_loss=8.5816 ppl=9787.49\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 400 | joint=16.7489 task=16.3185 ppl_loss=8.6072 ppl=9644.22\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5 | joint=16.7593 task=16.3281 ppl_loss=8.6239 ppl=9862.33 val_loss=21.8224 val_acc=0.7966 (true=0.8067 false=0.7801) prompt_ppl=5498.90\n",
            "Prompt: jurnal reprezintachette Alexrigg Daca Printr Kwarack decat\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 50 | joint=16.9225 task=16.4956 ppl_loss=8.5378 ppl=8923.91\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 100 | joint=16.7527 task=16.3309 ppl_loss=8.4361 ppl=7523.98\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 150 | joint=16.8029 task=16.3797 ppl_loss=8.4648 ppl=7397.04\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 200 | joint=16.7890 task=16.3633 ppl_loss=8.5152 ppl=8242.40\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 250 | joint=16.7836 task=16.3530 ppl_loss=8.6119 ppl=9020.79\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 300 | joint=16.7240 task=16.2906 ppl_loss=8.6685 ppl=9891.08\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 350 | joint=16.7367 task=16.3042 ppl_loss=8.6485 ppl=9873.50\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 400 | joint=16.7184 task=16.2854 ppl_loss=8.6598 ppl=9696.88\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5 | joint=16.7230 task=16.2905 ppl_loss=8.6501 ppl=9557.13 val_loss=21.5895 val_acc=0.7905 (true=0.7885 false=0.7939) prompt_ppl=4558.66\n",
            "Prompt: ceilaltiMessage organizatoriAsociația peisaj Carson Archiv tweakancygardinen\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 50 | joint=16.8479 task=16.4068 ppl_loss=8.8215 ppl=10118.71\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 100 | joint=16.8267 task=16.3898 ppl_loss=8.7379 ppl=11396.65\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 150 | joint=16.8518 task=16.4180 ppl_loss=8.6752 ppl=10070.28\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 200 | joint=16.8163 task=16.3824 ppl_loss=8.6783 ppl=10007.10\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 250 | joint=16.7908 task=16.3565 ppl_loss=8.6872 ppl=9837.42\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 300 | joint=16.8126 task=16.3778 ppl_loss=8.6968 ppl=10089.22\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 350 | joint=16.8193 task=16.3835 ppl_loss=8.7151 ppl=10204.20\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 400 | joint=16.8338 task=16.3995 ppl_loss=8.6871 ppl=10104.49\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5 | joint=16.8213 task=16.3887 ppl_loss=8.6532 ppl=9700.56 val_loss=22.1967 val_acc=0.7924 (true=0.7978 false=0.7833) prompt_ppl=17242.17\n",
            "Prompt: liteEdituraPI formularul Singerpartyromânii augustgasesc bubble\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 50 | joint=16.8787 task=16.4409 ppl_loss=8.7566 ppl=9948.50\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 100 | joint=16.8481 task=16.4156 ppl_loss=8.6484 ppl=9592.32\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 150 | joint=16.8694 task=16.4315 ppl_loss=8.7564 ppl=11703.41\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 200 | joint=16.8224 task=16.3861 ppl_loss=8.7278 ppl=10827.34\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 250 | joint=16.8384 task=16.4012 ppl_loss=8.7444 ppl=11201.31\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 300 | joint=16.8230 task=16.3861 ppl_loss=8.7374 ppl=10755.16\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 350 | joint=16.7733 task=16.3367 ppl_loss=8.7324 ppl=10681.90\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 400 | joint=16.7593 task=16.3218 ppl_loss=8.7486 ppl=10829.18\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5 | joint=16.7597 task=16.3237 ppl_loss=8.7194 ppl=10542.04 val_loss=21.7898 val_acc=0.7939 (true=0.7993 false=0.7850) prompt_ppl=3766.19\n",
            "Prompt: dispozitiv Sä calatoriFUiolo proprietatirânEdituraBiserica Berkeley\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 50 | joint=16.5497 task=16.1102 ppl_loss=8.7899 ppl=9545.96\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 100 | joint=16.6870 task=16.2570 ppl_loss=8.6001 ppl=8051.28\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 150 | joint=16.7005 task=16.2664 ppl_loss=8.6833 ppl=9516.82\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 200 | joint=16.7317 task=16.2997 ppl_loss=8.6399 ppl=8983.91\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 250 | joint=16.6896 task=16.2571 ppl_loss=8.6499 ppl=9059.53\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 300 | joint=16.6892 task=16.2563 ppl_loss=8.6588 ppl=9840.92\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 350 | joint=16.7107 task=16.2775 ppl_loss=8.6630 ppl=9955.62\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 400 | joint=16.6950 task=16.2604 ppl_loss=8.6927 ppl=10058.92\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5 | joint=16.6981 task=16.2627 ppl_loss=8.7082 ppl=10245.72 val_loss=20.4036 val_acc=0.7960 (true=0.8091 false=0.7745) prompt_ppl=6750.71\n",
            "Prompt: collect proaspat bijuteriiutilizatoriibor suprafaț OliveoaseANCCamere\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.1) ===\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 50 | joint=17.3947 task=16.5183 ppl_loss=8.7640 ppl=10284.84\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 100 | joint=17.1653 task=16.2920 ppl_loss=8.7329 ppl=10322.31\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 150 | joint=17.1947 task=16.3224 ppl_loss=8.7228 ppl=11235.43\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 200 | joint=17.2125 task=16.3346 ppl_loss=8.7784 ppl=11777.31\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 250 | joint=17.1879 task=16.3064 ppl_loss=8.8151 ppl=12068.04\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 300 | joint=17.1806 task=16.2983 ppl_loss=8.8227 ppl=11863.73\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 350 | joint=17.1616 task=16.2799 ppl_loss=8.8171 ppl=11759.67\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 400 | joint=17.1706 task=16.2919 ppl_loss=8.7868 ppl=11420.27\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5 | joint=17.1919 task=16.3131 ppl_loss=8.7877 ppl=11508.50 val_loss=21.5276 val_acc=0.7969 (true=0.8116 false=0.7728) prompt_ppl=10731.85\n",
            "Prompt: Marriott Sfant temporarrickCAPNATflipped fericire șofersko\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 50 | joint=17.1722 task=16.3027 ppl_loss=8.6950 ppl=10989.19\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 100 | joint=17.2993 task=16.4320 ppl_loss=8.6730 ppl=13207.86\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 150 | joint=17.1818 task=16.3208 ppl_loss=8.6103 ppl=11160.89\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 200 | joint=17.1778 task=16.3124 ppl_loss=8.6549 ppl=11478.93\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 250 | joint=17.1838 task=16.3230 ppl_loss=8.6071 ppl=10630.41\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 300 | joint=17.1808 task=16.3231 ppl_loss=8.5777 ppl=10253.30\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 350 | joint=17.1842 task=16.3239 ppl_loss=8.6027 ppl=10083.62\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 400 | joint=17.1807 task=16.3173 ppl_loss=8.6340 ppl=10266.52\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5 | joint=17.1861 task=16.3213 ppl_loss=8.6487 ppl=10412.45 val_loss=22.0951 val_acc=0.7908 (true=0.7988 false=0.7777) prompt_ppl=8026.06\n",
            "Prompt: mărturi acceleratorceapaTotusiCartea ventMod dimineatanumarul obtine\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 50 | joint=17.2754 task=16.4416 ppl_loss=8.3379 ppl=5958.71\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 100 | joint=17.1584 task=16.3170 ppl_loss=8.4137 ppl=7264.27\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 150 | joint=17.2389 task=16.3872 ppl_loss=8.5173 ppl=7817.22\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 200 | joint=17.1750 task=16.3162 ppl_loss=8.5880 ppl=8768.59\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 250 | joint=17.1614 task=16.2988 ppl_loss=8.6260 ppl=9518.55\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 300 | joint=17.1745 task=16.3108 ppl_loss=8.6376 ppl=10132.99\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 350 | joint=17.1687 task=16.3066 ppl_loss=8.6210 ppl=9826.08\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 400 | joint=17.1467 task=16.2811 ppl_loss=8.6556 ppl=10078.45\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5 | joint=17.1368 task=16.2717 ppl_loss=8.6516 ppl=10060.56 val_loss=21.4068 val_acc=0.7917 (true=0.7831 false=0.8060) prompt_ppl=1104.49\n",
            "Prompt: Târgu formularul senzati„carui clientilor cărți ingrijiredies excursi\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 50 | joint=17.3584 task=16.4759 ppl_loss=8.8247 ppl=14017.93\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 100 | joint=17.3173 task=16.4435 ppl_loss=8.7379 ppl=11274.67\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 150 | joint=17.3407 task=16.4686 ppl_loss=8.7211 ppl=10739.82\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 200 | joint=17.3010 task=16.4323 ppl_loss=8.6871 ppl=10363.04\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 250 | joint=17.2534 task=16.3869 ppl_loss=8.6655 ppl=10537.20\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 300 | joint=17.2250 task=16.3585 ppl_loss=8.6653 ppl=10256.27\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 350 | joint=17.2403 task=16.3758 ppl_loss=8.6453 ppl=9816.66\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 400 | joint=17.2212 task=16.3557 ppl_loss=8.6555 ppl=10236.91\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5 | joint=17.2327 task=16.3663 ppl_loss=8.6641 ppl=10153.86 val_loss=20.4139 val_acc=0.7801 (true=0.7447 false=0.8383) prompt_ppl=7904.72\n",
            "Prompt: Madrid atenti prosecutioncie imbunatatiKT răc ferrypermalinkare\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 50 | joint=17.0278 task=16.1547 ppl_loss=8.7310 ppl=9786.88\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 100 | joint=17.0428 task=16.1823 ppl_loss=8.6050 ppl=10075.16\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 150 | joint=17.0703 task=16.1993 ppl_loss=8.7103 ppl=13415.16\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 200 | joint=17.1600 task=16.2911 ppl_loss=8.6885 ppl=12378.44\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 250 | joint=17.1836 task=16.3208 ppl_loss=8.6282 ppl=11098.55\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 300 | joint=17.1552 task=16.2934 ppl_loss=8.6172 ppl=11044.56\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 350 | joint=17.1493 task=16.2878 ppl_loss=8.6153 ppl=10539.12\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 400 | joint=17.1483 task=16.2860 ppl_loss=8.6235 ppl=10416.17\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5 | joint=17.1469 task=16.2857 ppl_loss=8.6119 ppl=10494.18 val_loss=22.4182 val_acc=0.7972 (true=0.8210 false=0.7583) prompt_ppl=5061.08\n",
            "Prompt: aparatulgrijaloyer Cause senzati frunzeuntaCartearobolass\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.25) ===\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 50 | joint=18.4414 task=16.2374 ppl_loss=8.8162 ppl=12654.85\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 100 | joint=18.5331 task=16.3541 ppl_loss=8.7162 ppl=10294.23\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 150 | joint=18.5331 task=16.3529 ppl_loss=8.7205 ppl=9675.93\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 200 | joint=18.5296 task=16.3542 ppl_loss=8.7016 ppl=9595.45\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 250 | joint=18.5403 task=16.3643 ppl_loss=8.7041 ppl=9509.27\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 300 | joint=18.5381 task=16.3623 ppl_loss=8.7033 ppl=9606.08\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 350 | joint=18.5132 task=16.3344 ppl_loss=8.7152 ppl=9616.39\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 400 | joint=18.5157 task=16.3421 ppl_loss=8.6942 ppl=9451.44\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5 | joint=18.4949 task=16.3185 ppl_loss=8.7057 ppl=9463.30 val_loss=20.7432 val_acc=0.7924 (true=0.7939 false=0.7898) prompt_ppl=2654.03\n",
            "Prompt: evrei prevazutporestehende pamant TârguOTA counties Um corpul\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 50 | joint=18.5539 task=16.3665 ppl_loss=8.7495 ppl=9499.67\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 100 | joint=18.5172 task=16.3571 ppl_loss=8.6404 ppl=8990.86\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 150 | joint=18.5453 task=16.3761 ppl_loss=8.6767 ppl=8984.20\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 200 | joint=18.5312 task=16.3810 ppl_loss=8.6006 ppl=8346.08\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 250 | joint=18.5270 task=16.3605 ppl_loss=8.6661 ppl=9695.40\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 300 | joint=18.4831 task=16.3221 ppl_loss=8.6441 ppl=9434.82\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 350 | joint=18.4724 task=16.3014 ppl_loss=8.6838 ppl=11152.73\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 400 | joint=18.4905 task=16.3167 ppl_loss=8.6952 ppl=11076.54\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5 | joint=18.4453 task=16.2763 ppl_loss=8.6758 ppl=10615.13 val_loss=21.7550 val_acc=0.7991 (true=0.8101 false=0.7809) prompt_ppl=18004.31\n",
            "Prompt: handed Kick Guvernului Nici RileyDAYEdituraromâniiOBelle\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 50 | joint=18.4304 task=16.2781 ppl_loss=8.6094 ppl=11196.03\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 100 | joint=18.4337 task=16.2823 ppl_loss=8.6056 ppl=9441.57\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 150 | joint=18.4584 task=16.2987 ppl_loss=8.6387 ppl=9865.97\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 200 | joint=18.4755 task=16.3281 ppl_loss=8.5894 ppl=9524.71\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 250 | joint=18.5143 task=16.3822 ppl_loss=8.5283 ppl=8772.69\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 300 | joint=18.5239 task=16.3859 ppl_loss=8.5519 ppl=8871.20\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 350 | joint=18.5250 task=16.3826 ppl_loss=8.5694 ppl=8929.95\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 400 | joint=18.5136 task=16.3688 ppl_loss=8.5794 ppl=8970.86\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5 | joint=18.5026 task=16.3525 ppl_loss=8.6003 ppl=9062.64 val_loss=21.4822 val_acc=0.8003 (true=0.8308 false=0.7502) prompt_ppl=4193.19\n",
            "Prompt: pacate5\" reusit billetoxidoileadus lucrariuploadshnlich\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 50 | joint=18.0247 task=15.8721 ppl_loss=8.6102 ppl=10217.59\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 100 | joint=18.2536 task=16.1082 ppl_loss=8.5814 ppl=9422.93\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 150 | joint=18.2692 task=16.1076 ppl_loss=8.6464 ppl=10084.23\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 200 | joint=18.2724 task=16.1182 ppl_loss=8.6168 ppl=9539.06\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 250 | joint=18.3676 task=16.2119 ppl_loss=8.6226 ppl=9514.86\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 300 | joint=18.3820 task=16.2276 ppl_loss=8.6176 ppl=9060.55\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 350 | joint=18.4364 task=16.2777 ppl_loss=8.6350 ppl=8994.89\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 400 | joint=18.4445 task=16.2874 ppl_loss=8.6283 ppl=9013.96\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5 | joint=18.4659 task=16.3072 ppl_loss=8.6348 ppl=9090.49 val_loss=21.3204 val_acc=0.8009 (true=0.8160 false=0.7761) prompt_ppl=8874.94\n",
            "Prompt: INFObol MAI VIôt Mission201me Rügue\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 50 | joint=18.5269 task=16.4014 ppl_loss=8.5021 ppl=7701.00\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 100 | joint=18.6246 task=16.4972 ppl_loss=8.5097 ppl=8550.17\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 150 | joint=18.5213 task=16.3846 ppl_loss=8.5469 ppl=8946.44\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 200 | joint=18.5386 task=16.3991 ppl_loss=8.5580 ppl=8849.43\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 250 | joint=18.5456 task=16.4014 ppl_loss=8.5768 ppl=8822.42\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 300 | joint=18.5252 task=16.3779 ppl_loss=8.5892 ppl=9096.24\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 350 | joint=18.5051 task=16.3597 ppl_loss=8.5816 ppl=9003.89\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 400 | joint=18.4858 task=16.3328 ppl_loss=8.6117 ppl=9370.29\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5 | joint=18.4816 task=16.3221 ppl_loss=8.6381 ppl=9535.09 val_loss=20.7772 val_acc=0.7994 (true=0.8057 false=0.7890) prompt_ppl=16366.06\n",
            "Prompt: gravel afla senzor proaspatbugLAS seal kettlelique Row\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.5) ===\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 50 | joint=20.5634 task=16.0980 ppl_loss=8.9308 ppl=12143.06\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 100 | joint=20.5569 task=16.1721 ppl_loss=8.7696 ppl=10323.69\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 150 | joint=20.6060 task=16.1904 ppl_loss=8.8313 ppl=11531.58\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 200 | joint=20.6218 task=16.2427 ppl_loss=8.7583 ppl=11559.89\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 250 | joint=20.6747 task=16.3091 ppl_loss=8.7313 ppl=12013.57\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 300 | joint=20.6622 task=16.3169 ppl_loss=8.6905 ppl=11326.72\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 350 | joint=20.6814 task=16.3369 ppl_loss=8.6890 ppl=10868.32\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 400 | joint=20.6633 task=16.3252 ppl_loss=8.6761 ppl=10584.34\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5 | joint=20.6729 task=16.3217 ppl_loss=8.7025 ppl=10766.67 val_loss=21.3056 val_acc=0.7924 (true=0.7880 false=0.7995) prompt_ppl=2892.78\n",
            "Prompt: inaltime căt cazare corp autonomePalatul peisajANCzar impozit\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 50 | joint=20.8728 task=16.4783 ppl_loss=8.7890 ppl=11454.61\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 100 | joint=20.6533 task=16.2778 ppl_loss=8.7509 ppl=11421.31\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 150 | joint=20.7521 task=16.3516 ppl_loss=8.8010 ppl=11095.42\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 200 | joint=20.7385 task=16.3458 ppl_loss=8.7854 ppl=10401.46\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 250 | joint=20.7448 task=16.3603 ppl_loss=8.7690 ppl=10133.72\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 300 | joint=20.7064 task=16.3336 ppl_loss=8.7456 ppl=10031.79\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 350 | joint=20.6891 task=16.3165 ppl_loss=8.7453 ppl=9914.98\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 400 | joint=20.6662 task=16.3098 ppl_loss=8.7128 ppl=9649.65\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5 | joint=20.6626 task=16.3071 ppl_loss=8.7111 ppl=9591.53 val_loss=21.5592 val_acc=0.7936 (true=0.7954 false=0.7906) prompt_ppl=11489.64\n",
            "Prompt: spital Schlagcrunui spoil piele prevazutiblylopidio\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 50 | joint=20.7964 task=16.5078 ppl_loss=8.5771 ppl=7319.14\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 100 | joint=20.5809 task=16.3139 ppl_loss=8.5340 ppl=7355.37\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 150 | joint=20.5811 task=16.2854 ppl_loss=8.5913 ppl=8409.48\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 200 | joint=20.6087 task=16.3004 ppl_loss=8.6166 ppl=8329.02\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 250 | joint=20.6342 task=16.3289 ppl_loss=8.6108 ppl=8237.61\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 300 | joint=20.6174 task=16.3265 ppl_loss=8.5820 ppl=8209.99\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 350 | joint=20.6501 task=16.3385 ppl_loss=8.6232 ppl=8829.94\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 400 | joint=20.6401 task=16.3264 ppl_loss=8.6273 ppl=8669.93\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5 | joint=20.6309 task=16.3157 ppl_loss=8.6304 ppl=8635.94 val_loss=21.9305 val_acc=0.7960 (true=0.8082 false=0.7761) prompt_ppl=16416.00\n",
            "Prompt: mediculDameLouisgardionism gib spațiu VandfoliGF\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 50 | joint=21.0768 task=16.5868 ppl_loss=8.9800 ppl=11435.81\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 100 | joint=20.8480 task=16.4839 ppl_loss=8.7281 ppl=9388.14\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 150 | joint=20.8072 task=16.4597 ppl_loss=8.6949 ppl=9955.75\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 200 | joint=20.7913 task=16.4746 ppl_loss=8.6335 ppl=9307.59\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 250 | joint=20.7912 task=16.4558 ppl_loss=8.6707 ppl=9595.46\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 300 | joint=20.7640 task=16.4335 ppl_loss=8.6611 ppl=9294.39\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 350 | joint=20.7340 task=16.3985 ppl_loss=8.6710 ppl=9929.14\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 400 | joint=20.7150 task=16.3899 ppl_loss=8.6500 ppl=9667.29\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5 | joint=20.7231 task=16.4033 ppl_loss=8.6396 ppl=9587.23 val_loss=21.7798 val_acc=0.8003 (true=0.8210 false=0.7664) prompt_ppl=16518.23\n",
            "Prompt: Constanța160graphievremea picioareancies ciel permittinggleAsadar\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 50 | joint=20.6357 task=16.3709 ppl_loss=8.5296 ppl=7058.18\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 100 | joint=20.6498 task=16.4131 ppl_loss=8.4734 ppl=7161.48\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 150 | joint=20.6015 task=16.3308 ppl_loss=8.5414 ppl=7514.30\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 200 | joint=20.6112 task=16.3098 ppl_loss=8.6029 ppl=9139.25\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 250 | joint=20.6116 task=16.3187 ppl_loss=8.5857 ppl=9084.77\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 300 | joint=20.5833 task=16.3038 ppl_loss=8.5591 ppl=8824.76\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 350 | joint=20.5921 task=16.3006 ppl_loss=8.5830 ppl=9210.64\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 400 | joint=20.6115 task=16.3213 ppl_loss=8.5805 ppl=9137.71\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5 | joint=20.5941 task=16.3054 ppl_loss=8.5774 ppl=9125.27 val_loss=22.1222 val_acc=0.7936 (true=0.8037 false=0.7769) prompt_ppl=8815.34\n",
            "Prompt: achiziti urmari functioneazaBuenos compozitidatorita Testrivmentedbou\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=0.75) ===\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 50 | joint=22.8277 task=16.3040 ppl_loss=8.6983 ppl=9200.77\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 100 | joint=22.9596 task=16.3726 ppl_loss=8.7827 ppl=12697.96\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 150 | joint=22.9019 task=16.3306 ppl_loss=8.7616 ppl=11842.90\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 200 | joint=22.9252 task=16.3732 ppl_loss=8.7360 ppl=11226.97\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 250 | joint=22.9166 task=16.3924 ppl_loss=8.6989 ppl=10567.78\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 300 | joint=22.8820 task=16.3534 ppl_loss=8.7048 ppl=11393.69\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 350 | joint=22.8749 task=16.3335 ppl_loss=8.7218 ppl=11347.45\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 400 | joint=22.8493 task=16.3274 ppl_loss=8.6959 ppl=10962.12\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5 | joint=22.8128 task=16.2907 ppl_loss=8.6961 ppl=10714.61 val_loss=20.7310 val_acc=0.7899 (true=0.7919 false=0.7866) prompt_ppl=1342.26\n",
            "Prompt: syndicpermalink inchis bucătări calatori rugam orizont Stadi spui Jocuri\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 50 | joint=22.8317 task=16.4528 ppl_loss=8.5053 ppl=8399.03\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 100 | joint=22.9243 task=16.4156 ppl_loss=8.6783 ppl=11445.27\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 150 | joint=22.8661 task=16.3171 ppl_loss=8.7320 ppl=12067.15\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 200 | joint=22.7713 task=16.2918 ppl_loss=8.6394 ppl=11060.32\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 250 | joint=22.7286 task=16.2581 ppl_loss=8.6273 ppl=10548.03\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 300 | joint=22.7546 task=16.2765 ppl_loss=8.6374 ppl=10378.74\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 350 | joint=22.7756 task=16.2760 ppl_loss=8.6662 ppl=10508.53\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 400 | joint=22.8187 task=16.2993 ppl_loss=8.6926 ppl=10557.24\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5 | joint=22.8259 task=16.2943 ppl_loss=8.7089 ppl=10756.39 val_loss=21.8206 val_acc=0.7997 (true=0.8165 false=0.7720) prompt_ppl=33103.76\n",
            "Prompt: dureriSW cursurisoleLASures adaptiveHCBisericaCamp\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 50 | joint=22.9401 task=16.4604 ppl_loss=8.6397 ppl=8868.61\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 100 | joint=22.8791 task=16.3046 ppl_loss=8.7661 ppl=10737.87\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 150 | joint=22.8531 task=16.3625 ppl_loss=8.6541 ppl=9154.71\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 200 | joint=22.8601 task=16.3637 ppl_loss=8.6618 ppl=9343.00\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 250 | joint=22.7846 task=16.3156 ppl_loss=8.6253 ppl=9070.09\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 300 | joint=22.7747 task=16.3120 ppl_loss=8.6170 ppl=9064.05\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 350 | joint=22.8302 task=16.3407 ppl_loss=8.6527 ppl=9688.27\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 400 | joint=22.8569 task=16.3529 ppl_loss=8.6721 ppl=10004.11\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5 | joint=22.8756 task=16.3391 ppl_loss=8.7153 ppl=10329.80 val_loss=21.4180 val_acc=0.7920 (true=0.7978 false=0.7825) prompt_ppl=3679.54\n",
            "Prompt: utzvertKonzern Bosch AtlasaturaChancelloryland cantitati Emp\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 50 | joint=22.8101 task=16.3505 ppl_loss=8.6128 ppl=7497.85\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 100 | joint=22.7477 task=16.2414 ppl_loss=8.6751 ppl=9111.83\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 150 | joint=22.7501 task=16.2178 ppl_loss=8.7097 ppl=9586.05\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 200 | joint=22.6962 task=16.1983 ppl_loss=8.6638 ppl=9100.04\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 250 | joint=22.7169 task=16.2240 ppl_loss=8.6572 ppl=9280.24\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 300 | joint=22.7575 task=16.2492 ppl_loss=8.6776 ppl=9467.85\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 350 | joint=22.7665 task=16.2656 ppl_loss=8.6680 ppl=9236.29\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 400 | joint=22.7659 task=16.2643 ppl_loss=8.6688 ppl=9067.65\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5 | joint=22.7122 task=16.2357 ppl_loss=8.6352 ppl=8845.31 val_loss=19.5448 val_acc=0.7914 (true=0.7900 false=0.7939) prompt_ppl=13389.36\n",
            "Prompt: SH renunt Danishnev coloan uscatänd viteze holders projection\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 50 | joint=22.7163 task=16.3112 ppl_loss=8.5402 ppl=9397.80\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 100 | joint=22.9263 task=16.4524 ppl_loss=8.6318 ppl=9107.89\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 150 | joint=22.9188 task=16.4574 ppl_loss=8.6152 ppl=8876.53\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 200 | joint=22.8373 task=16.4085 ppl_loss=8.5718 ppl=8329.89\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 250 | joint=22.8231 task=16.4064 ppl_loss=8.5557 ppl=8583.73\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 300 | joint=22.8489 task=16.3763 ppl_loss=8.6301 ppl=9384.13\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 350 | joint=22.8793 task=16.3834 ppl_loss=8.6612 ppl=10116.00\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 400 | joint=22.9142 task=16.4007 ppl_loss=8.6847 ppl=10484.48\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5 | joint=22.9004 task=16.3803 ppl_loss=8.6935 ppl=10336.73 val_loss=21.8841 val_acc=0.8012 (true=0.8111 false=0.7850) prompt_ppl=5632.18\n",
            "Prompt: Pel nascut portofoliucript iubescmaj preferat cablu mancarehowcasing\n",
            "\n",
            "=== PEZ Non-Adversarial (λ=1.0) ===\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 50 | joint=24.8445 task=16.2891 ppl_loss=8.5554 ppl=7577.14\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 100 | joint=24.8772 task=16.2684 ppl_loss=8.6087 ppl=8207.95\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 150 | joint=24.9239 task=16.3413 ppl_loss=8.5826 ppl=7885.68\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 200 | joint=24.9024 task=16.3276 ppl_loss=8.5749 ppl=8183.50\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 250 | joint=24.9293 task=16.3368 ppl_loss=8.5925 ppl=8856.02\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 300 | joint=24.8951 task=16.2819 ppl_loss=8.6132 ppl=8801.31\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 350 | joint=24.9088 task=16.2914 ppl_loss=8.6174 ppl=8700.79\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 400 | joint=24.9067 task=16.3068 ppl_loss=8.5999 ppl=8532.60\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5 | joint=24.9020 task=16.3128 ppl_loss=8.5892 ppl=8480.30 val_loss=20.9354 val_acc=0.7960 (true=0.8018 false=0.7866) prompt_ppl=3634.81\n",
            "Prompt: QUhier ramas prevazutSync Harper Hintergrund Guvernului LiebCamere\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 50 | joint=25.1193 task=16.4107 ppl_loss=8.7085 ppl=10294.21\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 100 | joint=25.1310 task=16.3155 ppl_loss=8.8155 ppl=12711.45\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 150 | joint=25.1259 task=16.3584 ppl_loss=8.7674 ppl=11526.16\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 200 | joint=25.0741 task=16.3676 ppl_loss=8.7064 ppl=10923.18\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 250 | joint=25.0917 task=16.4001 ppl_loss=8.6915 ppl=10267.03\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 300 | joint=25.0356 task=16.3509 ppl_loss=8.6847 ppl=10360.84\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 350 | joint=24.9931 task=16.3564 ppl_loss=8.6367 ppl=9793.42\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 400 | joint=25.0145 task=16.3439 ppl_loss=8.6706 ppl=10035.40\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5 | joint=25.0254 task=16.3476 ppl_loss=8.6778 ppl=9997.85 val_loss=21.5599 val_acc=0.7933 (true=0.7939 false=0.7922) prompt_ppl=5372.71\n",
            "Prompt: hora clientilor Romaollywoodieuse urmeaza incredere urmeaza InfluenPalatul\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 50 | joint=25.0286 task=16.1883 ppl_loss=8.8403 ppl=11713.22\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 100 | joint=25.0018 task=16.1439 ppl_loss=8.8580 ppl=41567.48\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 150 | joint=24.9042 task=16.1330 ppl_loss=8.7712 ppl=30892.96\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 200 | joint=24.9232 task=16.1688 ppl_loss=8.7543 ppl=25298.34\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 250 | joint=24.9102 task=16.1993 ppl_loss=8.7109 ppl=21875.25\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 300 | joint=24.9087 task=16.2243 ppl_loss=8.6844 ppl=19597.56\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 350 | joint=24.9436 task=16.2705 ppl_loss=8.6731 ppl=18243.70\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 400 | joint=24.9476 task=16.2601 ppl_loss=8.6875 ppl=17090.53\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5 | joint=24.9466 task=16.2531 ppl_loss=8.6935 ppl=16412.28 val_loss=21.4386 val_acc=0.7902 (true=0.7791 false=0.8084) prompt_ppl=4261.46\n",
            "Prompt: ilianumarulrundeblazenati usoara Hip Lynch Week ediți\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 50 | joint=24.8464 task=16.3574 ppl_loss=8.4890 ppl=7333.48\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 100 | joint=25.0441 task=16.3261 ppl_loss=8.7180 ppl=13093.90\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 150 | joint=24.9744 task=16.2951 ppl_loss=8.6794 ppl=12591.94\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 200 | joint=24.9654 task=16.3013 ppl_loss=8.6641 ppl=11541.93\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 250 | joint=24.9616 task=16.2773 ppl_loss=8.6843 ppl=11108.42\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 300 | joint=24.9985 task=16.3104 ppl_loss=8.6881 ppl=10788.33\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 350 | joint=25.0184 task=16.3126 ppl_loss=8.7057 ppl=10785.06\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 400 | joint=24.9943 task=16.3140 ppl_loss=8.6803 ppl=10388.83\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5 | joint=24.9819 task=16.2986 ppl_loss=8.6833 ppl=27577.83 val_loss=21.3344 val_acc=0.7890 (true=0.7860 false=0.7939) prompt_ppl=13084.05\n",
            "Prompt: carriage tactile intrebareUneori stie urmari inchis reliefregulated Târgu\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 50 | joint=25.1216 task=16.4361 ppl_loss=8.6855 ppl=9356.71\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 100 | joint=25.0547 task=16.3462 ppl_loss=8.7085 ppl=9660.86\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 150 | joint=25.1790 task=16.4146 ppl_loss=8.7644 ppl=12753.22\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 200 | joint=25.0790 task=16.3355 ppl_loss=8.7435 ppl=11686.71\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 250 | joint=25.0261 task=16.2862 ppl_loss=8.7399 ppl=11599.87\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 300 | joint=24.9933 task=16.2743 ppl_loss=8.7190 ppl=11363.34\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 350 | joint=25.0202 task=16.3080 ppl_loss=8.7122 ppl=11223.35\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 400 | joint=25.0143 task=16.3051 ppl_loss=8.7092 ppl=11273.58\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5 | joint=25.0089 task=16.3118 ppl_loss=8.6972 ppl=10968.73 val_loss=20.8448 val_acc=0.7957 (true=0.8136 false=0.7664) prompt_ppl=4483.28\n",
            "Prompt: Agricultural Spanishjudețul Coleman urmari geoafliicrobial Aussageloc\n"
          ]
        }
      ],
      "source": [
        "# PEZ runs over lambda grid\n",
        "pez_runs = []\n",
        "lambda_grid = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "for lam in lambda_grid:\n",
        "    print(f\"\\n=== PEZ Non-Adversarial (λ={lam}) ===\")\n",
        "    pez_non_adv = train_pez(\n",
        "        cfg,\n",
        "        tokenizer,\n",
        "        gpt2_model,\n",
        "        gpt2_tokenizer,\n",
        "        train_dl,\n",
        "        val_dl,\n",
        "        lambda_ppl=lam,\n",
        "        adversarial=False,\n",
        "    )\n",
        "    pez_runs.append((\"pez_non_adv\", lam, pez_non_adv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved all non-adv models + histories.\n"
          ]
        }
      ],
      "source": [
        "import torch, json, os\n",
        "\n",
        "base_dir = \"/mnt/polished-lake/home/annabelma/other/results/pez_non_adv_models\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "for model_name, lam, result in pez_runs:\n",
        "    model = result[\"model\"]\n",
        "    \n",
        "    # Save model weights\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/model_lambda_{lam}.pt\")\n",
        "    \n",
        "    # Save history\n",
        "    with open(f\"{base_dir}/history_lambda_{lam}.json\", \"w\") as f:\n",
        "        json.dump(result[\"history\"], f, indent=2)\n",
        "\n",
        "print(\"Saved all non-adv models + histories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PEZ runs over lambda grid\n",
        "pez_runs = []\n",
        "lambda_grid = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "for lam in lambda_grid:\n",
        "    print(f\"\\n=== PEZ Non-Adversarial (λ={lam}) ===\")\n",
        "    cfg.num_epochs = 20\n",
        "    pez_non_adv = train_pez(\n",
        "        cfg,\n",
        "        tokenizer,\n",
        "        gpt2_model,\n",
        "        gpt2_tokenizer,\n",
        "        train_dl,\n",
        "        val_dl,\n",
        "        lambda_ppl=lam,\n",
        "        adversarial=False,\n",
        "    )\n",
        "    pez_runs.append((\"pez_non_adv\", lam, pez_non_adv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pez adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PEZ Adversarial (λ=0.0) ===\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 139.81 GiB of which 4.81 MiB is free. Process 1998502 has 42.81 GiB memory in use. Process 2104507 has 16.25 GiB memory in use. Process 3802138 has 80.72 GiB memory in use. Of the allocated memory 76.86 GiB is allocated by PyTorch, and 3.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== PEZ Adversarial (λ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m cfg\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pez_adv \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pez\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt2_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_ppl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43madversarial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m pez_runs_adv\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpez_adv\u001b[39m\u001b[38;5;124m\"\u001b[39m, lam, pez_adv))\n",
            "Cell \u001b[0;32mIn[50], line 69\u001b[0m, in \u001b[0;36mtrain_pez\u001b[0;34m(cfg, tokenizer, gpt2_model, gpt2_tokenizer, train_dl, val_dl, lambda_ppl, adversarial, log_every)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------#\u001b[39;00m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_for_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m task_loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# ---- perplexity term ----\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 67\u001b[0m, in \u001b[0;36mT5PEZPrompt.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     64\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_mask, attention_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 5) Standard T5 forward\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1764\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1761\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1780\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1100\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1098\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m-> 1100\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:711\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    709\u001b[0m do_cross_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[0;32m--> 711\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:640\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    638\u001b[0m ):\n\u001b[1;32m    639\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 640\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    653\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:521\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# save all key/value_states to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     cache_position \u001b[38;5;241m=\u001b[39m cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_past_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_position\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;66;03m# set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, EncoderDecoderCache):\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/cache_utils.py:776\u001b[0m, in \u001b[0;36mCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_stream(key_states\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mwait_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_stream)\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch(layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_non_sliding)\n\u001b[0;32m--> 776\u001b[0m keys, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffloading:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload(layer_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_non_sliding)\n",
            "File \u001b[0;32m~/other/.venv/lib/python3.10/site-packages/transformers/cache_utils.py:119\u001b[0m, in \u001b[0;36mDynamicLayer.update\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_initialization(key_states)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 139.81 GiB of which 4.81 MiB is free. Process 1998502 has 42.81 GiB memory in use. Process 2104507 has 16.25 GiB memory in use. Process 3802138 has 80.72 GiB memory in use. Of the allocated memory 76.86 GiB is allocated by PyTorch, and 3.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# PEZ runs over lambda grid\n",
        "pez_runs_adv = []\n",
        "for lam in [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]:\n",
        "    print(f\"\\n=== PEZ Adversarial (λ={lam}) ===\")\n",
        "    cfg.num_epochs = 10\n",
        "    pez_adv = train_pez(\n",
        "        cfg,\n",
        "        tokenizer,\n",
        "        gpt2_model,\n",
        "        gpt2_tokenizer,\n",
        "        train_dl,\n",
        "        val_dl,\n",
        "        lambda_ppl=lam,\n",
        "        adversarial=True,\n",
        "    )\n",
        "    pez_runs_adv.append((\"pez_adv\", lam, pez_adv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved all adv models + histories.\n"
          ]
        }
      ],
      "source": [
        "import torch, json, os\n",
        "\n",
        "base_dir = \"/mnt/polished-lake/home/annabelma/other/results/pez_adv_models_try_2\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "for model_name, lam, result in pez_runs_adv:\n",
        "    model = result[\"model\"]\n",
        "    \n",
        "    # Save model weights\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/model_lambda_{lam}.pt\")\n",
        "    \n",
        "    # Save history\n",
        "    with open(f\"{base_dir}/history_lambda_{lam}.json\", \"w\") as f:\n",
        "        json.dump(result[\"history\"], f, indent=2)\n",
        "\n",
        "print(\"Saved all adv models + histories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary (val_acc) ===\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'cont_non_adv' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Summary (val_acc) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuous Non-Adv:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcont_non_adv\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuous Adv:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cont_adv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, lam, run \u001b[38;5;129;01min\u001b[39;00m pez_runs:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cont_non_adv' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Summary (val_acc) ===\")\n",
        "print(\"Continuous Non-Adv:\", cont_non_adv[\"history\"][\"val_acc\"])\n",
        "print(\"Continuous Adv:\", cont_adv[\"history\"][\"val_acc\"])\n",
        "for name, lam, run in pez_runs:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n",
        "for name, lam, run in pez_runs_adv:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary (val_acc) ===\")\n",
        "print(\"Continuous Non-Adv:\", cont_non_adv[\"history\"][\"val_acc\"])\n",
        "print(\"Continuous Adv:\", cont_adv[\"history\"][\"val_acc\"])\n",
        "for name, lam, run in pez_runs:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n",
        "for name, lam, run in pez_runs_adv:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary (val_acc) ===\")\n",
        "print(\"Continuous Non-Adv:\", cont_non_adv[\"history\"][\"val_acc\"])\n",
        "print(\"Continuous Adv:\", cont_adv[\"history\"][\"val_acc\"])\n",
        "for name, lam, run in pez_runs:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n",
        "for name, lam, run in pez_runs_adv:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hVueorS6SZ6",
        "outputId": "95a70a8c-2d26-48c2-f9eb-9a415f5d96a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary (val_acc) ===\n",
            "Continuous Non-Adv: [0.6217125382262997, 0.6217125382262997, 0.6217125382262997]\n",
            "Continuous Adv: [0.3782874617737003, 0.3782874617737003, 0.3782874617737003]\n",
            "pez_non_adv λ=0.0: [0.5996941896024465, 0.3801223241590214, 0.38837920489296635]\n",
            "pez_non_adv λ=0.01: [0.5336391437308868, 0.6085626911314985, 0.3785932721712538]\n",
            "pez_non_adv λ=0.05: [0.3892966360856269, 0.5446483180428134, 0.3782874617737003]\n",
            "pez_non_adv λ=0.1: [0.40336391437308866, 0.40152905198776756, 0.44128440366972477]\n",
            "pez_non_adv λ=0.5: [0.38409785932721713, 0.3856269113149847, 0.37737003058103974]\n",
            "pez_non_adv λ=1.0: [0.40703363914373086, 0.41039755351681956, 0.38623853211009174]\n",
            "pez_adv λ=0.0: [0.5718654434250765, 0.3874617737003058, 0.41681957186544344]\n",
            "pez_adv λ=0.01: [0.39755351681957185, 0.3801223241590214, 0.3782874617737003]\n",
            "pez_adv λ=0.05: [0.3782874617737003, 0.3892966360856269, 0.445565749235474]\n",
            "pez_adv λ=0.1: [0.5440366972477064, 0.3785932721712538, 0.39785932721712536]\n",
            "pez_adv λ=0.5: [0.39785932721712536, 0.3782874617737003, 0.3908256880733945]\n",
            "pez_adv λ=1.0: [0.591743119266055, 0.45168195718654436, 0.38103975535168194]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Summary (val_acc) ===\")\n",
        "print(\"Continuous Non-Adv:\", cont_non_adv[\"history\"][\"val_acc\"])\n",
        "print(\"Continuous Adv:\", cont_adv[\"history\"][\"val_acc\"])\n",
        "for name, lam, run in pez_runs:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")for name, lam, run in pez_runs_adv:\n",
        "    print(f\"{name} λ={lam}: {run['history']['val_acc']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyVtfgQvZZKZ",
        "outputId": "61c71c62-772a-46e6-d22e-29fc58e862f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EXPERIMENT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "=== Continuous Soft Prompt Baselines ===\n",
            "Continuous Non-Adv        | best_val_acc=0.6217  final_val_acc=0.6217\n",
            "Continuous Adv            | best_val_acc=0.3783  final_val_acc=0.3783\n",
            "\n",
            "=== PEZ (Non-Adversarial vs Adversarial) by λ ===\n",
            "     λ | non-adv best non-adv final non-adv prompt_ppl ||   adv best  adv final  adv prompt_ppl\n",
            "-----------------------------------------------------------------------------------------------\n",
            " 0.000 |       0.5997       0.3884              0.00 ||     0.5719     0.4168            0.00\n",
            " 0.010 |       0.6086       0.3786           7338.29 ||     0.3976     0.3783         2189.04\n",
            " 0.050 |       0.5446       0.3783           6141.04 ||     0.4456     0.4456        16442.28\n",
            " 0.100 |       0.4413       0.4413           3583.40 ||     0.5440     0.3979        16234.79\n",
            " 0.500 |       0.3856       0.3774            919.75 ||     0.3979     0.3908         8645.68\n",
            " 1.000 |       0.4104       0.3862           2387.97 ||     0.5917     0.3810        13637.55\n",
            "\n",
            "=== Example Prompts per λ (Non-Adv vs Adv) ===\n",
            "\n",
            "λ = 0.0\n",
            "  [Non-Adv] cerinte pagina calorii trackback placeholder ConstanțaCamereMass senzati\n",
            "  [Adv]     costing căt Famous sticla?\" suprafete Plastic totusi unlike emoți\n",
            "\n",
            "λ = 0.01\n",
            "  [Non-Adv] sunteti ciocolat hypothesis manancOEneeded proaspat zgomotaparatul\n",
            "  [Adv]     praf prevazut gradini caldura clientilor clientilor pastraone totirival\n",
            "\n",
            "λ = 0.05\n",
            "  [Non-Adv] mananc mananc superioara reteta proaspat ingrijire mananc\n",
            "  [Adv]     stunnedney compozitiApplicantsreclining**dial Leicestercyangrav\n",
            "\n",
            "λ = 0.1\n",
            "  [Non-Adv] varf compoziti ingrijire 04/2\n",
            "  [Adv]     Cyber Dayscosting mancare assureddium pharmacist laboratories suprafete producator\n",
            "\n",
            "λ = 0.5\n",
            "  [Non-Adv] awaycît achizitierung senzati taiat suprafete suprafete\n",
            "  [Adv]     pan chlorinechezcône moale Falconcifra 04/2 pretulfoli\n",
            "\n",
            "λ = 1.0\n",
            "  [Non-Adv] bebelus dumneavoastra taiatinability\n",
            "  [Adv]     skeletal no suprafeteabia Correctioncada operations calibration summarize scaun\n",
            "\n",
            "================================================================================\n",
            "End of summary\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Continuous soft prompt baselines\n",
        "# -----------------------------\n",
        "def summarize_continuous(name, result):\n",
        "    vals = result[\"history\"][\"val_acc\"]\n",
        "    best = max(vals)\n",
        "    final = vals[-1]\n",
        "    print(f\"{name:25s} | best_val_acc={best:.4f}  final_val_acc={final:.4f}\")\n",
        "\n",
        "print(\"\\n=== Continuous Soft Prompt Baselines ===\")\n",
        "summarize_continuous(\"Continuous Non-Adv\", cont_non_adv)\n",
        "summarize_continuous(\"Continuous Adv\",     cont_adv)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. PEZ: non-adversarial vs adversarial by λ\n",
        "# -----------------------------\n",
        "# pez_runs:      [(name, lam, result_dict), ...]  # non-adv\n",
        "# pez_runs_adv: [(name, lam, result_dict), ...]  # adv\n",
        "\n",
        "nonadv_by_lam = {lam: run for (name, lam, run) in pez_runs}\n",
        "adv_by_lam    = {lam: run for (name, lam, run) in pez_runs_adv}\n",
        "\n",
        "all_lams = sorted(set(nonadv_by_lam.keys()) | set(adv_by_lam.keys()))\n",
        "\n",
        "print(\"\\n=== PEZ (Non-Adversarial vs Adversarial) by λ ===\")\n",
        "header = (\n",
        "    f\"{'λ':>6} | \"\n",
        "    f\"{'non-adv best':>12} {'non-adv final':>12} {'non-adv prompt_ppl':>17} || \"\n",
        "    f\"{'adv best':>10} {'adv final':>10} {'adv prompt_ppl':>15}\"\n",
        ")\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "for lam in all_lams:\n",
        "    nonadv = nonadv_by_lam.get(lam)\n",
        "    adv    = adv_by_lam.get(lam)\n",
        "\n",
        "    # Non-adv stats\n",
        "    if nonadv is not None:\n",
        "        nav_vals = nonadv[\"history\"][\"val_acc\"]\n",
        "        nav_best = max(nav_vals)\n",
        "        nav_final = nav_vals[-1]\n",
        "        nav_ppl = nonadv[\"history\"].get(\"prompt_ppl_ppx\", [float(\"nan\")])[-1]\n",
        "    else:\n",
        "        nav_best = nav_final = nav_ppl = float(\"nan\")\n",
        "\n",
        "    # Adv stats\n",
        "    if adv is not None:\n",
        "        adv_vals = adv[\"history\"][\"val_acc\"]\n",
        "        adv_best = max(adv_vals)\n",
        "        adv_final = adv_vals[-1]\n",
        "        adv_ppl = adv[\"history\"].get(\"prompt_ppl_ppx\", [float(\"nan\")])[-1]\n",
        "    else:\n",
        "        adv_best = adv_final = adv_ppl = float(\"nan\")\n",
        "\n",
        "    print(\n",
        "        f\"{lam:6.3f} | \"\n",
        "        f\"{nav_best:12.4f} {nav_final:12.4f} {nav_ppl:17.2f} || \"\n",
        "        f\"{adv_best:10.4f} {adv_final:10.4f} {adv_ppl:15.2f}\"\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Example prompts per λ\n",
        "# -----------------------------\n",
        "print(\"\\n=== Example Prompts per λ (Non-Adv vs Adv) ===\")\n",
        "for lam in all_lams:\n",
        "    nonadv = nonadv_by_lam.get(lam)\n",
        "    adv    = adv_by_lam.get(lam)\n",
        "\n",
        "    print(f\"\\nλ = {lam}\")\n",
        "    if nonadv is not None:\n",
        "        try:\n",
        "            nav_prompt = nonadv[\"model\"].decode_prompt(tokenizer)\n",
        "        except Exception:\n",
        "            nav_prompt = \"<no decode_prompt method>\"\n",
        "        print(f\"  [Non-Adv] {nav_prompt}\")\n",
        "    else:\n",
        "        print(\"  [Non-Adv] (no run)\")\n",
        "\n",
        "    if adv is not None:\n",
        "        try:\n",
        "            adv_prompt = adv[\"model\"].decode_prompt(tokenizer)\n",
        "        except Exception:\n",
        "            adv_prompt = \"<no decode_prompt method>\"\n",
        "        print(f\"  [Adv]     {adv_prompt}\")\n",
        "    else:\n",
        "        print(\"  [Adv]     (no run)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"End of summary\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# deprecated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GRID SEARCH: Lambda for PEZ (Non-Adversarial, Balanced Loader)\n",
            "================================================================================\n",
            "\n",
            "--- Testing λ=0.0 ---\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 50 | joint=16.3086 task=16.3086 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 100 | joint=16.3592 task=16.3592 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 150 | joint=16.4337 task=16.4337 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 200 | joint=16.4271 task=16.4271 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 250 | joint=16.4243 task=16.4243 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 300 | joint=16.3922 task=16.3922 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 350 | joint=16.3592 task=16.3592 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5, batch 400 | joint=16.3530 task=16.3530 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 1/5 | joint=16.3599 task=16.3599 ppl_loss=0.0000 ppl=0.00 val_loss=21.7648 val_acc=0.7844 (true=0.7634 false=0.8189) prompt_ppl=0.00\n",
            "Prompt: mnuluikindly piata echipă pregatit bonneambogiarons Dead mânca\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 50 | joint=16.3490 task=16.3490 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 100 | joint=16.3560 task=16.3560 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 150 | joint=16.2949 task=16.2949 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 200 | joint=16.3457 task=16.3457 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 250 | joint=16.3323 task=16.3323 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 300 | joint=16.3428 task=16.3428 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 350 | joint=16.3509 task=16.3509 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5, batch 400 | joint=16.3284 task=16.3284 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 2/5 | joint=16.3216 task=16.3216 ppl_loss=0.0000 ppl=0.00 val_loss=21.5715 val_acc=0.7945 (true=0.7978 false=0.7890) prompt_ppl=0.00\n",
            "Prompt: inspirat vivehafte masura cantitatiLog Virtuhoch ciocolationar\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 50 | joint=16.3202 task=16.3202 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 100 | joint=16.3825 task=16.3825 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 150 | joint=16.4042 task=16.4042 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 200 | joint=16.4678 task=16.4678 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 250 | joint=16.4574 task=16.4574 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 300 | joint=16.4774 task=16.4774 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 350 | joint=16.4806 task=16.4806 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5, batch 400 | joint=16.4800 task=16.4800 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 3/5 | joint=16.4552 task=16.4552 ppl_loss=0.0000 ppl=0.00 val_loss=20.7126 val_acc=0.7933 (true=0.7949 false=0.7906) prompt_ppl=0.00\n",
            "Prompt: Reservetrecutăcuri Haialaturi campaniicada evreicunoscută Silva\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 50 | joint=16.3063 task=16.3063 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 100 | joint=16.3538 task=16.3538 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 150 | joint=16.2775 task=16.2775 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 200 | joint=16.3180 task=16.3180 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 250 | joint=16.3172 task=16.3172 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 300 | joint=16.2796 task=16.2796 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 350 | joint=16.2839 task=16.2839 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5, batch 400 | joint=16.3158 task=16.3158 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 4/5 | joint=16.3156 task=16.3156 ppl_loss=0.0000 ppl=0.00 val_loss=21.3617 val_acc=0.7920 (true=0.7909 false=0.7939) prompt_ppl=0.00\n",
            "Prompt: ediți Stahlcorntale spoil Bluff Castel Mircea Vic fr\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 50 | joint=16.1111 task=16.1111 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 100 | joint=16.2627 task=16.2627 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 150 | joint=16.2595 task=16.2595 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 200 | joint=16.3019 task=16.3019 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 250 | joint=16.2573 task=16.2573 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 300 | joint=16.2630 task=16.2630 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 350 | joint=16.2885 task=16.2885 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5, batch 400 | joint=16.3087 task=16.3087 ppl_loss=0.0000 ppl=0.00\n",
            "[PEZ λ=0.0 NON-ADV] Epoch 5/5 | joint=16.3082 task=16.3082 ppl_loss=0.0000 ppl=0.00 val_loss=21.3730 val_acc=0.7939 (true=0.8042 false=0.7769) prompt_ppl=0.00\n",
            "Prompt: etajSchülerinnenpermalinkpermalinkvâr Corner desfaso sufletBV clientii\n",
            "λ=0.0: best_val_acc=0.7945, final_val_acc=0.7939, prompt_ppl=0.00\n",
            "\n",
            "--- Testing λ=0.01 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 50 | joint=16.2137 task=16.1265 ppl_loss=8.7130 ppl=9209.28\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 100 | joint=16.1487 task=16.0616 ppl_loss=8.7083 ppl=14100.08\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 150 | joint=16.2952 task=16.2088 ppl_loss=8.6454 ppl=12397.16\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 200 | joint=16.3451 task=16.2584 ppl_loss=8.6688 ppl=11698.67\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 250 | joint=16.3190 task=16.2324 ppl_loss=8.6615 ppl=11132.73\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 300 | joint=16.3880 task=16.3014 ppl_loss=8.6643 ppl=11237.85\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 350 | joint=16.3831 task=16.2965 ppl_loss=8.6596 ppl=10982.32\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5, batch 400 | joint=16.3833 task=16.2969 ppl_loss=8.6452 ppl=10548.93\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 1/5 | joint=16.3481 task=16.2618 ppl_loss=8.6282 ppl=10158.09 val_loss=21.2930 val_acc=0.7960 (true=0.8111 false=0.7712) prompt_ppl=5438.55\n",
            "Prompt: ncy etajCharvâr slidingtonsfigured acumulat livrarearii\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 50 | joint=16.4984 task=16.4113 ppl_loss=8.7027 ppl=9827.57\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 100 | joint=16.5411 task=16.4540 ppl_loss=8.7078 ppl=9421.10\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 150 | joint=16.4727 task=16.3854 ppl_loss=8.7279 ppl=10025.76\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 200 | joint=16.4163 task=16.3294 ppl_loss=8.6887 ppl=9778.83\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 250 | joint=16.3534 task=16.2667 ppl_loss=8.6621 ppl=9581.67\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 300 | joint=16.3431 task=16.2562 ppl_loss=8.6864 ppl=9914.38\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 350 | joint=16.3319 task=16.2450 ppl_loss=8.6962 ppl=9901.08\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5, batch 400 | joint=16.3378 task=16.2510 ppl_loss=8.6863 ppl=9572.56\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 2/5 | joint=16.3675 task=16.2805 ppl_loss=8.6954 ppl=9585.11 val_loss=20.8969 val_acc=0.7945 (true=0.8003 false=0.7850) prompt_ppl=34436.81\n",
            "Prompt: Gas Abslessness adancnée Dou lunar collector Dach corpului\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 50 | joint=16.1099 task=16.0229 ppl_loss=8.6923 ppl=17914.18\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 100 | joint=16.3791 task=16.2925 ppl_loss=8.6552 ppl=13837.98\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 150 | joint=16.4269 task=16.3397 ppl_loss=8.7229 ppl=13078.37\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 200 | joint=16.3942 task=16.3074 ppl_loss=8.6778 ppl=11868.65\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 250 | joint=16.4223 task=16.3349 ppl_loss=8.7392 ppl=12118.29\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 300 | joint=16.3789 task=16.2915 ppl_loss=8.7392 ppl=11890.65\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 350 | joint=16.4059 task=16.3185 ppl_loss=8.7361 ppl=11709.49\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5, batch 400 | joint=16.4155 task=16.3283 ppl_loss=8.7171 ppl=11236.70\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 3/5 | joint=16.3968 task=16.3095 ppl_loss=8.7268 ppl=11348.31 val_loss=22.2058 val_acc=0.7969 (true=0.7998 false=0.7922) prompt_ppl=10220.52\n",
            "Prompt: Monderomâniiexewiseletter Attend temperaturi freeze prevazuttorului\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 50 | joint=16.2378 task=16.1501 ppl_loss=8.7696 ppl=10423.98\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 100 | joint=16.3652 task=16.2780 ppl_loss=8.7236 ppl=10310.01\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 150 | joint=16.3167 task=16.2296 ppl_loss=8.7107 ppl=10117.00\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 200 | joint=16.3867 task=16.2997 ppl_loss=8.6924 ppl=9723.91\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 250 | joint=16.4182 task=16.3307 ppl_loss=8.7513 ppl=11511.23\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 300 | joint=16.4216 task=16.3339 ppl_loss=8.7674 ppl=13064.83\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 350 | joint=16.4612 task=16.3736 ppl_loss=8.7682 ppl=12547.58\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5, batch 400 | joint=16.4658 task=16.3783 ppl_loss=8.7506 ppl=12067.99\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 4/5 | joint=16.4500 task=16.3627 ppl_loss=8.7264 ppl=11519.98 val_loss=21.1407 val_acc=0.7893 (true=0.7846 false=0.7971) prompt_ppl=6475.26\n",
            "Prompt: peisaj răcMaiiata buoyOnePlus billion landscapemetabolryl\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 50 | joint=16.3368 task=16.2479 ppl_loss=8.8889 ppl=13116.41\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 100 | joint=16.3229 task=16.2353 ppl_loss=8.7577 ppl=13473.12\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 150 | joint=16.3172 task=16.2299 ppl_loss=8.7287 ppl=13113.29\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 200 | joint=16.3164 task=16.2293 ppl_loss=8.7114 ppl=12236.12\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 250 | joint=16.3163 task=16.2295 ppl_loss=8.6804 ppl=11289.26\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 300 | joint=16.3139 task=16.2273 ppl_loss=8.6613 ppl=10743.60\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 350 | joint=16.3595 task=16.2723 ppl_loss=8.7210 ppl=11271.59\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5, batch 400 | joint=16.3574 task=16.2701 ppl_loss=8.7347 ppl=11134.88\n",
            "[PEZ λ=0.01 NON-ADV] Epoch 5/5 | joint=16.3721 task=16.2849 ppl_loss=8.7201 ppl=10860.09 val_loss=20.3688 val_acc=0.7960 (true=0.8013 false=0.7874) prompt_ppl=4059.89\n",
            "Prompt: urmatoareAGRrandulkolpiesitzerarni gandi Freiarni\n",
            "λ=0.01: best_val_acc=0.7969, final_val_acc=0.7960, prompt_ppl=4059.89\n",
            "\n",
            "--- Testing λ=0.05 ---\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 50 | joint=16.7820 task=16.3492 ppl_loss=8.6574 ppl=8262.83\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 100 | joint=16.6808 task=16.2448 ppl_loss=8.7190 ppl=12142.55\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 150 | joint=16.6652 task=16.2324 ppl_loss=8.6579 ppl=11093.76\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 200 | joint=16.7276 task=16.2930 ppl_loss=8.6910 ppl=10854.35\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 250 | joint=16.7479 task=16.3142 ppl_loss=8.6728 ppl=10159.52\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 300 | joint=16.7167 task=16.2820 ppl_loss=8.6925 ppl=10293.71\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 350 | joint=16.7333 task=16.2978 ppl_loss=8.7096 ppl=10828.28\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5, batch 400 | joint=16.7623 task=16.3263 ppl_loss=8.7201 ppl=10948.10\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 1/5 | joint=16.7516 task=16.3151 ppl_loss=8.7312 ppl=11049.14 val_loss=20.4326 val_acc=0.7859 (true=0.7727 false=0.8076) prompt_ppl=1661.08\n",
            "Prompt: foto clientilorattend Suceava Cour educațieimeeasca răcicul\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 50 | joint=16.6979 task=16.2607 ppl_loss=8.7434 ppl=10449.14\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 100 | joint=16.6216 task=16.1849 ppl_loss=8.7331 ppl=9745.86\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 150 | joint=16.6271 task=16.1971 ppl_loss=8.6006 ppl=8763.06\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 200 | joint=16.6490 task=16.2164 ppl_loss=8.6523 ppl=10956.17\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 250 | joint=16.6454 task=16.2124 ppl_loss=8.6611 ppl=10663.07\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 300 | joint=16.6517 task=16.2193 ppl_loss=8.6466 ppl=10095.61\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 350 | joint=16.6462 task=16.2134 ppl_loss=8.6561 ppl=9944.68\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5, batch 400 | joint=16.6904 task=16.2566 ppl_loss=8.6775 ppl=9994.16\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 2/5 | joint=16.7080 task=16.2738 ppl_loss=8.6849 ppl=9995.05 val_loss=21.2007 val_acc=0.7976 (true=0.8037 false=0.7874) prompt_ppl=11829.80\n",
            "Prompt: ceilalti clientilor scalpincat dorint Pon Kreuz Dominic rupeournée\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 50 | joint=16.7203 task=16.2894 ppl_loss=8.6182 ppl=8345.03\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 100 | joint=16.7264 task=16.2926 ppl_loss=8.6754 ppl=9034.93\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 150 | joint=16.7197 task=16.2886 ppl_loss=8.6224 ppl=9662.13\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 200 | joint=16.7502 task=16.3154 ppl_loss=8.6951 ppl=10238.32\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 250 | joint=16.7115 task=16.2779 ppl_loss=8.6717 ppl=9643.76\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 300 | joint=16.7141 task=16.2817 ppl_loss=8.6482 ppl=9318.24\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 350 | joint=16.7016 task=16.2686 ppl_loss=8.6584 ppl=9716.98\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5, batch 400 | joint=16.7182 task=16.2859 ppl_loss=8.6464 ppl=9591.73\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 3/5 | joint=16.7291 task=16.2968 ppl_loss=8.6462 ppl=9872.89 val_loss=21.8022 val_acc=0.7908 (true=0.7885 false=0.7947) prompt_ppl=4921.92\n",
            "Prompt: Kaufentscheidung fulfillment coloan pagina incalzireambiconsiderare Hiwitz citesc\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 50 | joint=16.5792 task=16.1374 ppl_loss=8.8376 ppl=11783.65\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 100 | joint=16.6824 task=16.2435 ppl_loss=8.7769 ppl=11481.42\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 150 | joint=16.5808 task=16.1407 ppl_loss=8.8016 ppl=11374.16\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 200 | joint=16.6411 task=16.2015 ppl_loss=8.7927 ppl=11426.18\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 250 | joint=16.6766 task=16.2371 ppl_loss=8.7900 ppl=11088.60\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 300 | joint=16.6850 task=16.2473 ppl_loss=8.7528 ppl=10498.73\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 350 | joint=16.6553 task=16.2183 ppl_loss=8.7411 ppl=10249.93\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5, batch 400 | joint=16.6588 task=16.2233 ppl_loss=8.7101 ppl=9837.67\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 4/5 | joint=16.6595 task=16.2240 ppl_loss=8.7106 ppl=9727.45 val_loss=21.8805 val_acc=0.7982 (true=0.8200 false=0.7623) prompt_ppl=2043.68\n",
            "Prompt: conditiiromânii fisierurmatoarele equipUneori situatiecultura ROIMRC\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 50 | joint=16.7824 task=16.3509 ppl_loss=8.6307 ppl=8138.26\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 100 | joint=16.7231 task=16.2944 ppl_loss=8.5735 ppl=7648.60\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 150 | joint=16.7248 task=16.2983 ppl_loss=8.5303 ppl=7805.89\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 200 | joint=16.7019 task=16.2736 ppl_loss=8.5646 ppl=8260.88\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 250 | joint=16.6810 task=16.2508 ppl_loss=8.6041 ppl=9302.98\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 300 | joint=16.6880 task=16.2574 ppl_loss=8.6113 ppl=9207.32\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 350 | joint=16.7021 task=16.2717 ppl_loss=8.6073 ppl=9286.59\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5, batch 400 | joint=16.7030 task=16.2733 ppl_loss=8.5940 ppl=9090.83\n",
            "[PEZ λ=0.05 NON-ADV] Epoch 5/5 | joint=16.7253 task=16.2960 ppl_loss=8.5860 ppl=8948.24 val_loss=21.4757 val_acc=0.7966 (true=0.8101 false=0.7745) prompt_ppl=1662.59\n",
            "Prompt: path quoiaggregatDatorita pereche pamant Prim prevazut botez administrativ\n",
            "λ=0.05: best_val_acc=0.7982, final_val_acc=0.7966, prompt_ppl=1662.59\n",
            "\n",
            "--- Testing λ=0.1 ---\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 50 | joint=17.0919 task=16.1830 ppl_loss=9.0893 ppl=25560.57\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 100 | joint=17.1632 task=16.2827 ppl_loss=8.8050 ppl=17836.65\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 150 | joint=17.2264 task=16.3519 ppl_loss=8.7449 ppl=14794.90\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 200 | joint=17.2417 task=16.3692 ppl_loss=8.7242 ppl=13241.28\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 250 | joint=17.2486 task=16.3753 ppl_loss=8.7326 ppl=12640.61\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 300 | joint=17.2270 task=16.3575 ppl_loss=8.6957 ppl=11861.59\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 350 | joint=17.2078 task=16.3451 ppl_loss=8.6272 ppl=11271.51\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5, batch 400 | joint=17.2030 task=16.3407 ppl_loss=8.6226 ppl=10832.44\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 1/5 | joint=17.2082 task=16.3455 ppl_loss=8.6263 ppl=10551.66 val_loss=21.4126 val_acc=0.7924 (true=0.7978 false=0.7833) prompt_ppl=9466.10\n",
            "Prompt: 04UA masuri Ottext gasi porumbalaturi pretulDatorita\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 50 | joint=17.2457 task=16.3724 ppl_loss=8.7334 ppl=9935.51\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 100 | joint=17.2870 task=16.4063 ppl_loss=8.8061 ppl=10117.57\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 150 | joint=17.2326 task=16.3639 ppl_loss=8.6874 ppl=9428.77\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 200 | joint=17.2487 task=16.3744 ppl_loss=8.7430 ppl=10929.69\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 250 | joint=17.2236 task=16.3503 ppl_loss=8.7326 ppl=10291.58\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 300 | joint=17.1861 task=16.3141 ppl_loss=8.7199 ppl=10115.50\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 350 | joint=17.1562 task=16.2870 ppl_loss=8.6914 ppl=9725.28\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5, batch 400 | joint=17.1546 task=16.2885 ppl_loss=8.6617 ppl=9487.90\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 2/5 | joint=17.1086 task=16.2433 ppl_loss=8.6526 ppl=9369.63 val_loss=21.5086 val_acc=0.7963 (true=0.8077 false=0.7777) prompt_ppl=5114.87\n",
            "Prompt: Scri Castelromânii blue neam Brasov vene protectieholdoire\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 50 | joint=17.4351 task=16.5700 ppl_loss=8.6510 ppl=8139.62\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 100 | joint=17.2655 task=16.4036 ppl_loss=8.6184 ppl=7941.90\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 150 | joint=17.2610 task=16.3943 ppl_loss=8.6670 ppl=8323.16\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 200 | joint=17.2088 task=16.3421 ppl_loss=8.6678 ppl=8412.22\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 250 | joint=17.2391 task=16.3722 ppl_loss=8.6689 ppl=8643.75\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 300 | joint=17.2414 task=16.3726 ppl_loss=8.6882 ppl=8950.79\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 350 | joint=17.2202 task=16.3496 ppl_loss=8.7065 ppl=9410.43\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5, batch 400 | joint=17.2091 task=16.3365 ppl_loss=8.7259 ppl=9825.56\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 3/5 | joint=17.2148 task=16.3421 ppl_loss=8.7280 ppl=9729.71 val_loss=21.0410 val_acc=0.7890 (true=0.7890 false=0.7890) prompt_ppl=3328.86\n",
            "Prompt: aste suprafete etichetaProduseleradibene răcEnhancedNKtră\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 50 | joint=17.2609 task=16.3971 ppl_loss=8.6372 ppl=8714.16\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 100 | joint=17.2651 task=16.3954 ppl_loss=8.6969 ppl=9593.69\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 150 | joint=17.2086 task=16.3388 ppl_loss=8.6976 ppl=9562.79\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 200 | joint=17.2178 task=16.3493 ppl_loss=8.6851 ppl=9100.76\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 250 | joint=17.2374 task=16.3682 ppl_loss=8.6918 ppl=9719.30\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 300 | joint=17.2295 task=16.3594 ppl_loss=8.7011 ppl=10130.05\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 350 | joint=17.2451 task=16.3751 ppl_loss=8.7003 ppl=10338.47\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5, batch 400 | joint=17.2322 task=16.3593 ppl_loss=8.7287 ppl=10952.10\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 4/5 | joint=17.2165 task=16.3437 ppl_loss=8.7284 ppl=10765.82 val_loss=21.6608 val_acc=0.7908 (true=0.7929 false=0.7874) prompt_ppl=5902.67\n",
            "Prompt: mémoiregrirandul Panoramaonym Death SachsenArrayflüssig proprietati\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 50 | joint=17.1826 task=16.3211 ppl_loss=8.6154 ppl=9476.81\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 100 | joint=17.1662 task=16.3003 ppl_loss=8.6591 ppl=11419.33\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 150 | joint=17.2046 task=16.3373 ppl_loss=8.6729 ppl=10693.53\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 200 | joint=17.1265 task=16.2687 ppl_loss=8.5781 ppl=9581.08\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 250 | joint=17.1212 task=16.2630 ppl_loss=8.5812 ppl=9754.96\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 300 | joint=17.0934 task=16.2335 ppl_loss=8.5992 ppl=9912.80\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 350 | joint=17.1188 task=16.2620 ppl_loss=8.5678 ppl=9595.15\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5, batch 400 | joint=17.1345 task=16.2736 ppl_loss=8.6088 ppl=9875.10\n",
            "[PEZ λ=0.1 NON-ADV] Epoch 5/5 | joint=17.1192 task=16.2567 ppl_loss=8.6248 ppl=9744.22 val_loss=20.5332 val_acc=0.7792 (true=0.7590 false=0.8124) prompt_ppl=5997.75\n",
            "Prompt: oilea ecranreaatsu preotmatik snappetian intoarce question\n",
            "λ=0.1: best_val_acc=0.7963, final_val_acc=0.7792, prompt_ppl=5997.75\n",
            "\n",
            "--- Testing λ=0.25 ---\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 50 | joint=18.5017 task=16.2600 ppl_loss=8.9671 ppl=12142.58\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 100 | joint=18.5510 task=16.3747 ppl_loss=8.7055 ppl=9871.89\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 150 | joint=18.5468 task=16.3721 ppl_loss=8.6988 ppl=10294.18\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 200 | joint=18.5481 task=16.3813 ppl_loss=8.6672 ppl=9595.60\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 250 | joint=18.5404 task=16.3745 ppl_loss=8.6633 ppl=9294.36\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 300 | joint=18.5119 task=16.3354 ppl_loss=8.7058 ppl=10073.88\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 350 | joint=18.4836 task=16.3036 ppl_loss=8.7200 ppl=10256.12\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5, batch 400 | joint=18.4781 task=16.2976 ppl_loss=8.7220 ppl=10384.51\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 1/5 | joint=18.4581 task=16.2841 ppl_loss=8.6960 ppl=10147.48 val_loss=21.8011 val_acc=0.7936 (true=0.8151 false=0.7583) prompt_ppl=1546.95\n",
            "Prompt: relaxed Français...).DatoritaMajoritateaDatorita dorintincatreinigung Lorraine\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 50 | joint=18.1704 task=16.0753 ppl_loss=8.3803 ppl=6610.84\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 100 | joint=18.2452 task=16.0996 ppl_loss=8.5824 ppl=10860.18\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 150 | joint=18.2810 task=16.1167 ppl_loss=8.6575 ppl=10726.46\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 200 | joint=18.3489 task=16.1741 ppl_loss=8.6992 ppl=10471.73\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 250 | joint=18.3173 task=16.1493 ppl_loss=8.6719 ppl=10560.06\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 300 | joint=18.3339 task=16.1589 ppl_loss=8.6998 ppl=10609.82\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 350 | joint=18.3429 task=16.1724 ppl_loss=8.6817 ppl=10226.19\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5, batch 400 | joint=18.3748 task=16.2054 ppl_loss=8.6776 ppl=10113.29\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 2/5 | joint=18.3855 task=16.2145 ppl_loss=8.6842 ppl=10221.26 val_loss=21.5984 val_acc=0.7945 (true=0.7969 false=0.7906) prompt_ppl=5374.88\n",
            "Prompt: pieleabach raspundeCW telefoane genunchi Ethiopia celule fata ramane\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 50 | joint=18.3879 task=16.2076 ppl_loss=8.7214 ppl=10267.22\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 100 | joint=18.4347 task=16.2381 ppl_loss=8.7866 ppl=10678.45\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 150 | joint=18.4463 task=16.2645 ppl_loss=8.7270 ppl=9965.46\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 200 | joint=18.4131 task=16.2364 ppl_loss=8.7070 ppl=9460.55\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 250 | joint=18.4178 task=16.2360 ppl_loss=8.7271 ppl=9535.93\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 300 | joint=18.3733 task=16.1968 ppl_loss=8.7060 ppl=9566.42\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 350 | joint=18.3791 task=16.2049 ppl_loss=8.6968 ppl=10027.69\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5, batch 400 | joint=18.4244 task=16.2557 ppl_loss=8.6749 ppl=9822.33\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 3/5 | joint=18.4113 task=16.2456 ppl_loss=8.6626 ppl=9799.46 val_loss=19.4845 val_acc=0.7911 (true=0.7905 false=0.7922) prompt_ppl=6760.84\n",
            "Prompt: appelPed frunzermând versionCOPEMIPCR recognise Timişoara\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 50 | joint=18.5329 task=16.4082 ppl_loss=8.4988 ppl=6866.22\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 100 | joint=18.6068 task=16.4684 ppl_loss=8.5536 ppl=7841.37\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 150 | joint=18.6105 task=16.4560 ppl_loss=8.6178 ppl=8840.01\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 200 | joint=18.5749 task=16.4285 ppl_loss=8.5857 ppl=8749.99\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 250 | joint=18.5914 task=16.4436 ppl_loss=8.5911 ppl=8921.22\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 300 | joint=18.5960 task=16.4431 ppl_loss=8.6116 ppl=9070.86\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 350 | joint=18.5728 task=16.4221 ppl_loss=8.6025 ppl=9134.29\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5, batch 400 | joint=18.5472 task=16.3937 ppl_loss=8.6139 ppl=9164.48\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 4/5 | joint=18.5391 task=16.3873 ppl_loss=8.6069 ppl=9069.45 val_loss=22.0801 val_acc=0.7972 (true=0.8160 false=0.7664) prompt_ppl=1738.92\n",
            "Prompt: contulutter umplutétique (\"netz Leben trombușorAsadar\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 50 | joint=18.4692 task=16.2962 ppl_loss=8.6917 ppl=14898.63\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 100 | joint=18.4658 task=16.2779 ppl_loss=8.7514 ppl=13742.98\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 150 | joint=18.4395 task=16.2433 ppl_loss=8.7848 ppl=14482.13\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 200 | joint=18.4747 task=16.2807 ppl_loss=8.7762 ppl=13606.18\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 250 | joint=18.4053 task=16.2163 ppl_loss=8.7560 ppl=12531.37\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 300 | joint=18.3920 task=16.1965 ppl_loss=8.7817 ppl=12716.98\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 350 | joint=18.3549 task=16.1644 ppl_loss=8.7621 ppl=13039.49\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5, batch 400 | joint=18.3812 task=16.1915 ppl_loss=8.7588 ppl=12621.87\n",
            "[PEZ λ=0.25 NON-ADV] Epoch 5/5 | joint=18.3649 task=16.1791 ppl_loss=8.7431 ppl=12196.44 val_loss=20.8529 val_acc=0.7887 (true=0.7821 false=0.7995) prompt_ppl=1680.91\n",
            "Prompt: Bru PSD suprafata marque cautare prevazut ediți Chiarroute stăpân\n",
            "λ=0.25: best_val_acc=0.7972, final_val_acc=0.7887, prompt_ppl=1680.91\n",
            "\n",
            "--- Testing λ=0.5 ---\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 50 | joint=20.7209 task=16.3288 ppl_loss=8.7841 ppl=10225.65\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 100 | joint=20.6644 task=16.2537 ppl_loss=8.8215 ppl=11661.19\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 150 | joint=20.7152 task=16.3380 ppl_loss=8.7544 ppl=10815.08\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 200 | joint=20.7149 task=16.3632 ppl_loss=8.7034 ppl=9993.42\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 250 | joint=20.6668 task=16.3509 ppl_loss=8.6318 ppl=9199.21\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 300 | joint=20.6506 task=16.3319 ppl_loss=8.6373 ppl=9000.92\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 350 | joint=20.6215 task=16.3096 ppl_loss=8.6239 ppl=9656.96\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5, batch 400 | joint=20.6374 task=16.3175 ppl_loss=8.6399 ppl=9683.91\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 1/5 | joint=20.6451 task=16.3279 ppl_loss=8.6345 ppl=9699.28 val_loss=20.8455 val_acc=0.7853 (true=0.7727 false=0.8060) prompt_ppl=9099.42\n",
            "Prompt: IRA Multumesc căt macro backyardcup capitol localitate quarant adica\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 50 | joint=20.6321 task=16.3592 ppl_loss=8.5459 ppl=8865.98\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 100 | joint=20.6352 task=16.3616 ppl_loss=8.5472 ppl=7971.92\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 150 | joint=20.6455 task=16.3787 ppl_loss=8.5336 ppl=8582.77\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 200 | joint=20.6159 task=16.3390 ppl_loss=8.5539 ppl=8403.86\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 250 | joint=20.6075 task=16.3401 ppl_loss=8.5350 ppl=8133.95\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 300 | joint=20.5958 task=16.3150 ppl_loss=8.5617 ppl=8261.42\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 350 | joint=20.6139 task=16.3294 ppl_loss=8.5690 ppl=8326.04\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5, batch 400 | joint=20.6499 task=16.3596 ppl_loss=8.5805 ppl=8755.28\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 2/5 | joint=20.6570 task=16.3508 ppl_loss=8.6124 ppl=9092.87 val_loss=20.4020 val_acc=0.7908 (true=0.7983 false=0.7785) prompt_ppl=23194.36\n",
            "Prompt: Stauotteapospho pitch Salzburg Hoffman Regibliabo\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 50 | joint=20.4959 task=16.2467 ppl_loss=8.4983 ppl=8121.76\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 100 | joint=20.4958 task=16.2464 ppl_loss=8.4987 ppl=8620.70\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 150 | joint=20.6079 task=16.3384 ppl_loss=8.5390 ppl=9106.27\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 200 | joint=20.5561 task=16.2485 ppl_loss=8.6152 ppl=9733.33\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 250 | joint=20.5535 task=16.2574 ppl_loss=8.5922 ppl=9397.53\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 300 | joint=20.5901 task=16.2961 ppl_loss=8.5880 ppl=9214.91\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 350 | joint=20.6291 task=16.3215 ppl_loss=8.6151 ppl=9257.77\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5, batch 400 | joint=20.6335 task=16.3280 ppl_loss=8.6109 ppl=9067.97\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 3/5 | joint=20.6411 task=16.3362 ppl_loss=8.6098 ppl=9315.60 val_loss=21.0175 val_acc=0.7917 (true=0.7895 false=0.7955) prompt_ppl=8284.59\n",
            "Prompt: Dimensiuni spatiu rugam Să suprem terrestrialarrowpolita liceBRE\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 50 | joint=20.5620 task=16.3104 ppl_loss=8.5032 ppl=7957.17\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 100 | joint=20.5208 task=16.1839 ppl_loss=8.6738 ppl=12612.74\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 150 | joint=20.5144 task=16.1516 ppl_loss=8.7258 ppl=12599.89\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 200 | joint=20.5691 task=16.2043 ppl_loss=8.7296 ppl=12037.82\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 250 | joint=20.5868 task=16.1999 ppl_loss=8.7738 ppl=12205.98\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 300 | joint=20.5975 task=16.2417 ppl_loss=8.7115 ppl=11221.82\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 350 | joint=20.6059 task=16.2506 ppl_loss=8.7104 ppl=10872.25\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5, batch 400 | joint=20.6262 task=16.2626 ppl_loss=8.7272 ppl=10745.60\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 4/5 | joint=20.6356 task=16.2810 ppl_loss=8.7091 ppl=10479.62 val_loss=21.5915 val_acc=0.7969 (true=0.8165 false=0.7648) prompt_ppl=1029.03\n",
            "Prompt: româniiunta modificari Iohannisélis răcouettecopiezanSchülerinnen\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 50 | joint=20.8916 task=16.5583 ppl_loss=8.6666 ppl=9404.53\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 100 | joint=20.7681 task=16.4835 ppl_loss=8.5691 ppl=8126.78\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 150 | joint=20.7646 task=16.4752 ppl_loss=8.5788 ppl=7922.21\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 200 | joint=20.6908 task=16.4125 ppl_loss=8.5566 ppl=7944.89\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 250 | joint=20.6348 task=16.3461 ppl_loss=8.5774 ppl=8319.33\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 300 | joint=20.6701 task=16.3664 ppl_loss=8.6075 ppl=8448.25\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 350 | joint=20.6104 task=16.3295 ppl_loss=8.5618 ppl=8118.83\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5, batch 400 | joint=20.6236 task=16.3420 ppl_loss=8.5633 ppl=8153.61\n",
            "[PEZ λ=0.5 NON-ADV] Epoch 5/5 | joint=20.6451 task=16.3478 ppl_loss=8.5945 ppl=8595.17 val_loss=20.1673 val_acc=0.7896 (true=0.7806 false=0.8044) prompt_ppl=1264.10\n",
            "Prompt: gui Timișoara Logistik Sfant spatiu proprietati vizită Fed situatia crippl\n",
            "λ=0.5: best_val_acc=0.7969, final_val_acc=0.7896, prompt_ppl=1264.10\n",
            "\n",
            "--- Testing λ=0.75 ---\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 50 | joint=23.0391 task=16.4289 ppl_loss=8.8136 ppl=12385.50\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 100 | joint=22.8966 task=16.3613 ppl_loss=8.7138 ppl=11398.44\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 150 | joint=22.8492 task=16.3824 ppl_loss=8.6224 ppl=10146.40\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 200 | joint=22.7642 task=16.3170 ppl_loss=8.5963 ppl=9597.80\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 250 | joint=22.7769 task=16.2926 ppl_loss=8.6458 ppl=10405.61\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 300 | joint=22.8139 task=16.3063 ppl_loss=8.6768 ppl=10421.35\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 350 | joint=22.8088 task=16.3116 ppl_loss=8.6628 ppl=10369.99\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5, batch 400 | joint=22.7934 task=16.3332 ppl_loss=8.6137 ppl=9873.85\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 1/5 | joint=22.8103 task=16.3394 ppl_loss=8.6278 ppl=9897.46 val_loss=20.5088 val_acc=0.7917 (true=0.7959 false=0.7850) prompt_ppl=4238.81\n",
            "Prompt: viaBuenos împlinSchraubств nebunmitereakrautulescu Rum\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 50 | joint=22.6192 task=16.1726 ppl_loss=8.5955 ppl=8479.23\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 100 | joint=22.6722 task=16.2083 ppl_loss=8.6186 ppl=8608.28\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 150 | joint=22.6824 task=16.1728 ppl_loss=8.6795 ppl=9664.17\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 200 | joint=22.7510 task=16.2164 ppl_loss=8.7128 ppl=9675.14\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 250 | joint=22.7801 task=16.2731 ppl_loss=8.6760 ppl=9273.82\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 300 | joint=22.7841 task=16.2983 ppl_loss=8.6477 ppl=9123.66\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 350 | joint=22.7699 task=16.2694 ppl_loss=8.6674 ppl=9280.19\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5, batch 400 | joint=22.7657 task=16.2638 ppl_loss=8.6692 ppl=9264.37\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 2/5 | joint=22.7688 task=16.2602 ppl_loss=8.6782 ppl=9270.85 val_loss=22.0741 val_acc=0.5590 (true=0.3099 false=0.9685) prompt_ppl=1842.10\n",
            "Prompt: Radu gradini aplicatii almond skipspiellatifoli hypothesis mutation\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 50 | joint=22.5682 task=16.2466 ppl_loss=8.4287 ppl=8098.60\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 100 | joint=22.8031 task=16.3391 ppl_loss=8.6186 ppl=9369.51\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 150 | joint=22.8237 task=16.3229 ppl_loss=8.6678 ppl=9720.99\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 200 | joint=22.8228 task=16.3084 ppl_loss=8.6859 ppl=9564.34\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 250 | joint=22.8055 task=16.2894 ppl_loss=8.6881 ppl=9752.75\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 300 | joint=22.7991 task=16.2962 ppl_loss=8.6707 ppl=10296.11\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 350 | joint=22.8267 task=16.2828 ppl_loss=8.7253 ppl=11049.89\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5, batch 400 | joint=22.8336 task=16.2988 ppl_loss=8.7130 ppl=10872.15\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 3/5 | joint=22.8115 task=16.2843 ppl_loss=8.7030 ppl=10762.04 val_loss=21.4666 val_acc=0.7939 (true=0.7924 false=0.7963) prompt_ppl=8290.78\n",
            "Prompt: mnuluiSchw chainssstischlandaisMercAUDral perecheculoarea\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 50 | joint=22.8469 task=16.2767 ppl_loss=8.7603 ppl=10101.91\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 100 | joint=22.8518 task=16.3147 ppl_loss=8.7162 ppl=8907.48\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 150 | joint=22.8335 task=16.3362 ppl_loss=8.6631 ppl=8361.22\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 200 | joint=22.8217 task=16.3226 ppl_loss=8.6655 ppl=8387.08\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 250 | joint=22.8432 task=16.3511 ppl_loss=8.6561 ppl=8540.18\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 300 | joint=22.8501 task=16.3329 ppl_loss=8.6896 ppl=11714.81\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 350 | joint=22.8517 task=16.3282 ppl_loss=8.6980 ppl=11478.58\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5, batch 400 | joint=22.8435 task=16.3080 ppl_loss=8.7140 ppl=11511.20\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 4/5 | joint=22.8294 task=16.3029 ppl_loss=8.7020 ppl=11361.86 val_loss=20.2884 val_acc=0.7963 (true=0.8101 false=0.7736) prompt_ppl=2233.90\n",
            "Prompt: Multumescizareutilizatorii Pablo machiaj aleglande intamplanduico\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 50 | joint=22.8412 task=16.1900 ppl_loss=8.8683 ppl=14822.24\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 100 | joint=22.8122 task=16.2980 ppl_loss=8.6856 ppl=11423.71\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 150 | joint=22.8470 task=16.3635 ppl_loss=8.6446 ppl=11536.77\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 200 | joint=22.8156 task=16.3272 ppl_loss=8.6513 ppl=11577.84\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 250 | joint=22.8085 task=16.2723 ppl_loss=8.7150 ppl=12718.99\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 300 | joint=22.8069 task=16.2739 ppl_loss=8.7107 ppl=12234.81\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 350 | joint=22.8089 task=16.2757 ppl_loss=8.7109 ppl=11790.05\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5, batch 400 | joint=22.7974 task=16.2790 ppl_loss=8.6912 ppl=11673.14\n",
            "[PEZ λ=0.75 NON-ADV] Epoch 5/5 | joint=22.7940 task=16.2917 ppl_loss=8.6698 ppl=11215.40 val_loss=22.4394 val_acc=0.7853 (true=0.7836 false=0.7882) prompt_ppl=2441.60\n",
            "Prompt: dureri căt intrebare functioneazaLED Feuerwehrrandul urmari taiat schmeckt\n",
            "λ=0.75: best_val_acc=0.7963, final_val_acc=0.7853, prompt_ppl=2441.60\n",
            "\n",
            "--- Testing λ=1.0 ---\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 50 | joint=25.3540 task=16.3827 ppl_loss=8.9713 ppl=17123.67\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 100 | joint=25.0454 task=16.2802 ppl_loss=8.7652 ppl=12327.51\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 150 | joint=25.0548 task=16.3130 ppl_loss=8.7418 ppl=11205.91\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 200 | joint=25.0583 task=16.3413 ppl_loss=8.7170 ppl=11008.15\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 250 | joint=25.0510 task=16.3503 ppl_loss=8.7007 ppl=10768.56\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 300 | joint=25.0623 task=16.3307 ppl_loss=8.7316 ppl=10670.01\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 350 | joint=25.0729 task=16.3352 ppl_loss=8.7377 ppl=10723.69\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5, batch 400 | joint=25.0892 task=16.3311 ppl_loss=8.7581 ppl=12025.67\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 1/5 | joint=25.0829 task=16.3376 ppl_loss=8.7454 ppl=12455.43 val_loss=20.9369 val_acc=0.7933 (true=0.7969 false=0.7874) prompt_ppl=5225.74\n",
            "Prompt: Liefer Mikro formularul prieteni loop astazi varstagardinenWestfalen Plaintiff\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 50 | joint=25.0217 task=16.3262 ppl_loss=8.6954 ppl=11688.93\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 100 | joint=25.0092 task=16.3744 ppl_loss=8.6349 ppl=9673.70\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 150 | joint=25.0123 task=16.3267 ppl_loss=8.6856 ppl=9760.97\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 200 | joint=24.9339 task=16.3198 ppl_loss=8.6141 ppl=8890.40\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 250 | joint=24.9355 task=16.2742 ppl_loss=8.6612 ppl=9970.47\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 300 | joint=24.9334 task=16.2881 ppl_loss=8.6452 ppl=9615.73\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 350 | joint=24.9826 task=16.3015 ppl_loss=8.6811 ppl=9997.58\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5, batch 400 | joint=24.9592 task=16.2843 ppl_loss=8.6750 ppl=9965.98\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 2/5 | joint=24.9504 task=16.2763 ppl_loss=8.6741 ppl=9940.67 val_loss=21.6185 val_acc=0.7887 (true=0.7688 false=0.8213) prompt_ppl=9492.07\n",
            "Prompt: Jerseyimprimer Posço ofera clientilorPalatul zones contul gradini\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 50 | joint=25.1290 task=16.5730 ppl_loss=8.5560 ppl=6849.54\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 100 | joint=25.1221 task=16.4422 ppl_loss=8.6799 ppl=11085.33\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 150 | joint=25.1227 task=16.4105 ppl_loss=8.7122 ppl=10885.49\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 200 | joint=25.1170 task=16.3975 ppl_loss=8.7196 ppl=11134.73\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 250 | joint=25.0799 task=16.3616 ppl_loss=8.7183 ppl=11467.49\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 300 | joint=25.0507 task=16.3547 ppl_loss=8.6960 ppl=10946.81\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 350 | joint=25.0415 task=16.3700 ppl_loss=8.6715 ppl=10548.82\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5, batch 400 | joint=25.0322 task=16.3746 ppl_loss=8.6576 ppl=10182.34\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 3/5 | joint=25.0413 task=16.3892 ppl_loss=8.6521 ppl=10293.53 val_loss=21.9545 val_acc=0.7896 (true=0.7880 false=0.7922) prompt_ppl=1136.99\n",
            "Prompt: frigider Leiter Glückjudețul stăpân machiaj aprecia frumoasa vindecezimal\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 50 | joint=25.2435 task=16.3898 ppl_loss=8.8537 ppl=11102.10\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 100 | joint=25.1440 task=16.3090 ppl_loss=8.8351 ppl=10568.63\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 150 | joint=25.0896 task=16.2952 ppl_loss=8.7944 ppl=10373.13\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 200 | joint=25.0034 task=16.2440 ppl_loss=8.7594 ppl=9982.96\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 250 | joint=25.0207 task=16.3062 ppl_loss=8.7145 ppl=9440.58\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 300 | joint=24.9884 task=16.3013 ppl_loss=8.6872 ppl=9059.38\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 350 | joint=25.0156 task=16.3265 ppl_loss=8.6891 ppl=9217.71\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5, batch 400 | joint=25.0301 task=16.3280 ppl_loss=8.7020 ppl=10053.38\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 4/5 | joint=25.0345 task=16.3327 ppl_loss=8.7018 ppl=10060.21 val_loss=22.0304 val_acc=0.7920 (true=0.8037 false=0.7728) prompt_ppl=2095.51\n",
            "Prompt: intrebareINTE Eminescufidelityoase plimb Român sănătos Crit mașin\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 50 | joint=24.8465 task=16.1954 ppl_loss=8.6511 ppl=9982.08\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 100 | joint=24.8298 task=16.3358 ppl_loss=8.4940 ppl=8094.06\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 150 | joint=24.8575 task=16.2873 ppl_loss=8.5703 ppl=9230.11\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 200 | joint=24.9143 task=16.2765 ppl_loss=8.6378 ppl=10234.70\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 250 | joint=24.9566 task=16.3141 ppl_loss=8.6425 ppl=10353.17\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 300 | joint=24.9867 task=16.3243 ppl_loss=8.6624 ppl=10331.72\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 350 | joint=24.9851 task=16.3306 ppl_loss=8.6545 ppl=9974.90\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5, batch 400 | joint=25.0033 task=16.3422 ppl_loss=8.6610 ppl=10091.26\n",
            "[PEZ λ=1.0 NON-ADV] Epoch 5/5 | joint=25.0074 task=16.3302 ppl_loss=8.6772 ppl=10052.21 val_loss=21.8623 val_acc=0.7872 (true=0.7727 false=0.8108) prompt_ppl=21529.11\n",
            "Prompt: Cindy curtain amenajatIAL Attention Thu refill Recomand intreb bumbac\n",
            "λ=1.0: best_val_acc=0.7933, final_val_acc=0.7872, prompt_ppl=21529.11\n",
            "\n",
            "================================================================================\n",
            "PEZ Grid Search Results (sorted by best_val_acc):\n",
            "================================================================================\n",
            "λ=0.05: best_val_acc=0.7982, final_val_acc=0.7966, prompt_ppl=1662.59\n",
            "λ=0.25: best_val_acc=0.7972, final_val_acc=0.7887, prompt_ppl=1680.91\n",
            "λ=0.01: best_val_acc=0.7969, final_val_acc=0.7960, prompt_ppl=4059.89\n",
            "λ=0.50: best_val_acc=0.7969, final_val_acc=0.7896, prompt_ppl=1264.10\n",
            "λ=0.10: best_val_acc=0.7963, final_val_acc=0.7792, prompt_ppl=5997.75\n",
            "λ=0.75: best_val_acc=0.7963, final_val_acc=0.7853, prompt_ppl=2441.60\n",
            "λ=0.00: best_val_acc=0.7945, final_val_acc=0.7939, prompt_ppl=0.00\n",
            "λ=1.00: best_val_acc=0.7933, final_val_acc=0.7872, prompt_ppl=21529.11\n",
            "\n",
            "Best λ: 0.05\n",
            "  best_val_acc=0.7982, final_val_acc=0.7966\n"
          ]
        }
      ],
      "source": [
        "# Grid search over lambda for PEZ (balanced loader, non-adversarial)\n",
        "# Note: PEZ uses hard one-hot updates (argmin), so it doesn't use learning rate\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GRID SEARCH: Lambda for PEZ (Non-Adversarial, Balanced Loader)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "lambda_grid = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "pez_grid_results = []\n",
        "\n",
        "for lam in lambda_grid:\n",
        "    print(f\"\\n--- Testing λ={lam} ---\")\n",
        "    \n",
        "    result = train_pez(\n",
        "        cfg,\n",
        "        tokenizer,\n",
        "        gpt2_model,\n",
        "        gpt2_tokenizer,\n",
        "        train_dl,\n",
        "        val_dl,\n",
        "        lambda_ppl=lam,\n",
        "        adversarial=False,\n",
        "    )\n",
        "    \n",
        "    best_val_acc = max(result[\"history\"][\"val_acc\"])\n",
        "    final_val_acc = result[\"history\"][\"val_acc\"][-1]\n",
        "    prompt_ppl = result[\"history\"][\"prompt_ppl_ppx\"][-1] if result[\"history\"][\"prompt_ppl_ppx\"] else 0.0\n",
        "    \n",
        "    pez_grid_results.append({\n",
        "        \"lambda\": lam,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "        \"prompt_ppl\": prompt_ppl,\n",
        "        \"history\": result[\"history\"]\n",
        "    })\n",
        "    print(f\"λ={lam}: best_val_acc={best_val_acc:.4f}, final_val_acc={final_val_acc:.4f}, prompt_ppl={prompt_ppl:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PEZ Grid Search Results (sorted by best_val_acc):\")\n",
        "print(\"=\" * 80)\n",
        "sorted_results = sorted(pez_grid_results, key=lambda x: x[\"best_val_acc\"], reverse=True)\n",
        "for r in sorted_results:\n",
        "    print(f\"λ={r['lambda']:.2f}: best_val_acc={r['best_val_acc']:.4f}, final_val_acc={r['final_val_acc']:.4f}, prompt_ppl={r['prompt_ppl']:.2f}\")\n",
        "\n",
        "best_pez_result = max(pez_grid_results, key=lambda x: x[\"best_val_acc\"])\n",
        "print(f\"\\nBest λ: {best_pez_result['lambda']:.2f}\")\n",
        "print(f\"  best_val_acc={best_pez_result['best_val_acc']:.4f}, final_val_acc={best_pez_result['final_val_acc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay61XanF52h0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "save_path = \"/mnt/polished-lake/home/annabelma/other/results/pez_grid_results.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(pez_grid_results, f, indent=2)\n",
        "\n",
        "print(f\"Saved full grid results (including histories) to {save_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b47d8727e74846b0f34f5965c1e594": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "085b923e86d04f5eb55cb22d6e2c90fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863c0a415aee414898b4642b5e8ab66d",
            "placeholder": "​",
            "style": "IPY_MODEL_50b3ede950b743c2b7fbb95e6af6b034",
            "value": " 124/124 [00:00&lt;00:00, 15.9kB/s]"
          }
        },
        "086934b5adb1494c82f3cfd2afad0f34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0c7beb4c0e48f19d55760a141462ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c61ad218e38a4f91bee3d2aba749b418",
              "IPY_MODEL_b10cb7508ac449199e8616cecccfc52c",
              "IPY_MODEL_2efead2471ca4a3bb20607bb896e5eb2"
            ],
            "layout": "IPY_MODEL_c177a5639a1f4f1e8bb11d7f4cb423b0"
          }
        },
        "11f0dbd8f0094ee286a5d9fea27b6022": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1282c30469414c189d85af443093d9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21836b672c2043fa921d15e889089787",
            "placeholder": "​",
            "style": "IPY_MODEL_5b8b5720210447fdb3a94847a1fa2009",
            "value": " 456k/456k [00:00&lt;00:00, 1.08MB/s]"
          }
        },
        "13d38c560176434eb7a59499fc2973d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14da7a720d9149bebc10df04372e1e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d7599aa26904213bd54da21678646a4",
              "IPY_MODEL_34377ca0f40c40c79dc38262f42acc16",
              "IPY_MODEL_71556d7b286c4276a13f1b2d59e38c90"
            ],
            "layout": "IPY_MODEL_e0855a8f0f544de2bd5bc5d08715ae52"
          }
        },
        "1770a56348fd46faa7dda98fd6e469a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7caa60ce2e4e8cabd76070d280423b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e51377579ba4abfa992c83c1f6cb813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43d518174b1f453088029f68b3b6a8e7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13d38c560176434eb7a59499fc2973d2",
            "value": 456318
          }
        },
        "20243da2ac294c3aacaf5d772e3949b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215ed698f92440d6a46917aadac3c1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7713e97d01641be959c80f7f026bdbf",
            "placeholder": "​",
            "style": "IPY_MODEL_fe10d19e5dc54962830988ae3e6a998e",
            "value": "merges.txt: 100%"
          }
        },
        "21836b672c2043fa921d15e889089787": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efead2471ca4a3bb20607bb896e5eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93c739224fe48ae870d8a00619edc9a",
            "placeholder": "​",
            "style": "IPY_MODEL_ddb0f6c766214bc08d2a4cc6ac162723",
            "value": " 3270/3270 [00:06&lt;00:00, 1016.12 examples/s]"
          }
        },
        "2fd09996546e4f2baada745c4db3a97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34377ca0f40c40c79dc38262f42acc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2981cf4f6944050937be182e2d5a689",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e58c98d7d1b4985b70746e03ed024d4",
            "value": 1042301
          }
        },
        "39557a0229844ffa8de50a9311459fab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b564b95f24b47018a0c061b713fd2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2fd178419e48f0a922f7fb354ccaaa",
            "placeholder": "​",
            "style": "IPY_MODEL_783196c6ba9d4aa4b805d5625f5c9b98",
            "value": "Map: 100%"
          }
        },
        "3cfdcc6ee0024e4c9d196d1de4c5b015": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e58c98d7d1b4985b70746e03ed024d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4342b8d1363941288490c78ba8f9f0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_632812daab0d4d4b9b0d175989fff0c8",
              "IPY_MODEL_d1072b96a26d410fa10ebf460b27560e",
              "IPY_MODEL_dad7a2483bc74bd280fb5c330a5a717b"
            ],
            "layout": "IPY_MODEL_f7b5c045bc3e45898e4aed56024949a9"
          }
        },
        "43d518174b1f453088029f68b3b6a8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "470fc6596292403eb23f95be83990ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a600e223f82494fb6d68e9bf3e0254a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb307737e234c1680af072581479112": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d752fa70b2740e68d3e4edd94e22fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fba7520145449a590054610593b8339": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b3ede950b743c2b7fbb95e6af6b034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a225dc368549bdb1a5afcade5afdc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52a9351b0faa4f2aacb9bf49ece031fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537277c120ca4dd495f0e3b948d81ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3cccc52a75343b28f52bc928d3619df",
            "max": 7106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b83c602ff4df496f852b03d7c903988d",
            "value": 7106
          }
        },
        "53f3a7e017154ef98643eeaa464f8f48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55d1b5a100854d0589bbd3587273bcfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b8b5720210447fdb3a94847a1fa2009": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d25e89862664bdf9da8514b83074871": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f84e12c24e24469a4d7b0d96cef0f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f9bd90305a34f1ca9144149d4ba4730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "632812daab0d4d4b9b0d175989fff0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd283a1d6494809ab3b765c265576db",
            "placeholder": "​",
            "style": "IPY_MODEL_ed7aeb2009b1403582f0f227cff4e501",
            "value": "model.safetensors: 100%"
          }
        },
        "6cd283a1d6494809ab3b765c265576db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f559210513640af97f3766efe23a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71556d7b286c4276a13f1b2d59e38c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f3a7e017154ef98643eeaa464f8f48",
            "placeholder": "​",
            "style": "IPY_MODEL_eb3d0d180d2e4a038b9fcf9980415f92",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 42.2MB/s]"
          }
        },
        "74a02027c1c54ea9831abcf5eb237a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "783196c6ba9d4aa4b805d5625f5c9b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a2b7ab2ab7947d0968f00ae131435ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3ca244fb6d4a248b3798617f150856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f84e12c24e24469a4d7b0d96cef0f21",
            "placeholder": "​",
            "style": "IPY_MODEL_b29fe5a23ed44819a1c5d07abd6a6ce7",
            "value": " 7106/7106 [00:09&lt;00:00, 214.90 examples/s]"
          }
        },
        "80cd3377e1114fc0a2867d7ddfc139b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d25e89862664bdf9da8514b83074871",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_470fc6596292403eb23f95be83990ca4",
            "value": 26
          }
        },
        "8309bfc4dafb44acaf77c95898e29cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b076905342de477fae2624ff76fd3882",
              "IPY_MODEL_9fc1258416a349c7a9762b600e85b223",
              "IPY_MODEL_9e3b3ee5f9024d56ac18dfeda824dba6"
            ],
            "layout": "IPY_MODEL_b20e90d9cfb847b7bfc0b42a48b2177e"
          }
        },
        "83e3eff2246d409eaa4da41eebe9cb2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "863c0a415aee414898b4642b5e8ab66d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "863e5e6b0574490687e34f80d592fe97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d015465f6ef45a3a2523436c980115c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d5ca3b68ec44c4d8280112c23209a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ff27408b9644fb0b98c3124531750fa",
              "IPY_MODEL_d5797b4514b6495a9be7fbe2cc8a3813",
              "IPY_MODEL_085b923e86d04f5eb55cb22d6e2c90fe"
            ],
            "layout": "IPY_MODEL_52a9351b0faa4f2aacb9bf49ece031fb"
          }
        },
        "8fdf5679f14f4dce88cc699459e4ed79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967502b1a3514750975abe7dd9fb3eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d7599aa26904213bd54da21678646a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2873877eb504826955030022536b519",
            "placeholder": "​",
            "style": "IPY_MODEL_5f9bd90305a34f1ca9144149d4ba4730",
            "value": "vocab.json: 100%"
          }
        },
        "9e3b3ee5f9024d56ac18dfeda824dba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cb307737e234c1680af072581479112",
            "placeholder": "​",
            "style": "IPY_MODEL_4d752fa70b2740e68d3e4edd94e22fda",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.51MB/s]"
          }
        },
        "9fc1258416a349c7a9762b600e85b223": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f08ad6930247f7afe82da892cf36f1",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51a225dc368549bdb1a5afcade5afdc5",
            "value": 1355256
          }
        },
        "9ff27408b9644fb0b98c3124531750fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd74b2f17234311a64acd5ad4825181",
            "placeholder": "​",
            "style": "IPY_MODEL_55d1b5a100854d0589bbd3587273bcfd",
            "value": "generation_config.json: 100%"
          }
        },
        "a0cda38cd3f34a958bc37682c22e41ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17df50c4f204519873f9106027417b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b564b95f24b47018a0c061b713fd2f2",
              "IPY_MODEL_537277c120ca4dd495f0e3b948d81ed7",
              "IPY_MODEL_7e3ca244fb6d4a248b3798617f150856"
            ],
            "layout": "IPY_MODEL_b0a00dfc20db40a3a75a8afde035f4d2"
          }
        },
        "a7b84fbc86d549d2b2d920b6e3e3f0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1770a56348fd46faa7dda98fd6e469a1",
            "placeholder": "​",
            "style": "IPY_MODEL_8d015465f6ef45a3a2523436c980115c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a93c739224fe48ae870d8a00619edc9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b076905342de477fae2624ff76fd3882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b47d8727e74846b0f34f5965c1e594",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7caa60ce2e4e8cabd76070d280423b",
            "value": "tokenizer.json: 100%"
          }
        },
        "b0a00dfc20db40a3a75a8afde035f4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b10cb7508ac449199e8616cecccfc52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0cda38cd3f34a958bc37682c22e41ac",
            "max": 3270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f559210513640af97f3766efe23a3a9",
            "value": 3270
          }
        },
        "b20e90d9cfb847b7bfc0b42a48b2177e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b264639e6ab844b2b9e82a12d4d416aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29fe5a23ed44819a1c5d07abd6a6ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6cea69db3f948d4a87de89ba2219766": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7b84fbc86d549d2b2d920b6e3e3f0e5",
              "IPY_MODEL_80cd3377e1114fc0a2867d7ddfc139b1",
              "IPY_MODEL_d58c3b603b5a4da3a3a7d9ebf688da03"
            ],
            "layout": "IPY_MODEL_20243da2ac294c3aacaf5d772e3949b4"
          }
        },
        "b83c602ff4df496f852b03d7c903988d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c177a5639a1f4f1e8bb11d7f4cb423b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2981cf4f6944050937be182e2d5a689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61ad218e38a4f91bee3d2aba749b418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086934b5adb1494c82f3cfd2afad0f34",
            "placeholder": "​",
            "style": "IPY_MODEL_863e5e6b0574490687e34f80d592fe97",
            "value": "Map: 100%"
          }
        },
        "c7477dbb47fb435fb346d77f02d608be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1072b96a26d410fa10ebf460b27560e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fba7520145449a590054610593b8339",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7477dbb47fb435fb346d77f02d608be",
            "value": 548105171
          }
        },
        "d4a756c485ed480abdfe51b4301d1e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5797b4514b6495a9be7fbe2cc8a3813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f32d7cb08ca24cdf8ada2ef29a808686",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83e3eff2246d409eaa4da41eebe9cb2c",
            "value": 124
          }
        },
        "d58c3b603b5a4da3a3a7d9ebf688da03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b264639e6ab844b2b9e82a12d4d416aa",
            "placeholder": "​",
            "style": "IPY_MODEL_e59b649c3ab24a4c8eee1a467f43bd87",
            "value": " 26.0/26.0 [00:00&lt;00:00, 3.78kB/s]"
          }
        },
        "d7f08ad6930247f7afe82da892cf36f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b46c27ef8043f595b98f4325e209ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_215ed698f92440d6a46917aadac3c1c7",
              "IPY_MODEL_1e51377579ba4abfa992c83c1f6cb813",
              "IPY_MODEL_1282c30469414c189d85af443093d9e9"
            ],
            "layout": "IPY_MODEL_8fdf5679f14f4dce88cc699459e4ed79"
          }
        },
        "d9c22dc8fb8a4b5089e10d11d15860c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2b7ab2ab7947d0968f00ae131435ff",
            "placeholder": "​",
            "style": "IPY_MODEL_967502b1a3514750975abe7dd9fb3eff",
            "value": " 665/665 [00:00&lt;00:00, 86.2kB/s]"
          }
        },
        "dad7a2483bc74bd280fb5c330a5a717b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cfdcc6ee0024e4c9d196d1de4c5b015",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd09996546e4f2baada745c4db3a97f",
            "value": " 548M/548M [00:03&lt;00:00, 332MB/s]"
          }
        },
        "ddb0f6c766214bc08d2a4cc6ac162723": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddd74b2f17234311a64acd5ad4825181": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0855a8f0f544de2bd5bc5d08715ae52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d9e9075bcf460b85dd4a67ff2641c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a02027c1c54ea9831abcf5eb237a5e",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11f0dbd8f0094ee286a5d9fea27b6022",
            "value": 665
          }
        },
        "e2873877eb504826955030022536b519": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3cccc52a75343b28f52bc928d3619df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59b649c3ab24a4c8eee1a467f43bd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7713e97d01641be959c80f7f026bdbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb3d0d180d2e4a038b9fcf9980415f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed7aeb2009b1403582f0f227cff4e501": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f32d7cb08ca24cdf8ada2ef29a808686": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74e787f1ed147c19fb487a581973321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39557a0229844ffa8de50a9311459fab",
            "placeholder": "​",
            "style": "IPY_MODEL_d4a756c485ed480abdfe51b4301d1e54",
            "value": "config.json: 100%"
          }
        },
        "f7b5c045bc3e45898e4aed56024949a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb0254c92eb444729fca77fa6d1ef4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f74e787f1ed147c19fb487a581973321",
              "IPY_MODEL_e0d9e9075bcf460b85dd4a67ff2641c9",
              "IPY_MODEL_d9c22dc8fb8a4b5089e10d11d15860c0"
            ],
            "layout": "IPY_MODEL_4a600e223f82494fb6d68e9bf3e0254a"
          }
        },
        "fe10d19e5dc54962830988ae3e6a998e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2fd178419e48f0a922f7fb354ccaaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
