Using device: cuda
Lambda: 0.01, LR: 0.1, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 ADV] Epoch 1/10, batch 50 | joint=15.8328 task=15.8328 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 100 | joint=15.8015 task=15.8015 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 150 | joint=15.7864 task=15.7864 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 200 | joint=15.8625 task=15.8625 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 250 | joint=15.8532 task=15.8532 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 300 | joint=15.8418 task=15.8418 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 350 | joint=15.8739 task=15.8739 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 400 | joint=15.8815 task=15.8815 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10 | joint=15.8787 task=15.8787 ppl_loss=0.0000 ppl=1.00 val_loss=0.2334 val_acc=0.6217 (true=0.9990 false=0.0016) prompt_ppl=nan
Prompt: FER
[PEZ λ=0.01 ADV] Epoch 2/10, batch 50 | joint=15.7782 task=15.7782 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 2/10, batch 100 | joint=15.7888 task=15.7888 ppl_loss=0.0000 ppl=1.00
