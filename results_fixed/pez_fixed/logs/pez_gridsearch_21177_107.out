Using device: cuda
Lambda: 0.75, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.75 ADV] Epoch 1/10, batch 50 | joint=23.4462 task=16.6756 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 100 | joint=23.4151 task=16.6445 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 150 | joint=23.4639 task=16.6933 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 200 | joint=23.4720 task=16.7014 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 250 | joint=23.4776 task=16.7069 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 300 | joint=23.4680 task=16.6974 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 350 | joint=23.4956 task=16.7250 ppl_loss=9.0275 ppl=8329.36
[PEZ λ=0.75 ADV] Epoch 1/10, batch 400 | joint=23.4937 task=16.7231 ppl_loss=9.0275 ppl=8329.36
