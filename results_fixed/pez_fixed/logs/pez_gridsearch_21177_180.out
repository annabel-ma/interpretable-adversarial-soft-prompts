Using device: cuda
Lambda: 0.01, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 ADV] Epoch 1/10, batch 50 | joint=15.8854 task=15.8046 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 100 | joint=15.7553 task=15.6745 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 150 | joint=15.8091 task=15.7283 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 200 | joint=15.8456 task=15.7648 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 250 | joint=15.8418 task=15.7610 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 300 | joint=15.8360 task=15.7552 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 350 | joint=15.8676 task=15.7868 ppl_loss=8.0801 ppl=3229.62
[PEZ λ=0.01 ADV] Epoch 1/10, batch 400 | joint=15.8582 task=15.7774 ppl_loss=8.0801 ppl=3229.62
