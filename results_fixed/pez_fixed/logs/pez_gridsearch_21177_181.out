Using device: cuda
Lambda: 0.01, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 ADV] Epoch 1/10, batch 50 | joint=16.3708 task=16.3708 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 100 | joint=16.3250 task=16.3250 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 150 | joint=16.2775 task=16.2775 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 200 | joint=16.3142 task=16.3142 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 250 | joint=16.3149 task=16.3149 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 300 | joint=16.2992 task=16.2992 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 350 | joint=16.2940 task=16.2940 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10, batch 400 | joint=16.2885 task=16.2885 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 1/10 | joint=16.2912 task=16.2912 ppl_loss=0.0000 ppl=1.00 val_loss=14.8309 val_acc=0.6728 (true=0.5386 false=0.8933) prompt_ppl=nan
Prompt: bright
[PEZ λ=0.01 ADV] Epoch 2/10, batch 50 | joint=16.4726 task=16.4726 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 2/10, batch 100 | joint=16.3917 task=16.3917 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 2/10, batch 150 | joint=16.3344 task=16.3344 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.01 ADV] Epoch 2/10, batch 200 | joint=16.3134 task=16.3134 ppl_loss=0.0000 ppl=1.00
