Using device: cuda
Lambda: 0.5, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.5 ADV] Epoch 1/10, batch 50 | joint=21.3705 task=16.8839 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 100 | joint=21.4034 task=16.9168 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 150 | joint=21.3575 task=16.8709 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 200 | joint=21.3260 task=16.8394 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 250 | joint=21.3258 task=16.8392 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 300 | joint=21.3361 task=16.8494 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 350 | joint=21.3405 task=16.8538 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10, batch 400 | joint=21.3545 task=16.8679 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 1/10 | joint=21.3586 task=16.8720 ppl_loss=8.9733 ppl=7889.80 val_loss=16.5127 val_acc=0.7596 (true=0.7250 false=0.8165) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 2/10, batch 50 | joint=21.3573 task=16.8707 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 100 | joint=21.3721 task=16.8854 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 150 | joint=21.3323 task=16.8456 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 200 | joint=21.2932 task=16.8066 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 250 | joint=21.2851 task=16.7985 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 300 | joint=21.3250 task=16.8383 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 350 | joint=21.3088 task=16.8222 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10, batch 400 | joint=21.3301 task=16.8434 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 2/10 | joint=21.3444 task=16.8578 ppl_loss=8.9733 ppl=7889.80 val_loss=16.5165 val_acc=0.7376 (true=0.6734 false=0.8432) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 3/10, batch 50 | joint=21.2639 task=16.7772 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 100 | joint=21.2537 task=16.7670 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 150 | joint=21.3341 task=16.8474 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 200 | joint=21.3215 task=16.8348 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 250 | joint=21.3187 task=16.8320 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 300 | joint=21.3349 task=16.8482 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 350 | joint=21.3452 task=16.8585 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10, batch 400 | joint=21.3521 task=16.8654 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 3/10 | joint=21.3441 task=16.8574 ppl_loss=8.9733 ppl=7889.80 val_loss=15.0260 val_acc=0.7064 (true=0.5957 false=0.8884) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 4/10, batch 50 | joint=21.3893 task=16.9026 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 100 | joint=21.3401 task=16.8535 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 150 | joint=21.3563 task=16.8696 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 200 | joint=21.3746 task=16.8879 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 250 | joint=21.3886 task=16.9020 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 300 | joint=21.3672 task=16.8805 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 350 | joint=21.3645 task=16.8778 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10, batch 400 | joint=21.3667 task=16.8800 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 4/10 | joint=21.3408 task=16.8541 ppl_loss=8.9733 ppl=7889.80 val_loss=14.1427 val_acc=0.6746 (true=0.5347 false=0.9046) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 5/10, batch 50 | joint=21.3267 task=16.8400 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 100 | joint=21.3496 task=16.8630 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 150 | joint=21.3585 task=16.8719 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 200 | joint=21.2986 task=16.8119 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 250 | joint=21.3155 task=16.8289 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 300 | joint=21.3196 task=16.8329 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 350 | joint=21.3470 task=16.8603 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10, batch 400 | joint=21.3341 task=16.8475 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 5/10 | joint=21.3243 task=16.8376 ppl_loss=8.9733 ppl=7889.80 val_loss=13.6096 val_acc=0.6550 (true=0.4948 false=0.9184) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 6/10, batch 50 | joint=21.2986 task=16.8119 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 100 | joint=21.2361 task=16.7494 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 150 | joint=21.3462 task=16.8595 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 200 | joint=21.3300 task=16.8433 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 250 | joint=21.3367 task=16.8500 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 300 | joint=21.3732 task=16.8866 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 350 | joint=21.3772 task=16.8905 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10, batch 400 | joint=21.3805 task=16.8938 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 6/10 | joint=21.3807 task=16.8940 ppl_loss=8.9733 ppl=7889.80 val_loss=13.3651 val_acc=0.6404 (true=0.4668 false=0.9256) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 7/10, batch 50 | joint=21.2813 task=16.7947 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 100 | joint=21.3232 task=16.8366 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 150 | joint=21.2943 task=16.8076 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 200 | joint=21.2943 task=16.8076 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 250 | joint=21.3394 task=16.8527 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 300 | joint=21.3449 task=16.8582 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 350 | joint=21.3354 task=16.8487 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10, batch 400 | joint=21.3398 task=16.8531 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 7/10 | joint=21.3233 task=16.8366 ppl_loss=8.9733 ppl=7889.80 val_loss=13.4031 val_acc=0.6425 (true=0.4737 false=0.9200) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 8/10, batch 50 | joint=21.5119 task=17.0252 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 100 | joint=21.3957 task=16.9091 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 150 | joint=21.3582 task=16.8715 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 200 | joint=21.3752 task=16.8885 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 250 | joint=21.3745 task=16.8878 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 300 | joint=21.3701 task=16.8835 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 350 | joint=21.3750 task=16.8884 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10, batch 400 | joint=21.3555 task=16.8688 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 8/10 | joint=21.3630 task=16.8764 ppl_loss=8.9733 ppl=7889.80 val_loss=13.0939 val_acc=0.6187 (true=0.4245 false=0.9378) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 9/10, batch 50 | joint=21.4439 task=16.9573 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 100 | joint=21.3980 task=16.9113 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 150 | joint=21.3606 task=16.8739 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 200 | joint=21.3921 task=16.9054 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 250 | joint=21.4087 task=16.9220 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 300 | joint=21.4013 task=16.9146 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 350 | joint=21.3803 task=16.8936 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10, batch 400 | joint=21.3712 task=16.8845 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 9/10 | joint=21.3637 task=16.8770 ppl_loss=8.9733 ppl=7889.80 val_loss=12.9958 val_acc=0.6080 (true=0.4063 false=0.9394) prompt_ppl=7889.80
Prompt: dots
[PEZ λ=0.5 ADV] Epoch 10/10, batch 50 | joint=21.3568 task=16.8701 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 100 | joint=21.2834 task=16.7967 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 150 | joint=21.3880 task=16.9013 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 200 | joint=21.3626 task=16.8759 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 250 | joint=21.3704 task=16.8838 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 300 | joint=21.3299 task=16.8433 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 350 | joint=21.3547 task=16.8680 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10, batch 400 | joint=21.3627 task=16.8760 ppl_loss=8.9733 ppl=7889.80
[PEZ λ=0.5 ADV] Epoch 10/10 | joint=21.3406 task=16.8539 ppl_loss=8.9733 ppl=7889.80 val_loss=12.9278 val_acc=0.5991 (true=0.3925 false=0.9386) prompt_ppl=7889.80
Prompt: dots

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_fixed/adversarial_true
  Model: model_lambda_0.5_lr_1e-06_promptlen_1.pt
  History: history_lambda_0.5_lr_1e-06_promptlen_1.json
Job 121 completed: lambda=0.5, lr=1e-6, epochs=10, prompt_length=1, adversarial=--adversarial
