Using device: cuda
Lambda: 0.25, LR: 0.1, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.25 ADV] Epoch 1/10, batch 50 | joint=17.4592 task=17.4592 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 100 | joint=17.4849 task=17.4849 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 150 | joint=17.4542 task=17.4542 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 200 | joint=17.4156 task=17.4156 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 250 | joint=17.4154 task=17.4154 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 300 | joint=17.4010 task=17.4010 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 350 | joint=17.4017 task=17.4017 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10, batch 400 | joint=17.3977 task=17.3977 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 1/10 | joint=17.4013 task=17.4013 ppl_loss=0.0000 ppl=1.00 val_loss=0.7799 val_acc=0.3801 (true=0.0030 false=1.0000) prompt_ppl=nan
Prompt: ahead
[PEZ λ=0.25 ADV] Epoch 2/10, batch 50 | joint=17.3963 task=17.3963 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 100 | joint=17.4488 task=17.4488 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 150 | joint=17.4299 task=17.4299 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 200 | joint=17.4273 task=17.4273 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 250 | joint=17.4154 task=17.4154 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 300 | joint=17.4257 task=17.4257 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 350 | joint=17.4145 task=17.4145 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10, batch 400 | joint=17.4062 task=17.4062 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 2/10 | joint=17.4036 task=17.4036 ppl_loss=0.0000 ppl=1.00 val_loss=0.1857 val_acc=0.6450 (true=0.9911 false=0.0760) prompt_ppl=nan
Prompt: ahead
[PEZ λ=0.25 ADV] Epoch 3/10, batch 50 | joint=17.4583 task=17.4583 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 100 | joint=17.3624 task=17.3624 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 150 | joint=17.3606 task=17.3606 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 200 | joint=17.3901 task=17.3901 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 250 | joint=17.3689 task=17.3689 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 300 | joint=17.3560 task=17.3560 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 350 | joint=17.3584 task=17.3584 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10, batch 400 | joint=17.3695 task=17.3695 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 3/10 | joint=17.3650 task=17.3650 ppl_loss=0.0000 ppl=1.00 val_loss=0.1723 val_acc=0.6321 (true=0.9916 false=0.0412) prompt_ppl=nan
Prompt: ahead
[PEZ λ=0.25 ADV] Epoch 4/10, batch 50 | joint=17.3870 task=17.3870 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 100 | joint=17.3025 task=17.3025 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 150 | joint=17.2987 task=17.2987 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 200 | joint=17.3244 task=17.3244 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 250 | joint=17.3186 task=17.3186 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 300 | joint=17.3358 task=17.3358 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 350 | joint=17.3276 task=17.3276 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10, batch 400 | joint=17.3344 task=17.3344 ppl_loss=0.0000 ppl=1.00
[PEZ λ=0.25 ADV] Epoch 4/10 | joint=17.3433 task=17.3433 ppl_loss=0.0000 ppl=1.00 val_loss=0.2182 val_acc=0.6226 (true=1.0000 false=0.0024) prompt_ppl=nan
Prompt: ahead
