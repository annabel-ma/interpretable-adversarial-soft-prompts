Using device: cuda
Lambda: 0.1, LR: 0.01, Adversarial: False

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 50 | joint=16.2849 task=15.4196 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 100 | joint=16.4186 task=15.5533 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 150 | joint=16.3871 task=15.5218 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 200 | joint=16.3746 task=15.5093 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 250 | joint=16.3988 task=15.5335 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 300 | joint=16.3970 task=15.5317 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 350 | joint=16.4064 task=15.5411 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10, batch 400 | joint=16.4017 task=15.5364 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 1/10 | joint=16.3752 task=15.5098 ppl_loss=8.6535 ppl=5730.01 val_loss=10.2965 val_acc=0.6832 (true=0.6483 false=0.7405) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 50 | joint=16.2126 task=15.3472 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 100 | joint=16.2725 task=15.4071 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 150 | joint=16.2594 task=15.3940 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 200 | joint=16.3337 task=15.4684 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 250 | joint=16.3786 task=15.5133 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 300 | joint=16.4103 task=15.5450 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 350 | joint=16.4142 task=15.5489 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10, batch 400 | joint=16.4178 task=15.5524 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 2/10 | joint=16.4049 task=15.5396 ppl_loss=8.6535 ppl=5730.01 val_loss=9.0227 val_acc=0.7125 (true=0.8023 false=0.5651) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 50 | joint=16.3943 task=15.5289 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 100 | joint=16.3893 task=15.5239 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 150 | joint=16.3547 task=15.4894 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 200 | joint=16.3337 task=15.4684 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 250 | joint=16.3282 task=15.4629 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 300 | joint=16.3400 task=15.4747 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 350 | joint=16.3387 task=15.4733 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10, batch 400 | joint=16.3357 task=15.4704 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 3/10 | joint=16.3323 task=15.4670 ppl_loss=8.6535 ppl=5730.01 val_loss=1.2034 val_acc=0.4694 (true=0.2420 false=0.8432) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 50 | joint=16.5645 task=15.6992 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 100 | joint=16.5423 task=15.6769 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 150 | joint=16.5029 task=15.6376 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 200 | joint=16.4720 task=15.6066 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 250 | joint=16.4844 task=15.6191 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 300 | joint=16.4461 task=15.5808 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 350 | joint=16.4479 task=15.5826 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10, batch 400 | joint=16.4344 task=15.5690 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 4/10 | joint=16.4222 task=15.5568 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2058 val_acc=0.6526 (true=0.9651 false=0.1390) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 50 | joint=16.3055 task=15.4401 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 100 | joint=16.3270 task=15.4617 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 150 | joint=16.3695 task=15.5041 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 200 | joint=16.3610 task=15.4956 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 250 | joint=16.3746 task=15.5092 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 300 | joint=16.3605 task=15.4952 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 350 | joint=16.3939 task=15.5286 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10, batch 400 | joint=16.4019 task=15.5365 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 5/10 | joint=16.4134 task=15.5481 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2514 val_acc=0.6281 (true=0.9931 false=0.0283) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 50 | joint=16.4131 task=15.5477 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 100 | joint=16.3677 task=15.5023 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 150 | joint=16.3650 task=15.4997 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 200 | joint=16.2823 task=15.4170 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 250 | joint=16.2634 task=15.3981 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 300 | joint=16.2657 task=15.4004 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 350 | joint=16.2892 task=15.4239 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10, batch 400 | joint=16.3007 task=15.4353 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 6/10 | joint=16.2974 task=15.4320 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2267 val_acc=0.6349 (true=0.9852 false=0.0590) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 50 | joint=16.4040 task=15.5387 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 100 | joint=16.4098 task=15.5445 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 150 | joint=16.3347 task=15.4694 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 200 | joint=16.3094 task=15.4440 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 250 | joint=16.3112 task=15.4459 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 300 | joint=16.3192 task=15.4538 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 350 | joint=16.3066 task=15.4413 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10, batch 400 | joint=16.2964 task=15.4310 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 7/10 | joint=16.3280 task=15.4626 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2269 val_acc=0.6275 (true=0.9916 false=0.0291) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 50 | joint=16.3337 task=15.4683 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 100 | joint=16.3288 task=15.4634 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 150 | joint=16.3408 task=15.4755 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 200 | joint=16.3595 task=15.4941 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 250 | joint=16.4094 task=15.5441 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 300 | joint=16.4045 task=15.5392 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 350 | joint=16.4083 task=15.5429 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10, batch 400 | joint=16.4072 task=15.5418 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 8/10 | joint=16.3886 task=15.5233 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2295 val_acc=0.6251 (true=0.9911 false=0.0234) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 50 | joint=16.2850 task=15.4196 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 100 | joint=16.3462 task=15.4808 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 150 | joint=16.3612 task=15.4958 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 200 | joint=16.3637 task=15.4983 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 250 | joint=16.3732 task=15.5078 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 300 | joint=16.3906 task=15.5252 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 350 | joint=16.3936 task=15.5282 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10, batch 400 | joint=16.3878 task=15.5224 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 9/10 | joint=16.3893 task=15.5240 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2048 val_acc=0.6260 (true=0.9926 false=0.0234) prompt_ppl=5730.01
Prompt: retail
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 50 | joint=16.5006 task=15.6353 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 100 | joint=16.4885 task=15.6232 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 150 | joint=16.4374 task=15.5721 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 200 | joint=16.3832 task=15.5179 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 250 | joint=16.3838 task=15.5185 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 300 | joint=16.4294 task=15.5641 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 350 | joint=16.4194 task=15.5541 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10, batch 400 | joint=16.4161 task=15.5507 ppl_loss=8.6535 ppl=5730.01
[PEZ λ=0.1 NON-ADV] Epoch 10/10 | joint=16.4086 task=15.5433 ppl_loss=8.6535 ppl=5730.01 val_loss=0.2009 val_acc=0.6260 (true=0.9833 false=0.0388) prompt_ppl=5730.01
Prompt: retail

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_fixed/adversarial_false
  Model: model_lambda_0.1_lr_0.01_promptlen_1.pt
  History: history_lambda_0.1_lr_0.01_promptlen_1.json
Job 55 completed: lambda=0.1, lr=1e-2, epochs=10, prompt_length=1, adversarial=
