Using device: cuda
Lambda: 0.75, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.75 ADV] Epoch 1/10, batch 50 | joint=24.9356 task=16.9225 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 100 | joint=24.9176 task=16.9045 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 150 | joint=25.0200 task=17.0069 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 200 | joint=25.0528 task=17.0397 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 250 | joint=25.0678 task=17.0547 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 300 | joint=25.0587 task=17.0456 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 350 | joint=25.0797 task=17.0666 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10, batch 400 | joint=25.0597 task=17.0466 ppl_loss=10.6841 ppl=43657.47
[PEZ λ=0.75 ADV] Epoch 1/10 | joint=25.0649 task=17.0518 ppl_loss=10.6841 ppl=43657.47 val_loss=0.2393 val_acc=0.6969 (true=0.6862 false=0.7146) prompt_ppl=43657.47
Prompt: urmeaza
[PEZ λ=0.75 ADV] Epoch 2/10, batch 50 | joint=25.1513 task=17.1382 ppl_loss=10.6841 ppl=43657.47
