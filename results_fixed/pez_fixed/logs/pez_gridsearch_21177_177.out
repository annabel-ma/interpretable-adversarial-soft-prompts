Using device: cuda
Lambda: 0.01, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 ADV] Epoch 1/10, batch 50 | joint=17.2703 task=17.1568 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 100 | joint=17.3013 task=17.1879 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 150 | joint=17.3240 task=17.2106 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 200 | joint=17.3095 task=17.1960 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 250 | joint=17.3107 task=17.1973 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 300 | joint=17.2890 task=17.1755 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 350 | joint=17.2728 task=17.1593 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10, batch 400 | joint=17.2633 task=17.1498 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 1/10 | joint=17.2592 task=17.1457 ppl_loss=11.3445 ppl=84503.58 val_loss=19.0105 val_acc=0.7838 (true=0.7737 false=0.8003) prompt_ppl=84503.58
Prompt: ка Malcolm CFR plane $30 drasticallyhanRATsmelling Jung
[PEZ λ=0.01 ADV] Epoch 2/10, batch 50 | joint=17.2045 task=17.0911 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 2/10, batch 100 | joint=17.2393 task=17.1259 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 2/10, batch 150 | joint=17.3179 task=17.2044 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 2/10, batch 200 | joint=17.3105 task=17.1970 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 2/10, batch 250 | joint=17.2872 task=17.1737 ppl_loss=11.3445 ppl=84503.58
[PEZ λ=0.01 ADV] Epoch 2/10, batch 300 | joint=17.2697 task=17.1562 ppl_loss=11.3445 ppl=84503.58
