Using device: cuda
Lambda: 0.5, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.5 ADV] Epoch 1/10, batch 50 | joint=21.4932 task=16.6694 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 100 | joint=21.6483 task=16.8246 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 150 | joint=21.6712 task=16.8474 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 200 | joint=21.6164 task=16.7927 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 250 | joint=21.5954 task=16.7716 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 300 | joint=21.5987 task=16.7749 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 350 | joint=21.6001 task=16.7763 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10, batch 400 | joint=21.6034 task=16.7797 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 1/10 | joint=21.6046 task=16.7808 ppl_loss=9.6475 ppl=15483.67 val_loss=1.3792 val_acc=0.6306 (true=0.9975 false=0.0275) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 2/10, batch 50 | joint=21.6664 task=16.8426 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 100 | joint=21.5871 task=16.7633 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 150 | joint=21.6186 task=16.7948 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 200 | joint=21.6231 task=16.7993 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 250 | joint=21.6137 task=16.7899 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 300 | joint=21.5981 task=16.7743 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 350 | joint=21.5690 task=16.7452 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10, batch 400 | joint=21.5696 task=16.7458 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 2/10 | joint=21.5758 task=16.7520 ppl_loss=9.6475 ppl=15483.67 val_loss=0.4276 val_acc=0.6505 (true=0.9946 false=0.0849) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 3/10, batch 50 | joint=21.6923 task=16.8685 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 100 | joint=21.5922 task=16.7684 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 150 | joint=21.5992 task=16.7754 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 200 | joint=21.5579 task=16.7342 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 250 | joint=21.5734 task=16.7496 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 300 | joint=21.5801 task=16.7563 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 350 | joint=21.5751 task=16.7513 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10, batch 400 | joint=21.5453 task=16.7215 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 3/10 | joint=21.5640 task=16.7402 ppl_loss=9.6475 ppl=15483.67 val_loss=0.1446 val_acc=0.7266 (true=0.9429 false=0.3711) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 4/10, batch 50 | joint=21.7727 task=16.9489 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 100 | joint=21.6122 task=16.7884 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 150 | joint=21.5776 task=16.7538 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 200 | joint=21.5704 task=16.7466 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 250 | joint=21.5908 task=16.7670 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 300 | joint=21.5765 task=16.7527 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 350 | joint=21.6245 task=16.8007 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10, batch 400 | joint=21.5966 task=16.7728 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 4/10 | joint=21.5872 task=16.7635 ppl_loss=9.6475 ppl=15483.67 val_loss=0.1945 val_acc=0.6630 (true=0.9941 false=0.1188) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 5/10, batch 50 | joint=21.5650 task=16.7413 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 100 | joint=21.6120 task=16.7882 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 150 | joint=21.5790 task=16.7552 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 200 | joint=21.5831 task=16.7593 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 250 | joint=21.5882 task=16.7644 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 300 | joint=21.5922 task=16.7684 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 350 | joint=21.5779 task=16.7541 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10, batch 400 | joint=21.5778 task=16.7540 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 5/10 | joint=21.5725 task=16.7487 ppl_loss=9.6475 ppl=15483.67 val_loss=0.2034 val_acc=0.6575 (true=0.9951 false=0.1027) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 6/10, batch 50 | joint=21.3984 task=16.5746 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 100 | joint=21.5251 task=16.7014 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 150 | joint=21.5495 task=16.7258 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 200 | joint=21.5837 task=16.7600 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 250 | joint=21.5612 task=16.7375 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 300 | joint=21.5867 task=16.7630 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 350 | joint=21.5960 task=16.7723 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10, batch 400 | joint=21.5767 task=16.7530 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 6/10 | joint=21.5826 task=16.7588 ppl_loss=9.6475 ppl=15483.67 val_loss=0.1857 val_acc=0.6560 (true=0.9966 false=0.0962) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 7/10, batch 50 | joint=21.6114 task=16.7876 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 100 | joint=21.5221 task=16.6983 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 150 | joint=21.5431 task=16.7193 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 200 | joint=21.5326 task=16.7088 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 250 | joint=21.5457 task=16.7220 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 300 | joint=21.5621 task=16.7383 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 350 | joint=21.5581 task=16.7343 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10, batch 400 | joint=21.5610 task=16.7372 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 7/10 | joint=21.5724 task=16.7486 ppl_loss=9.6475 ppl=15483.67 val_loss=0.2353 val_acc=0.6294 (true=0.9990 false=0.0218) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 8/10, batch 50 | joint=21.5091 task=16.6853 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 100 | joint=21.5868 task=16.7630 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 150 | joint=21.6293 task=16.8055 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 200 | joint=21.6304 task=16.8066 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 250 | joint=21.6148 task=16.7910 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 300 | joint=21.6196 task=16.7958 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 350 | joint=21.6106 task=16.7868 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10, batch 400 | joint=21.5914 task=16.7677 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 8/10 | joint=21.5985 task=16.7747 ppl_loss=9.6475 ppl=15483.67 val_loss=0.2011 val_acc=0.6532 (true=0.9970 false=0.0881) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 9/10, batch 50 | joint=21.4494 task=16.6256 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 100 | joint=21.4504 task=16.6266 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 150 | joint=21.4905 task=16.6667 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 200 | joint=21.4844 task=16.6606 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 250 | joint=21.5044 task=16.6806 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 300 | joint=21.5136 task=16.6898 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 350 | joint=21.5000 task=16.6762 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10, batch 400 | joint=21.5058 task=16.6820 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 9/10 | joint=21.4991 task=16.6753 ppl_loss=9.6475 ppl=15483.67 val_loss=0.2071 val_acc=0.6361 (true=0.9990 false=0.0396) prompt_ppl=15483.67
Prompt: detaliat
[PEZ λ=0.5 ADV] Epoch 10/10, batch 50 | joint=21.5475 task=16.7238 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 100 | joint=21.6055 task=16.7817 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 150 | joint=21.5699 task=16.7461 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 200 | joint=21.6089 task=16.7852 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 250 | joint=21.6003 task=16.7765 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 300 | joint=21.5779 task=16.7541 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 350 | joint=21.5568 task=16.7330 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10, batch 400 | joint=21.5816 task=16.7578 ppl_loss=9.6475 ppl=15483.67
[PEZ λ=0.5 ADV] Epoch 10/10 | joint=21.5746 task=16.7509 ppl_loss=9.6475 ppl=15483.67 val_loss=0.1672 val_acc=0.6648 (true=0.9946 false=0.1229) prompt_ppl=15483.67
Prompt: detaliat

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_fixed/adversarial_true
  Model: model_lambda_0.5_lr_0.001_promptlen_1.pt
  History: history_lambda_0.5_lr_0.001_promptlen_1.json
Job 127 completed: lambda=0.5, lr=1e-3, epochs=10, prompt_length=1, adversarial=--adversarial
