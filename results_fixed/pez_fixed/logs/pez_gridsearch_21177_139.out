Using device: cuda
Lambda: 0.25, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.25 ADV] Epoch 1/10, batch 50 | joint=18.3624 task=16.6693 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 100 | joint=18.5058 task=16.8127 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 150 | joint=18.5090 task=16.8160 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 200 | joint=18.5405 task=16.8474 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 250 | joint=18.5128 task=16.8198 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 300 | joint=18.5060 task=16.8129 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 350 | joint=18.5129 task=16.8199 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10, batch 400 | joint=18.5166 task=16.8235 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 1/10 | joint=18.5374 task=16.8444 ppl_loss=6.7722 ppl=873.25 val_loss=15.4393 val_acc=0.7086 (true=0.5962 false=0.8933) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 2/10, batch 50 | joint=18.5987 task=16.9056 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 100 | joint=18.6100 task=16.9170 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 150 | joint=18.5456 task=16.8525 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 200 | joint=18.4937 task=16.8006 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 250 | joint=18.5114 task=16.8183 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 300 | joint=18.5393 task=16.8463 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 350 | joint=18.5325 task=16.8395 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10, batch 400 | joint=18.5411 task=16.8481 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 2/10 | joint=18.5307 task=16.8376 ppl_loss=6.7722 ppl=873.25 val_loss=13.4989 val_acc=0.6557 (true=0.4958 false=0.9184) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 3/10, batch 50 | joint=18.3415 task=16.6485 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 100 | joint=18.4535 task=16.7605 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 150 | joint=18.5338 task=16.8408 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 200 | joint=18.4975 task=16.8044 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 250 | joint=18.5059 task=16.8129 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 300 | joint=18.5052 task=16.8121 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 350 | joint=18.5209 task=16.8278 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10, batch 400 | joint=18.5271 task=16.8340 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 3/10 | joint=18.5251 task=16.8320 ppl_loss=6.7722 ppl=873.25 val_loss=12.8774 val_acc=0.6196 (true=0.4260 false=0.9378) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 4/10, batch 50 | joint=18.5198 task=16.8267 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 100 | joint=18.5085 task=16.8154 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 150 | joint=18.5631 task=16.8700 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 200 | joint=18.5447 task=16.8516 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 250 | joint=18.5338 task=16.8407 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 300 | joint=18.5626 task=16.8696 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 350 | joint=18.5552 task=16.8621 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10, batch 400 | joint=18.5473 task=16.8542 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 4/10 | joint=18.5365 task=16.8434 ppl_loss=6.7722 ppl=873.25 val_loss=12.7629 val_acc=0.6073 (true=0.4048 false=0.9402) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 5/10, batch 50 | joint=18.5477 task=16.8546 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 100 | joint=18.5329 task=16.8399 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 150 | joint=18.5851 task=16.8920 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 200 | joint=18.5510 task=16.8579 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 250 | joint=18.5642 task=16.8711 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 300 | joint=18.5444 task=16.8514 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 350 | joint=18.5295 task=16.8364 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10, batch 400 | joint=18.5421 task=16.8490 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 5/10 | joint=18.5507 task=16.8577 ppl_loss=6.7722 ppl=873.25 val_loss=12.7141 val_acc=0.6031 (true=0.3979 false=0.9402) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 6/10, batch 50 | joint=18.4286 task=16.7356 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 100 | joint=18.4742 task=16.7812 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 150 | joint=18.5370 task=16.8439 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 200 | joint=18.5519 task=16.8588 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 250 | joint=18.5206 task=16.8275 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 300 | joint=18.5301 task=16.8371 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 350 | joint=18.5362 task=16.8432 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10, batch 400 | joint=18.5374 task=16.8443 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 6/10 | joint=18.5348 task=16.8417 ppl_loss=6.7722 ppl=873.25 val_loss=12.7124 val_acc=0.6015 (true=0.3945 false=0.9418) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 7/10, batch 50 | joint=18.5828 task=16.8898 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 100 | joint=18.5658 task=16.8728 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 150 | joint=18.5910 task=16.8980 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 200 | joint=18.6101 task=16.9170 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 250 | joint=18.5823 task=16.8893 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 300 | joint=18.5750 task=16.8820 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 350 | joint=18.5779 task=16.8848 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10, batch 400 | joint=18.5619 task=16.8688 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 7/10 | joint=18.5692 task=16.8762 ppl_loss=6.7722 ppl=873.25 val_loss=12.6283 val_acc=0.5945 (true=0.3827 false=0.9426) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 8/10, batch 50 | joint=18.5407 task=16.8477 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 100 | joint=18.5133 task=16.8203 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 150 | joint=18.5316 task=16.8385 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 200 | joint=18.5090 task=16.8160 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 250 | joint=18.5216 task=16.8285 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 300 | joint=18.5312 task=16.8381 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 350 | joint=18.5351 task=16.8420 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10, batch 400 | joint=18.5125 task=16.8194 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 8/10 | joint=18.5155 task=16.8225 ppl_loss=6.7722 ppl=873.25 val_loss=12.5854 val_acc=0.5930 (true=0.3797 false=0.9434) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 9/10, batch 50 | joint=18.4612 task=16.7682 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 100 | joint=18.5279 task=16.8349 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 150 | joint=18.5632 task=16.8701 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 200 | joint=18.5300 task=16.8370 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 250 | joint=18.5251 task=16.8320 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 300 | joint=18.4995 task=16.8064 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 350 | joint=18.5012 task=16.8081 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10, batch 400 | joint=18.4948 task=16.8017 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 9/10 | joint=18.5015 task=16.8084 ppl_loss=6.7722 ppl=873.25 val_loss=12.5742 val_acc=0.5936 (true=0.3807 false=0.9434) prompt_ppl=873.25
Prompt: Inde
[PEZ λ=0.25 ADV] Epoch 10/10, batch 50 | joint=18.5073 task=16.8142 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 100 | joint=18.4502 task=16.7572 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 150 | joint=18.4922 task=16.7991 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 200 | joint=18.5329 task=16.8398 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 250 | joint=18.5490 task=16.8559 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 300 | joint=18.5386 task=16.8455 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 350 | joint=18.5329 task=16.8399 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10, batch 400 | joint=18.5187 task=16.8256 ppl_loss=6.7722 ppl=873.25
[PEZ λ=0.25 ADV] Epoch 10/10 | joint=18.5138 task=16.8208 ppl_loss=6.7722 ppl=873.25 val_loss=12.4794 val_acc=0.5817 (true=0.3601 false=0.9458) prompt_ppl=873.25
Prompt: Inde

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_fixed/adversarial_true
  Model: model_lambda_0.25_lr_1e-06_promptlen_1.pt
  History: history_lambda_0.25_lr_1e-06_promptlen_1.json
Job 139 completed: lambda=0.25, lr=1e-6, epochs=10, prompt_length=1, adversarial=--adversarial
