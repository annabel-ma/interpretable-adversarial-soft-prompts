Using device: cuda
Lambda: 0.01, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 ADV] Epoch 1/10, batch 50 | joint=16.9163 task=16.8337 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 100 | joint=16.9779 task=16.8952 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 150 | joint=17.0080 task=16.9253 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 200 | joint=17.0345 task=16.9519 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 250 | joint=17.0239 task=16.9413 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 300 | joint=17.0274 task=16.9448 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 350 | joint=17.0156 task=16.9330 ppl_loss=8.2618 ppl=3873.24
[PEZ λ=0.01 ADV] Epoch 1/10, batch 400 | joint=17.0356 task=16.9530 ppl_loss=8.2618 ppl=3873.24
