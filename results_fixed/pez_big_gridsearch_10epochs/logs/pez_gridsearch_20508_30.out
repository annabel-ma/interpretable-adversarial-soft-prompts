Using device: cuda
Lambda: 0.0, LR: 0.0001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.0 ADV] Epoch 1/10, batch 50 | joint=17.6521 task=17.6521 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 100 | joint=17.8343 task=17.8343 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 150 | joint=17.6767 task=17.6767 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 200 | joint=17.6887 task=17.6887 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 250 | joint=17.6735 task=17.6735 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 300 | joint=17.6968 task=17.6968 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 350 | joint=17.6882 task=17.6882 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10, batch 400 | joint=17.6912 task=17.6912 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 1/10 | joint=17.6933 task=17.6933 ppl_loss=0.0000 ppl=0.00 val_loss=0.1664 val_acc=0.6630 (true=0.9946 false=0.1180) prompt_ppl=0.00
Prompt: maintainedytonchnercapturing rencontré fühltîm handset repetitive Président
[PEZ λ=0.0 ADV] Epoch 2/10, batch 50 | joint=17.8245 task=17.8245 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 100 | joint=17.7570 task=17.7570 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 150 | joint=17.7201 task=17.7201 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 200 | joint=17.6988 task=17.6988 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 250 | joint=17.7066 task=17.7066 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 300 | joint=17.7360 task=17.7360 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 ADV] Epoch 2/10, batch 350 | joint=17.7256 task=17.7256 ppl_loss=0.0000 ppl=0.00
