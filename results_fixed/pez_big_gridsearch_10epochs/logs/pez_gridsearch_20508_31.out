Using device: cuda
Lambda: 0.0, LR: 0.001, Adversarial: False

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 50 | joint=15.7754 task=15.7754 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 100 | joint=15.8590 task=15.8590 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 150 | joint=15.7968 task=15.7968 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 200 | joint=15.8097 task=15.8097 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 250 | joint=15.8268 task=15.8268 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 300 | joint=15.8145 task=15.8145 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 350 | joint=15.8271 task=15.8271 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 400 | joint=15.8289 task=15.8289 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10 | joint=15.8456 task=15.8456 ppl_loss=0.0000 ppl=0.00 val_loss=0.1493 val_acc=0.6902 (true=0.9887 false=0.1997) prompt_ppl=0.00
Prompt: Benz malpractice affairsagne Eltern case advocat luxelage stories
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 50 | joint=15.8474 task=15.8474 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 100 | joint=15.9176 task=15.9176 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 150 | joint=15.9280 task=15.9280 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 200 | joint=15.9197 task=15.9197 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 250 | joint=15.9070 task=15.9070 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 300 | joint=15.9107 task=15.9107 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 350 | joint=15.8821 task=15.8821 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 400 | joint=15.8757 task=15.8757 ppl_loss=0.0000 ppl=0.00
