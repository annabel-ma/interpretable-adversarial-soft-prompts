Using device: cuda
Lambda: 0.1, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=16.9523 task=16.3401 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=16.9301 task=16.3179 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=16.8203 task=16.2080 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=16.8417 task=16.2295 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=16.8681 task=16.2559 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=16.8771 task=16.2649 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=16.8803 task=16.2681 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10, batch 400 | joint=16.8921 task=16.2799 ppl_loss=6.1226 ppl=456.03
[PEZ λ=0.1 ADV] Epoch 1/10 | joint=16.8931 task=16.2809 ppl_loss=6.1226 ppl=456.03 val_loss=1.4458 val_acc=0.6312 (true=0.9985 false=0.0275) prompt_ppl=456.03
Prompt: été
[PEZ λ=0.1 ADV] Epoch 2/10, batch 50 | joint=17.0387 task=16.4265 ppl_loss=6.1226 ppl=456.03
