Using device: cuda
Lambda: 0.1, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=18.9898 task=17.6971 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=18.9491 task=17.6565 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=18.9301 task=17.6375 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=18.9689 task=17.6762 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=18.9457 task=17.6531 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=18.9453 task=17.6527 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=18.9565 task=17.6639 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10, batch 400 | joint=18.9431 task=17.6504 ppl_loss=12.9263 ppl=410968.33
[PEZ λ=0.1 ADV] Epoch 1/10 | joint=18.9347 task=17.6421 ppl_loss=12.9263 ppl=410968.33 val_loss=0.1566 val_acc=0.6801 (true=0.8962 false=0.3250) prompt_ppl=410968.33
Prompt: viagra QuartzotherDH Regina
[PEZ λ=0.1 ADV] Epoch 2/10, batch 50 | joint=18.8705 task=17.5779 ppl_loss=12.9263 ppl=410968.33
