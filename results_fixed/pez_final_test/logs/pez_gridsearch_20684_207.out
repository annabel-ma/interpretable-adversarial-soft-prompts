Using device: cuda
Lambda: 0.1, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=17.6774 task=16.7347 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=17.6851 task=16.7424 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=17.6673 task=16.7247 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=17.6897 task=16.7471 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=17.6939 task=16.7512 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=17.6762 task=16.7336 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=17.6711 task=16.7284 ppl_loss=9.4263 ppl=12411.09
[PEZ λ=0.1 ADV] Epoch 1/10, batch 400 | joint=17.6574 task=16.7148 ppl_loss=9.4263 ppl=12411.09
