Using device: cuda
Lambda: 0.5, LR: 0.1, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.5 ADV] Epoch 1/10, batch 50 | joint=18.8514 task=16.0443 ppl_loss=5.6142 ppl=274.30
[PEZ λ=0.5 ADV] Epoch 1/10, batch 100 | joint=18.8062 task=15.9990 ppl_loss=5.6142 ppl=274.30
[PEZ λ=0.5 ADV] Epoch 1/10, batch 150 | joint=18.7749 task=15.9677 ppl_loss=5.6142 ppl=274.30
[PEZ λ=0.5 ADV] Epoch 1/10, batch 200 | joint=18.7757 task=15.9685 ppl_loss=5.6142 ppl=274.30
[PEZ λ=0.5 ADV] Epoch 1/10, batch 250 | joint=18.7454 task=15.9383 ppl_loss=5.6142 ppl=274.30
[PEZ λ=0.5 ADV] Epoch 1/10, batch 300 | joint=18.7401 task=15.9330 ppl_loss=5.6142 ppl=274.30
