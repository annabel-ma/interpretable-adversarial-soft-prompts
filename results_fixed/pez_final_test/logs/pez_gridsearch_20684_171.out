Using device: cuda
Lambda: 0.5, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.5 ADV] Epoch 1/10, batch 50 | joint=21.9476 task=17.6729 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 100 | joint=22.0349 task=17.7602 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 150 | joint=22.0503 task=17.7757 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 200 | joint=22.0725 task=17.7979 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 250 | joint=22.0756 task=17.8009 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 300 | joint=22.0915 task=17.8168 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 350 | joint=22.0670 task=17.7924 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10, batch 400 | joint=22.0677 task=17.7930 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 1/10 | joint=22.0538 task=17.7791 ppl_loss=8.5493 ppl=5163.11 val_loss=0.1569 val_acc=0.7043 (true=0.9134 false=0.3605) prompt_ppl=5163.11
Prompt: concentration Keep ausgeschlossen knotappro Driver dimensiune printsselbe rugam
[PEZ λ=0.5 ADV] Epoch 2/10, batch 50 | joint=21.9478 task=17.6731 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 2/10, batch 100 | joint=21.8844 task=17.6097 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 2/10, batch 150 | joint=21.9294 task=17.6548 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 2/10, batch 200 | joint=21.9576 task=17.6830 ppl_loss=8.5493 ppl=5163.11
[PEZ λ=0.5 ADV] Epoch 2/10, batch 250 | joint=21.9476 task=17.6729 ppl_loss=8.5493 ppl=5163.11
