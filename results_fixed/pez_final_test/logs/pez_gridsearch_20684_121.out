Using device: cuda
Lambda: 0.0, LR: 0.01, Adversarial: False

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 50 | joint=16.6759 task=16.6759 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 100 | joint=16.5988 task=16.5988 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 150 | joint=16.6252 task=16.6252 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 200 | joint=16.6538 task=16.6538 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 250 | joint=16.6416 task=16.6416 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 300 | joint=16.6150 task=16.6150 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 350 | joint=16.6037 task=16.6037 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10, batch 400 | joint=16.5949 task=16.5949 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 1/10 | joint=16.6080 task=16.6080 ppl_loss=0.0000 ppl=0.00 val_loss=0.2696 val_acc=0.6578 (true=0.9946 false=0.1043) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 50 | joint=16.7535 task=16.7535 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 100 | joint=16.6759 task=16.6759 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 150 | joint=16.6680 task=16.6680 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 200 | joint=16.6700 task=16.6700 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 250 | joint=16.6627 task=16.6627 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 300 | joint=16.6558 task=16.6558 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 350 | joint=16.6640 task=16.6640 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10, batch 400 | joint=16.6535 task=16.6535 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 2/10 | joint=16.6586 task=16.6586 ppl_loss=0.0000 ppl=0.00 val_loss=0.3140 val_acc=0.6382 (true=0.9995 false=0.0445) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 50 | joint=16.8546 task=16.8546 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 100 | joint=16.8298 task=16.8298 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 150 | joint=16.7555 task=16.7555 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 200 | joint=16.7392 task=16.7392 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 250 | joint=16.7338 task=16.7338 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 300 | joint=16.7423 task=16.7423 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 350 | joint=16.7325 task=16.7325 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10, batch 400 | joint=16.7292 task=16.7292 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 3/10 | joint=16.7253 task=16.7253 ppl_loss=0.0000 ppl=0.00 val_loss=0.2481 val_acc=0.6557 (true=0.9980 false=0.0930) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 50 | joint=16.7585 task=16.7585 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 100 | joint=16.6952 task=16.6952 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 150 | joint=16.6606 task=16.6606 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 200 | joint=16.6574 task=16.6574 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 250 | joint=16.6680 task=16.6680 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 300 | joint=16.6468 task=16.6468 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 350 | joint=16.6494 task=16.6494 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10, batch 400 | joint=16.6531 task=16.6531 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 4/10 | joint=16.6600 task=16.6600 ppl_loss=0.0000 ppl=0.00 val_loss=0.1831 val_acc=0.6957 (true=0.9877 false=0.2158) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 50 | joint=16.6940 task=16.6940 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 100 | joint=16.5995 task=16.5995 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 150 | joint=16.6197 task=16.6197 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 200 | joint=16.6303 task=16.6303 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 250 | joint=16.6197 task=16.6197 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 300 | joint=16.6144 task=16.6144 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 350 | joint=16.6083 task=16.6083 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10, batch 400 | joint=16.5876 task=16.5876 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 5/10 | joint=16.6032 task=16.6032 ppl_loss=0.0000 ppl=0.00 val_loss=0.1544 val_acc=0.7364 (true=0.9749 false=0.3444) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 50 | joint=16.7375 task=16.7375 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 100 | joint=16.7270 task=16.7270 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 150 | joint=16.6762 task=16.6762 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 200 | joint=16.6470 task=16.6470 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 250 | joint=16.6402 task=16.6402 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 300 | joint=16.6410 task=16.6410 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 350 | joint=16.6475 task=16.6475 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10, batch 400 | joint=16.6321 task=16.6321 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 6/10 | joint=16.6373 task=16.6373 ppl_loss=0.0000 ppl=0.00 val_loss=0.1663 val_acc=0.7187 (true=0.9803 false=0.2886) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 50 | joint=16.7078 task=16.7078 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 100 | joint=16.6461 task=16.6461 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 150 | joint=16.6719 task=16.6719 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 200 | joint=16.6817 task=16.6817 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 250 | joint=16.6729 task=16.6729 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 300 | joint=16.6409 task=16.6409 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 350 | joint=16.6116 task=16.6116 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10, batch 400 | joint=16.6271 task=16.6271 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 7/10 | joint=16.6359 task=16.6359 ppl_loss=0.0000 ppl=0.00 val_loss=0.1210 val_acc=0.7697 (true=0.9424 false=0.4859) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 50 | joint=16.7028 task=16.7028 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 100 | joint=16.5978 task=16.5978 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 150 | joint=16.6407 task=16.6407 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 200 | joint=16.6264 task=16.6264 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 250 | joint=16.6166 task=16.6166 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 300 | joint=16.6133 task=16.6133 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 350 | joint=16.6220 task=16.6220 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10, batch 400 | joint=16.6461 task=16.6461 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 8/10 | joint=16.6510 task=16.6510 ppl_loss=0.0000 ppl=0.00 val_loss=0.1311 val_acc=0.7547 (true=0.9606 false=0.4163) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 50 | joint=16.5900 task=16.5900 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 100 | joint=16.5877 task=16.5877 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 150 | joint=16.5991 task=16.5991 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 200 | joint=16.6155 task=16.6155 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 250 | joint=16.5892 task=16.5892 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 300 | joint=16.6027 task=16.6027 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 350 | joint=16.6252 task=16.6252 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10, batch 400 | joint=16.6426 task=16.6426 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 9/10 | joint=16.6369 task=16.6369 ppl_loss=0.0000 ppl=0.00 val_loss=0.1587 val_acc=0.7291 (true=0.9784 false=0.3193) prompt_ppl=0.00
Prompt: Lemon
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 50 | joint=16.7931 task=16.7931 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 100 | joint=16.7740 task=16.7740 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 150 | joint=16.7476 task=16.7476 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 200 | joint=16.7244 task=16.7244 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 250 | joint=16.7410 task=16.7410 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 300 | joint=16.7299 task=16.7299 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 350 | joint=16.7147 task=16.7147 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10, batch 400 | joint=16.7185 task=16.7185 ppl_loss=0.0000 ppl=0.00
[PEZ λ=0.0 NON-ADV] Epoch 10/10 | joint=16.7074 task=16.7074 ppl_loss=0.0000 ppl=0.00 val_loss=0.1217 val_acc=0.7749 (true=0.9429 false=0.4988) prompt_ppl=0.00
Prompt: Lemon

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_final_test/adversarial_false
  Model: model_lambda_0.0_lr_0.01.pt
  History: history_lambda_0.0_lr_0.01.json
Job 121 completed: lambda=0, lr=1e-2, epochs=10, prompt_length=1, adversarial=
