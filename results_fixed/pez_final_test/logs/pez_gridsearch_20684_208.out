Using device: cuda
Lambda: 0.1, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=18.6238 task=17.7254 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=18.6540 task=17.7556 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=18.6015 task=17.7031 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=18.5526 task=17.6542 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=18.5508 task=17.6524 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=18.5593 task=17.6609 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=18.5670 task=17.6686 ppl_loss=8.9841 ppl=7975.63
[PEZ λ=0.1 ADV] Epoch 1/10, batch 400 | joint=18.6022 task=17.7038 ppl_loss=8.9841 ppl=7975.63
