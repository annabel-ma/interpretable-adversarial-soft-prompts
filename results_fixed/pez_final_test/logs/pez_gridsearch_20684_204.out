Using device: cuda
Lambda: 0.1, LR: 1e-06, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=16.7182 task=15.7763 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=16.7958 task=15.8539 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=16.7431 task=15.8012 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=16.7439 task=15.8020 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=16.6876 task=15.7457 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=16.6975 task=15.7556 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=16.6879 task=15.7460 ppl_loss=9.4193 ppl=12323.58
[PEZ λ=0.1 ADV] Epoch 1/10, batch 400 | joint=16.6881 task=15.7462 ppl_loss=9.4193 ppl=12323.58
