Using device: cuda
Lambda: 1.0, LR: 0.001, Adversarial: False

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 50 | joint=28.8935 task=17.9499 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 100 | joint=29.0149 task=18.0713 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 150 | joint=29.0505 task=18.1069 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 200 | joint=29.0424 task=18.0988 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 250 | joint=29.0430 task=18.0994 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 300 | joint=29.0377 task=18.0942 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 350 | joint=29.0381 task=18.0945 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10, batch 400 | joint=29.0194 task=18.0759 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 1/10 | joint=29.0229 task=18.0794 ppl_loss=10.9436 ppl=56588.85 val_loss=0.4368 val_acc=0.7300 (true=0.9661 false=0.3420) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 50 | joint=28.9004 task=17.9568 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 100 | joint=28.9576 task=18.0141 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 150 | joint=28.9693 task=18.0257 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 200 | joint=28.9563 task=18.0127 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 250 | joint=28.9542 task=18.0107 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 300 | joint=28.9463 task=18.0027 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 350 | joint=28.9836 task=18.0400 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10, batch 400 | joint=28.9818 task=18.0382 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 2/10 | joint=28.9760 task=18.0325 ppl_loss=10.9436 ppl=56588.85 val_loss=0.2759 val_acc=0.6969 (true=0.9897 false=0.2158) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 50 | joint=28.9913 task=18.0477 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 100 | joint=28.8811 task=17.9375 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 150 | joint=28.8709 task=17.9273 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 200 | joint=28.8913 task=17.9477 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 250 | joint=28.9275 task=17.9839 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 300 | joint=28.9365 task=17.9929 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 350 | joint=28.9363 task=17.9928 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10, batch 400 | joint=28.9246 task=17.9810 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 3/10 | joint=28.9272 task=17.9837 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1345 val_acc=0.7654 (true=0.9474 false=0.4665) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 50 | joint=29.0055 task=18.0619 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 100 | joint=29.0857 task=18.1421 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 150 | joint=29.0288 task=18.0853 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 200 | joint=29.0304 task=18.0868 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 250 | joint=29.0384 task=18.0948 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 300 | joint=29.0178 task=18.0743 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 350 | joint=29.0026 task=18.0591 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10, batch 400 | joint=28.9935 task=18.0500 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 4/10 | joint=28.9838 task=18.0403 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1530 val_acc=0.7529 (true=0.9670 false=0.4010) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 50 | joint=28.9126 task=17.9691 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 100 | joint=28.9627 task=18.0191 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 150 | joint=28.9591 task=18.0156 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 200 | joint=28.9836 task=18.0400 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 250 | joint=28.9747 task=18.0311 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 300 | joint=28.9774 task=18.0339 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 350 | joint=28.9733 task=18.0297 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10, batch 400 | joint=28.9591 task=18.0155 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 5/10 | joint=28.9612 task=18.0176 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1367 val_acc=0.7737 (true=0.9316 false=0.5141) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 50 | joint=28.8459 task=17.9023 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 100 | joint=28.9290 task=17.9854 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 150 | joint=28.8693 task=17.9258 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 200 | joint=28.8760 task=17.9324 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 250 | joint=28.8894 task=17.9458 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 300 | joint=28.9256 task=17.9821 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 350 | joint=28.9430 task=17.9994 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10, batch 400 | joint=28.9509 task=18.0073 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 6/10 | joint=28.9599 task=18.0164 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1432 val_acc=0.7682 (true=0.9538 false=0.4632) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 50 | joint=28.9904 task=18.0468 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 100 | joint=29.0465 task=18.1030 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 150 | joint=29.0701 task=18.1266 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 200 | joint=29.0232 task=18.0796 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 250 | joint=29.0143 task=18.0707 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 300 | joint=29.0090 task=18.0655 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 350 | joint=29.0178 task=18.0743 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10, batch 400 | joint=29.0213 task=18.0777 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 7/10 | joint=29.0097 task=18.0661 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1245 val_acc=0.7945 (true=0.9021 false=0.6176) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 50 | joint=29.0093 task=18.0658 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 100 | joint=29.0551 task=18.1116 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 150 | joint=29.0014 task=18.0578 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 200 | joint=29.0308 task=18.0873 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 250 | joint=29.0069 task=18.0633 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 300 | joint=29.0154 task=18.0718 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 350 | joint=28.9936 task=18.0500 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10, batch 400 | joint=28.9939 task=18.0503 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 8/10 | joint=29.0045 task=18.0609 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1247 val_acc=0.7942 (true=0.9080 false=0.6071) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 50 | joint=28.9888 task=18.0452 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 100 | joint=29.0184 task=18.0748 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 150 | joint=29.0287 task=18.0851 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 200 | joint=29.0220 task=18.0785 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 250 | joint=29.0342 task=18.0906 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 300 | joint=29.0331 task=18.0895 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 350 | joint=29.0338 task=18.0903 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10, batch 400 | joint=29.0512 task=18.1077 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 9/10 | joint=29.0348 task=18.0913 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1241 val_acc=0.7887 (true=0.9257 false=0.5635) prompt_ppl=56588.85
Prompt: intrebari
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 50 | joint=28.8724 task=17.9289 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 100 | joint=28.9059 task=17.9624 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 150 | joint=28.8793 task=17.9358 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 200 | joint=28.8742 task=17.9306 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 250 | joint=28.8962 task=17.9526 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 300 | joint=28.9009 task=17.9574 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 350 | joint=28.8925 task=17.9489 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10, batch 400 | joint=28.8958 task=17.9522 ppl_loss=10.9436 ppl=56588.85
[PEZ λ=1.0 NON-ADV] Epoch 10/10 | joint=28.9090 task=17.9654 ppl_loss=10.9436 ppl=56588.85 val_loss=0.1263 val_acc=0.7865 (true=0.9267 false=0.5562) prompt_ppl=56588.85
Prompt: intrebari

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_final_test/adversarial_false
  Model: model_lambda_1.0_lr_0.001.pt
  History: history_lambda_1.0_lr_0.001.json
Job 1 completed: lambda=1, lr=1e-3, epochs=10, prompt_length=1, adversarial=
