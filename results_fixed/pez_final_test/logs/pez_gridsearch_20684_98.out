Using device: cuda
Lambda: 0.01, LR: 0.0001, Adversarial: False

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 50 | joint=17.0292 task=16.9379 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 100 | joint=16.9151 task=16.8238 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 150 | joint=16.8688 task=16.7775 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 200 | joint=16.8535 task=16.7622 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 250 | joint=16.8438 task=16.7525 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 300 | joint=16.8553 task=16.7640 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 350 | joint=16.8612 task=16.7699 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10, batch 400 | joint=16.8480 task=16.7567 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 1/10 | joint=16.8539 task=16.7626 ppl_loss=9.1308 ppl=9235.51 val_loss=7.6220 val_acc=0.6725 (true=0.5730 false=0.8359) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 50 | joint=16.7399 task=16.6486 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 100 | joint=16.8193 task=16.7280 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 150 | joint=16.8375 task=16.7462 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 200 | joint=16.8621 task=16.7708 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 250 | joint=16.8380 task=16.7467 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 300 | joint=16.8400 task=16.7487 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 350 | joint=16.8482 task=16.7569 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10, batch 400 | joint=16.8535 task=16.7622 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 2/10 | joint=16.8690 task=16.7777 ppl_loss=9.1308 ppl=9235.51 val_loss=1.6778 val_acc=0.6474 (true=0.5701 false=0.7745) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 50 | joint=16.8893 task=16.7980 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 100 | joint=17.0024 task=16.9111 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 150 | joint=16.9460 task=16.8547 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 200 | joint=16.9059 task=16.8146 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 250 | joint=16.8725 task=16.7812 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 300 | joint=16.8544 task=16.7631 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 350 | joint=16.8818 task=16.7905 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10, batch 400 | joint=16.8778 task=16.7865 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 3/10 | joint=16.8744 task=16.7831 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1970 val_acc=0.6878 (true=0.9897 false=0.1916) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 50 | joint=16.7322 task=16.6409 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 100 | joint=16.8085 task=16.7172 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 150 | joint=16.8443 task=16.7530 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 200 | joint=16.8720 task=16.7807 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 250 | joint=16.8680 task=16.7767 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 300 | joint=16.8659 task=16.7746 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 350 | joint=16.8774 task=16.7861 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10, batch 400 | joint=16.8700 task=16.7787 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 4/10 | joint=16.8409 task=16.7496 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1424 val_acc=0.7183 (true=0.9774 false=0.2926) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 50 | joint=16.9674 task=16.8761 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 100 | joint=16.9513 task=16.8600 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 150 | joint=16.8979 task=16.8066 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 200 | joint=16.8976 task=16.8063 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 250 | joint=16.8728 task=16.7815 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 300 | joint=16.8703 task=16.7790 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 350 | joint=16.8721 task=16.7808 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10, batch 400 | joint=16.8746 task=16.7833 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 5/10 | joint=16.8648 task=16.7735 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1952 val_acc=0.6443 (true=0.9985 false=0.0622) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 50 | joint=16.7783 task=16.6870 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 100 | joint=16.7919 task=16.7006 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 150 | joint=16.7643 task=16.6730 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 200 | joint=16.7567 task=16.6654 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 250 | joint=16.7496 task=16.6583 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 300 | joint=16.7522 task=16.6609 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 350 | joint=16.7777 task=16.6864 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10, batch 400 | joint=16.7649 task=16.6736 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 6/10 | joint=16.7647 task=16.6734 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1563 val_acc=0.6835 (true=0.9916 false=0.1770) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 50 | joint=17.0085 task=16.9172 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 100 | joint=16.9718 task=16.8805 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 150 | joint=16.8770 task=16.7856 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 200 | joint=16.8450 task=16.7537 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 250 | joint=16.7958 task=16.7044 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 300 | joint=16.7944 task=16.7031 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 350 | joint=16.7997 task=16.7083 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10, batch 400 | joint=16.7979 task=16.7066 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 7/10 | joint=16.8126 task=16.7213 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1517 val_acc=0.6927 (true=0.9892 false=0.2053) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 50 | joint=16.7913 task=16.7000 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 100 | joint=16.8791 task=16.7878 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 150 | joint=16.8249 task=16.7336 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 200 | joint=16.8031 task=16.7118 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 250 | joint=16.8210 task=16.7297 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 300 | joint=16.8059 task=16.7146 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 350 | joint=16.7934 task=16.7021 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10, batch 400 | joint=16.8027 task=16.7114 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 8/10 | joint=16.7999 task=16.7086 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1298 val_acc=0.7422 (true=0.9587 false=0.3864) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 50 | joint=16.9002 task=16.8089 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 100 | joint=16.9228 task=16.8315 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 150 | joint=16.9289 task=16.8376 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 200 | joint=16.9056 task=16.8143 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 250 | joint=16.8698 task=16.7785 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 300 | joint=16.8782 task=16.7869 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 350 | joint=16.8838 task=16.7925 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10, batch 400 | joint=16.8803 task=16.7890 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 9/10 | joint=16.8763 task=16.7849 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1281 val_acc=0.7657 (true=0.9080 false=0.5319) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 50 | joint=16.8442 task=16.7529 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 100 | joint=16.7844 task=16.6931 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 150 | joint=16.8163 task=16.7250 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 200 | joint=16.9016 task=16.8103 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 250 | joint=16.8690 task=16.7777 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 300 | joint=16.8731 task=16.7818 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 350 | joint=16.8858 task=16.7945 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10, batch 400 | joint=16.8844 task=16.7931 ppl_loss=9.1308 ppl=9235.51
[PEZ λ=0.01 NON-ADV] Epoch 10/10 | joint=16.8753 task=16.7840 ppl_loss=9.1308 ppl=9235.51 val_loss=0.1323 val_acc=0.7315 (true=0.9666 false=0.3452) prompt_ppl=9235.51
Prompt: anii redanalyzed pony cauza

Results saved to /mnt/polished-lake/home/annabelma/other/results_fixed/pez_final_test/adversarial_false
  Model: model_lambda_0.01_lr_0.0001.pt
  History: history_lambda_0.01_lr_0.0001.json
Job 98 completed: lambda=0.01, lr=1e-4, epochs=10, prompt_length=5, adversarial=
