Using device: cuda
Lambda: 0.1, LR: 0.001, Adversarial: True

Loading T5 tokenizer...

Loading GPT-2 for prompt perplexity...

Loading BoolQ dataset...
Balanced BoolQ train: 7106 examples (3553 True, 3553 False)
[PEZ λ=0.1 ADV] Epoch 1/10, batch 50 | joint=17.7846 task=16.8217 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 100 | joint=17.8104 task=16.8476 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 150 | joint=17.7469 task=16.7840 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 200 | joint=17.7538 task=16.7909 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 250 | joint=17.7622 task=16.7993 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 300 | joint=17.7848 task=16.8219 ppl_loss=9.6288 ppl=15195.49
[PEZ λ=0.1 ADV] Epoch 1/10, batch 350 | joint=17.7904 task=16.8275 ppl_loss=9.6288 ppl=15195.49
